---
title: "models-exercises"
author: "Veronica"
date: "July 5, 2016"
output: html_document
---

```{r require, include=FALSE}
require(dplyr)
require(ggplot2)
require(GGally)
require(MASS)
require(qdata)
```

```{r solution, include=FALSE}
show_solution <- TRUE 
```

## Anova

### Exercise 1

A society that produces ballpoint pens uses a multi-head machine for the production of caps for pens. The productor wants to compare the means and the variances on all 16 heads and to determine if there are variations on the means of the different heads.

```{r data_import_an1}
data(pencap)
head(pencap)
```

1. Let us compute descriptive statistics and generate the BW plot of `Width` for each head (`Cavity`).

```{r desc_stats_an1, echo=show_solution}
# Descriptive Statistics
summary_stat <- pencap %>% group_by(Cavity) %>% 
  summarise(n=n(),
  min = min(Width), 
  first_qu = quantile(Width, 0.25),
  median = median(Width),
  mean = mean(Width),
  third_qu = quantile(Width, 0.75),
  max = max(Width),
  sd = sd(Width))

print(summary_stat)

# BW Plot
pencap <- pencap %>% mutate(Cavity=factor(Cavity))
ggp <- ggplot(data=pencap, mapping = aes(x=Cavity, y=Width, fill=Cavity)) +
  geom_boxplot(show.legend = FALSE) 
print(ggp)
```

2. Let us fit a model to check if there are differences between the means of the different groups.
Let us use `options(contrasts = c("contr.treatment", "contr.poly"))` contrasts type and comment the results.  

```{r data_analysis_an1, echo=show_solution}
options(contrasts = c("contr.treatment", "contr.poly"))
fm <- aov(Width~Cavity, data = pencap)
summary(fm)
# aov()’s predefined output gives a global p-value of 5.92e-16 on Cavity effect.
summary.lm(fm)
# Results interpretation: Cavity1 mean appears significantly different from 0 (B0 = 9.981200, p = 2e-16), Cavity2 mean seems significantly different from Cavity1 (0.034150 greater, p-value=0.00148), ...
```

3. Let us check models residuals.

```{r data_analysis_an2, echo=show_solution}
op <- par(mfrow = c(2,2))
plot(fm)
par(op)
# The residual plots confirm that normality and homoscedasticity assumptions are met, and no outliers appear.
```


### Exercise 2




## Linear models

### Exercise 1

The number of impurities (lumps) present in the containers of paint depends on the rate of agitation applied to the container. A researcher wants to determine the relation between the rate of agitation and the number of lumps, so he conducts an experiment. He applies different rates of agitation (`Stirrate`) to 12 containers of paint and he counts the number of impurities (lumps) present in the containers of paint (`Impurity`).  

```{r ex_1_data_import, echo=show_solution}
data(paint)
head(paint)
```

1. Let us compute the main descriptive statistics of `Impurity` and the correlation between `Impurity` and `Stirrate`.

```{r ex_1_desc_stats, echo=show_solution}
# Descriptive Statistics
summary_stat <- paint %>% summarise(n=n(),
  min = min(Impurity), 
  first_qu = quantile(Impurity, 0.25),
  median = median(Impurity),
  mean = mean(Impurity),
  third_qu = quantile(Impurity, 0.75),
  max = max(Impurity),
  sd = sd(Impurity))

print(summary_stat)

# Boxplot
ggp <- ggplot(data = paint, mapping = aes(x="0", y=Impurity)) +
  geom_boxplot(fill="blue") + theme(axis.text.x=element_blank(), axis.title.x = element_blank(),
    axis.ticks.x=element_blank()) 

print(ggp)

# Correlation
cor(paint$Stirrate, paint$Impurity)
pl <- ggpairs(data = paint)
print(pl)
```

2. Let us graphically represent the relation between these variables.

```{r ex_1_reg_plot, echo=show_solution}
pl <- ggplot(data = paint, mapping = aes(x = Stirrate, y=Impurity)) +
  geom_point(color="blue") + 
  geom_smooth(method = "lm", colour="red", se = FALSE)
  
print(pl)  
```

3. Let us compute a simple linear regression between `Impurity` and `Stirrate` and check the residuals of the fitted model. Does `Stirrate` influence `Impurity`? How? 

```{r ex_1_data_analysis, echo=show_solution}
# Fit the linear model
fm <- lm(formula = Impurity ~ Stirrate, data = paint)
summary(fm)
# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm)
par(op)

# The relation that "links" the revolution rate seems to be well described by a simple linear regression, at least for the considered interval of Stirrate. The p-value for the significativity of the coefficient of Stirrate is extremely low. This fact confirms the "non random"" existing relation between the two variables. The p-value of the intercept estimate is 0.817. This would indicate an intercept potentially equal to 0. Keeping the intercept value equal to the estimate, with value of Stirrate=0, the mean number of imputities would be negative: the results of the regression couldn’t be "extended" too outside experimental field. R^2 value is equal to 0.9338; this fact confirms that the model explais well the observed data.
```


### Exercise 2

A pressure switch has a membrane whose thickness (in mm) influences the pressure required to trigger the switch itself. The aim is to determine the thickness of the membrane for which the switch "trig" with a pressure equal to 165 ± 15 KPa. 25 switches with different thickness (`DThickness`) of the membrane was analysed, measuring the the pressure at which each switch opens (KPa) (`SetPoint`).

```{r ex_2_data_import, echo=show_solution}
data(switcht)
head(switcht)
```

1. Let us compute the descriptive statistics of the variable `SetPoint`.

```{r ex_2_desc_stats, echo=show_solution}
# Descriptive Statistics
summary_stat <- switcht %>% summarise(n=n(),
  min = min(SetPoint), 
  first_qu = quantile(SetPoint, 0.25),
  median = median(SetPoint),
  mean = mean(SetPoint),
  third_qu = quantile(SetPoint, 0.75),
  max = max(SetPoint),
  sd = sd(SetPoint))

print(summary_stat)

# Boxplot
ggp <- ggplot(data = switcht, mapping = aes(x="0", y=SetPoint)) +
  geom_boxplot(fill = "brown1") + theme(axis.text.x=element_blank(), axis.title.x = element_blank(),
    axis.ticks.x=element_blank()) 

print(ggp)
```

2. Let us graphically represent the relation `DThickness` and `SetPoint`.

```{r ex_2_reg_plot, echo=show_solution}
pl <- ggplot(data = switcht, mapping = aes(x = DThickness, y=SetPoint)) +
  geom_point(color="blue") + 
  geom_smooth(method = "lm", colour="red", se = FALSE)
  
print(pl)  
```

3. Let us compute a linear regression between `DThickness` and `SetPoint` and check the residuals of the fitted model. Does `DThickness` influences `SetPoint`? Is the model correct?

```{r ex_2_data_analysis, echo=show_solution}
# Fit the linear model
fm1 <- lm(formula = SetPoint ~ DThickness, data = switcht)
summary(fm1)

# The relation that "links" the thickness of diaphragm with the pressure for which the switch trigs seems well described by a simple linear regression, at least in the considered interval. The p-value for the significativity of the coefficient of DThickness is extremely low. This fact confirms the "non random" existing relation between the two variables. The p-value of the estimate of the intercept is very low. The intercept is significative. R^2 value is 0.9354. This fact confirms that the model explains well the observed data.

# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm1)
par(op)

# The graphic of the residual values in respect of the estimated values shows a curvilinear trend. This fact implies that the model could be improved, for example with a quadratic model.
```

4. Let us improve the model and compare it with the previous model, if necessary. 

```{r ex_2_mod_improve, echo=show_solution}
# Improve the model 
fm2 <- update(fm1,.~. + I(DThickness^2))
summary(fm2)

# The p-value of the quadratic term is significant, so the quadratic term explains a great quantity of variation in the response in respect of that explained by the linear model. R^2 value is 0.9748, This value is certainly better than the same value of the simple linear model. 

# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm2)
par(op)

# The graphic of the estimated values vs residuals doesn’t show any systematic trend of the residuals and the variance seems to be the same for each estimated value. The residuals are quite well aligned on the line representing the quantile of the normal distribution: It is reasonable to assume that they are then normally distributed.

# Plot of predition from quadratic model
newdata <- data.frame(DThickness = seq(min(switcht$DThickness), 
  max(max(switcht$DThickness)), length = 100))
newdata$predict <- predict(fm2, newdata = newdata)

ggp <- ggplot(data=switcht, mapping = aes(x=DThickness, y=SetPoint)) +
  geom_point(colour="darkblue") + 
  geom_line(data=newdata, mapping=aes(x=DThickness, y=predict), colour="mediumvioletred")

print(ggp)

# Comaprison with the previous model
anova(fm1, fm2)
# This analysis confirms that the contribution of quadratic term is significant.
```


# Exercise 3

The engeneers want to reduce the knocking of the engines. Before doing this, they have to identify which variables influence this phenomenon. Data are randomly collected from 13 engines and contains the following variables:

* `Spark`: indicates the time of advance of the spark plug ignition;
* `AFR`: indicates the air fuel ratio (Air Fuel Ratio);
* `Intake`: indicates the inlet temperature;
* `Exhaust`: indicates the exhaust temperature;
* `Knock`: indicates the knocking of the engine.

```{r ex__data_import, echo=show_solution}
data(knock)
head(knock)
```

1. Let us produce a matrix of scatterplots.

```{r ex_3_desc_stats, echo=show_solution}
ggpairs(data = knock)

# Knock and Spark seem to be negatively correlated; Knock is positively correlated with each of other predictors.
```

2. Let us compute a multiple linear regression between `Knock` and the predictors.

```{r ex__data_analysis, echo=show_solution}
# Fit the linear model
fm1 <- lm(formula = Knock ~ Spark + AFR + Intake + Exhaust, data = knock)
summary(fm1)

# For example the variable Spark seems to be not significant, but if we exclude the variable Exhaust from the model, then Spark becomes significant. This fact is due to the high correlation between the two variables.
fm2 <- lm(formula = Knock ~ Spark + AFR + Intake, data = knock)
summary(fm2)

# Overall the model explains the 98.79% of the variability of the response; also the value of the adjusted R^2 (98.18%) is very high.

# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm1)
par(op)
```


3. Is It possible to improve the resulting model? How?

```{r ex__mod_improve, echo=show_solution}
# Improve the model: remove the variable Spark, which is not significant, from the model
fm3 <- update(fm1,.~. - Spark)
summary(fm3)

# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm3)
par(op)

```


### Exercise 4

A researcher wants to determine which variables are related with the percentage of mortality.
Data conatins the following variables:

* `Rain`: indicates the annual average rainfall;
* `JanTemp`: indicates the average temperatures in January;
* `JulyTemp`: indicates the average temperatures in July;
* `PctOver65`: indicates the percentage of the population over 65 years;
* `HHSize`: indicates the average size of housing;
* `Education`: indicates the years of education;
* `PctHomesLiveable`: indicates the percentage of “habitable” homes;
* `PopDensity`: indicates the density of population;
* `PctLowIncome`: indicates the percentage of low-income families;
* `PctWhiteCollar`: indicates the percentage of employees;
* `Hydrocarbon`: indicates the pollution level by hydrocarbons;
* `NititeOxide`: indicates the pollution level of nitrite oxide;
* `SulphurDioxide`: indicates the pollution level of sulfur dioxide;
* `RelHum`: indicates the annual average relative humidity at 1 PM;
* `MortalityRate`: indicates the mortality rate for 100000 people.


```{r ex_3_data_import, echo=show_solution}
data(mortality)
head(mortality)
```

1. Let us build the complete model, with all the regressors.

```{r ex_3_data_analysis1, echo=show_solution}
# Fit the linear model
fm1 <- lm(formula = MortalityRate ~ RelHum + SulphurDioxide + NititeOxide + Hydrocarbon + PctWhiteCollar + PctLowIncome + PopDensity + PctHomesLiveable + Education + HHSize + PctOver65 + JulyTemp + JanTemp + Rain, data = mortality)

summary(fm1)
```

2. Let us reach to a model in which all the regressors have significant terms at the 5% level, by eliminating a regressor at a time.

```{r ex_3_data_analysis2, echo=show_solution}
# Remove RelHum
fm2 <- update(fm1,.~. -RelHum)
summary(fm2)

# Remove PctWhiteCollar
fm3 <- update(fm2,.~. -PctWhiteCollar)
summary(fm3)

# Remove PctHomesLiveable 
fm4 <- update(fm3,.~. -PctHomesLiveable)
summary(fm4)

# Remove SulphurDioxide
fm5 <- update(fm4,.~. -SulphurDioxide)
summary(fm5)

# Remove PopDensity
fm6 <- update(fm5,.~. -PopDensity)
summary(fm6)

# Remove PctOver65
fm7 <- update(fm6,.~. -PctOver65)
summary(fm7)

# Remove Rain
fm8 <- update(fm7,.~. -Rain)
summary(fm8)

# Remove JulyTemp
fm9 <- update(fm8,.~. -JulyTemp)
summary(fm9)

# Remove HHSize
fm10 <- update(fm9,.~. -HHSize)
summary(fm10)

# Residual Analysis of the final model
op <- par(mfrow = c(2,2))
plot(fm10)
par(op)

# Using other criterions to determine the significant variables, the resulting model could be different.
fm11 <- stepAIC(object = fm1)
summary(fm11)

# Models Comparison 
anova(fm10, fm11)
```

### Exercise 5

A researcher wants to establish if there is a relation between the weight of the body and the weight of the brain of fifteen mammal (african elephant, cow, monkey, man, gray wolf, red fox, armadillo, chinchilla and so on).

```{r ex_4_data_import, echo=show_solution}
data(brainbod)
head(brainbod)
```

1. Let us show the descriptive univariate graphics (histograms and BW plot) of the `Body` and `Brain` variables.

```{r ex_4_desc_stats1, echo=show_solution}
# Body
pl1 <- ggplot(data = brainbod, mapping = aes(x=Body)) +
  geom_histogram()
print(pl1)

pl2 <- ggplot(data = brainbod, mapping = aes(y=Body, x="0")) +
  geom_boxplot() + theme(axis.text.x=element_blank(), axis.title.x = element_blank(),
    axis.ticks.x=element_blank()) 
print(pl2)

# Brain
pl1 <- ggplot(data = brainbod, mapping = aes(x=Brain)) +
  geom_histogram()
print(pl1)

pl2 <- ggplot(data = brainbod, mapping = aes(y=Brain, x="0")) +
  geom_boxplot() + theme(axis.text.x=element_blank(), axis.title.x = element_blank(),
    axis.ticks.x=element_blank()) 
print(pl2)
```

2. Let us show a scatterplot of the two variables and let us estimate a linear regression model. 

```{r ex_4_data_analysis1, echo=show_solution}
# Scatterplot
pl3 <- ggplot(data = brainbod, mapping = aes(y=Brain, x=Body)) +
  geom_point() + geom_smooth(method = "lm", colour= "red", se=FALSE)

print(pl3)

# Fit the linear model
fm1 <- lm(formula = Brain ~ Body, data = brainbod)
summary(fm1)
# A linear regression seems to exist between the two variables. An high value of the R^2 (95.16%) implies that the link between body weight and brain weight is well explained by a linear regression.

# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm1)
par(op)

# However, the weight of the African elephant is much higher than that of other animals, and It makes the graph and the regression curve illegible. A logarithmic transformation of the two variables may suggest a more clear reading of the graph and It may show understandably the relation between the two variables.
# The hypothesis of residual normality seems not to be accepted.
```

3. The linear model between the two variables is adequate? Why? How could it been improved?

```{r ex_4_desc_stats2, echo=show_solution}
# Body
pl1 <- ggplot(data = brainbod, mapping = aes(x=log(Body))) +
  geom_histogram()
print(pl1)

pl2 <- ggplot(data = brainbod, mapping = aes(y=log(Body), x="0")) +
  geom_boxplot() + theme(axis.text.x=element_blank(), axis.title.x = element_blank(),
    axis.ticks.x=element_blank()) 
print(pl2)

# Brain
pl1 <- ggplot(data = brainbod, mapping = aes(x=log(Brain))) +
  geom_histogram()
print(pl1)

pl2 <- ggplot(data = brainbod, mapping = aes(y=log(Brain), x="0")) +
  geom_boxplot() + theme(axis.text.x=element_blank(), axis.title.x = element_blank(),
    axis.ticks.x=element_blank()) 
print(pl2)
```

```{r ex_4_data_analysis2, echo=show_solution}
# Scatterplot
pl4 <- ggplot(data = brainbod, mapping = aes(y=log(Brain), x=log(Body))) +
  geom_point() + geom_smooth(method = "lm", colour= "red", se=FALSE)
print(pl4)

# The scatterplot of the logarithms of the weights seems to be more clear than in the same graph on the original data.

# Fit the linear model
fm2 <- lm(formula = log(Brain) ~ log(Body), data = brainbod)
summary(fm2)
# A linear regression seems to exist between the two variables. An high value of the R^2 (95.16%) implies that the link between body weight and brain weight is well explained by a linear regression.

# Residuals analysis
op <- par(mfrow = c(2,2))
plot(fm2)
par(op)
# In the model that considers logarithmic transformation the hypothesis of residual normality seems to be accepted.
```

## Generalized Linear models

### Exercise 1

A researcher is interested in how variables, such as `GRE` (Graduate Record Exam scores), `GPA` (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don't admit, is a binary variable.

```{r ex_5_data_import, echo=show_solution}
data(admission)
head(admission)
```

This dataset has a binary response (outcome, dependent) variable called `admit`. There are three predictor variables: `gre`, `gpa` and `rank`. We will treat the variables gre and gpa as continuous. The variable `rank` takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. 

1. Let us compute the descriptive statistics of the variables.

```{r ex_5_desc_stats, echo=show_solution}
# Descriptive Statistics
summary(admission)
admission %>% summarise_each(funs="sd")

# Graphs
ggp1 <- ggplot(data = admission, mapping = aes(x = gre, y= admit)) +
  geom_jitter(mapping = aes(y=admit), colour="red", height = 0.01) +
  geom_smooth(method = "loess", colour="blue", se = FALSE, span=2/3) +
  facet_wrap(facets = ~rank) 
print(ggp1)

ggp2 <- ggplot(data = admission, mapping = aes(x = gpa, y= admit)) +
  geom_jitter(mapping = aes(y=admit), colour="red", height = 0.01) +
  geom_smooth(method = "loess", colour="blue", se = FALSE, span=2/3) +
  facet_wrap(facets = ~rank) 
print(ggp2)
```

2.  Let us estimate a logistic regression model using the `glm` (generalized linear model) function. First, we convert `rank` to a factor to indicate that rank should be treated as a categorical variable.

```{r ex_5_data_analysis1, echo=show_solution}
admission <- admission %>% mutate(rank=factor(rank)) 
fm1 <- glm(admit ~ gre + gpa + rank, data = admission, family = "binomial")
summary(fm1)

# For every one unit change in gre, the log odds of admission (versus non-admission) increases by 0.002
# For a one unit increase in gpa, the log odds of being admitted to graduate school increases by 0.804.
# The indicator variables for rank have a slightly different interpretation. For example, having attended an undergraduate institution with rank of 2, versus an institution with a rank of 1, changes the log odds of admission by -0.675.

op <- par(mfrow = c(2,2))
plot(fm1)
par(op)
```

3. Let us compute model predictions.

```{r predictions_ex_5, echo=show_solution}
# Predicted probability of admission at each value of rank
newdata1 <- with(admission,
  data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
newdata1$rankP <- predict(fm1, newdata = newdata1, type = "response")
newdata2 <- with(admission,
  data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100), 4),
  gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))

newdata3 <- cbind(newdata2, predict(fm1, newdata = newdata2, type="link", se=TRUE))
newdata3 <- within(newdata3, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

ggp <- ggplot(newdata3, aes(x = gre, y = PredictedProb)) +
  geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = .2) +
  geom_line(aes(colour = rank), size=1)
print(ggp)
```



### Exercise 2

The number of awards earned by students at one high school. Predictors of the number of awards earned include the type of program in which the student was enrolled (e.g., vocational, general or academic) and the score on their final exam in math.

`num_awards` is the outcome variable and indicates the number of awards earned by students at a high school in a year, `math` is a continuous predictor variable and represents students' scores on their math final exam, and `prog` is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled. It is coded as 1 = "General", 2 = "Academic" and 3 = "Vocational".

```{r ex_6_data_import, echo=show_solution}
data(awards)
head(awards)

# Convert prog and id variables as factors
awards <- awards %>% mutate(prog=factor(prog, levels=1:3, labels=c("General", "Academic", "Vocational")),
            id = factor(id))
```

1. Let us compute some descriptive statistics.

```{r ex_6_desc_stats, echo=show_solution, warning=FALSE}
# Descriptive Statistics
summary(awards)
awards %>% summarise_each(funs(sd))

# Graphs
ggp <- ggplot(awards, aes(num_awards, fill = prog)) +
  geom_histogram(binwidth=.5, position="dodge")
ggp
```

2.  Let us perform a Poisson model analysis using the `glm` function, considering the complete model.

```{r ex_6_data_analysis1, echo=show_solution}
fm1 <- glm(num_awards ~ prog + math, family="poisson", data=awards)
summary(fm1)

## update m1 model dropping prog
fm2 <- update(fm1, . ~ . - prog)
summary(fm2)
## test model differences with chi square test
anova(fm2, fm1, test="Chisq")

```

3. Let us try to improve the model removing non significant variable/s. Compares this model with the previous one. 
```{r ex_6_data_analysis2, echo=show_solution}
## update m1 model dropping prog
fm2 <- update(fm1, . ~ . - prog)
summary(fm2)
## test model differences with chi square test
anova(fm2, fm1, test="Chisq")

```

4. Let us compure predictions of the best fitted model.
```{r predictions_ex_6}
## calculate and store predicted values
awards$phat <- predict(fm1, type="response")

## order by program and then by math
awards <- awards %>% arrange(prog, math)

## create the plot
ggp <- ggplot(awards, aes(x = math, y = phat, colour = prog)) +
  geom_point(aes(y = num_awards), alpha=.5, position=position_jitter(h=.2)) +
  geom_line(size = 1) +
  labs(x = "Math Score", y = "Expected number of awards")
print(ggp)
```




