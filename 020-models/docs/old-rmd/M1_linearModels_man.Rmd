---
title: 
author:
date: 
output:
  html_document:
    self_contained: no
---

```{r setup, include=FALSE}
library(knitr) 
opts_chunk$set(list(dev = 'png',fig.cap='',fig.show='hold',dpi=100,fig.width=7, fig.height=7,fig.pos='H!',fig.path="figure/lm-"))

```

```{r, echo=FALSE}
# La directory di lavoro va definita all'inizio in questo modo per non doverla ripetere in
# ogni chunk! (con echo=F si eseguono i comandi del chunk senza mostrarli)
opts_knit$set(root.dir = "../data/allData")
```

Linear Models
================================


A Brief Introduction to Linear Models
--------------------------------------
Let us consider a simple relation between height and weight in a sample of young people:

```{r,echo=FALSE}
rm(list = ls())
load("sample.Rda")
str(ds)
head(ds)
```
Each row of above fictitious `ds` dataframe contains height (in cm) weight (in Kg), and gender of a different young person.  
Now, one might want to find a simple (or **smooth**) numerical relation (a **regression**) between `height` and `weight` measures, where `weight` will depend on `height`.

```{r,fig.cap="Relation between Weight and Height of 200 young people",echo=FALSE,fig.show='hold'}
plot(ds$height,ds$weight,main="Plot of Weight Vs. Height in 200 young people",xlab="Height",ylab="Weight",pch=16,col="blue")
grid()
```

In other words, one might want to draw a smooth line that "best approximates" the causal relation between `height` and `weight` (i.e., the height influences the weight).

Note that the relation might be really simple, as finding a simple "central value"" (i.e., when one hypotesizes that `weight` does not change when varying the height); in this case the "best" smooth line could be the mean of `weight` values)

```{r,fig.cap="Relation between Weight and Height in 200 young people with 'best constant model'",echo=FALSE,fig.show='hold'}
ds1=ds[order(ds$height),]
plot(ds$height,ds$weight,main="Plot of Weight Vs. Height",xlab="Height",ylab="Weight",pch=16, col="blue")
abline(h=mean(ds$weight),col="red",lwd=2)
segments(x0=ds1$height[c(1,1:20*10)],y0=ds1$weight[c(1,1:20*10)],x1=ds1$height[c(1,1:20*10)],y1=mean(ds$weight),col="green",lty=3,lwd=2)
grid()
```

or the relation might be a few more complex, as a straight line of type (see next graph)
$$
y=\beta_0+ \beta_1 \cdot x
$$ 

```{r,fig.cap="Relation between Weight and Height in 200 young people with best straight line model",echo=FALSE, fig.show='hold'}
plot(ds$height,ds$weight,main="Plot of Weight Vs. Height",xlab="Height",ylab="Weight",pch=16, col="blue")
md=lm(weight~height,data=ds1)
abline(reg=md,col="red",lwd=2)
segments(x0=ds1$height[c(1,1:20*10)],y0=ds1$weight[c(1,1:20*10)],x1=ds1$height[c(1,1:20*10)],y1=fitted(md)[c(1,1:20*10)],col="green",lty=3,lwd=2)
grid()
```  

or also more complex.

Anyway, in either above cases the line is estimated by using the so-called "least squares" method: the "best" line is the one for which the sum of squares of differences between the points and line itself is minimum.

In first model, the "level" for which one can attain the minimum of sum of squares of distance of points from line is at the average of `weight` values ($\overline{y}$). In Figure 2, the dashed green lines represent the distances of some observed points from the "best" (estimated) line.

In second model, the result of application of least-squares method is a few more complex, but can be obtained in closed form, as:

$$
\begin{cases}
  \hat\beta_1=\dfrac{\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2} \\
  \hat\beta_0=\overline{y}-\hat\beta_1 \overline{x}
\end{cases}
$$

where $\hat\beta_0$ and $\hat\beta_1$ represent, respectively, the estimates of $\beta_0$ and $\beta_1$, $n$ is the number of people in sample (the sample size), and $\overline{x}$ and $\overline{y}$ represent, respectively, the arithmetic mean of `height` and the arithmetic mean of `weight` values.

The least-squares method may be seen as a particular case of the more general **Maximum Likelihood** method to data for which the Normality assumption may be applied (see the paragraphs on theoretical introduction).

However notice that least-squares method by itself does not give information about the "better" model: in above examples, least-squares estimates are available for the "only-mean" model as well as for the straight line model, but no information about the preferable model is returned by the least-sqares. Some tests can be used to choose the best model (in next paragraphs we will give information about that).

Finally, the relation can be more complex again: for example, a different line for male and female people may be estimated (see next Figure). 

```{r,fig.cap="Relation between Weight and Height in 100 male and 100 female young people",echo=FALSE}
plot(ds$height,ds$weight,main="Plot of Weight Vs. Height",xlab="Height",ylab="Weight",pch=rep(c(1,5),each=100),col=rep(c("red","blue"),each=100))
md=lm(weight~height*gender,data=ds1)
abline(reg=lm(weight~height,data=ds[ds$gender=="F",]),col="red",lwd=2)
abline(reg=lm(weight~height,data=ds[ds$gender=="M",]),col="blue",lwd=2)
segments(x0=ds1$height[1:50*4],y0=ds1$weight[1:50*4],x1=ds1$height[1:50*4],y1=fitted(md)[1:50*4],col="green",lty=3,lwd=2)
grid()
rm(ds1)
legend(x="topleft",legend=c("Female","Male"), pch=c(1,5),col=c("red","blue"),bg="white")
```

This is because, roughly speaking, a model is said a **linear model** if unknown parameters and error term combine linearly, and not necessarily if the functional form, or relation, between independent and dependent variables is linear. In theoretical introduction chapter this topic is covered more thoroughly.

\newpage

### Linear model formulae and functions in R

In R, the main functions to "build" linear model analysis objects are `lm()` and `aov()`, where the latter may be seen as a "special case" of the first.  
Either functions request a parameter `formula` that allows the analyst to specify the hypothesized relation between dependent (response) variable and independent variable(s).

In following table some formulas examples are shown, where `x`, `x1`, `x2`, `x3` represent continuous variates, whereas upper case letters represent factors

**Expression**                      **Description**
-------------------------------     ----------------------------
`y ~ x`                             Simple regression 
`y ~ 1 + x`                         Explicit intercept
`y ~ -1 + x`                        Through the origin 
`y ~ x + I(x^2)`                    Quadratic regression
`y ~ x1 + x2 + x3`                  Multiple regression
`y ~ G + x1 + x2`                   Parallel regressions
`y ~ G / (x1 + x2)`                 Separate regressions
`sqrt(y) ~ x + I(x^2)`              Transformed
`y ~ G`                             Single classification
`y ~ A + B`                         Randomized block
`y ~ B + N * P`                     Factorial in blocks
`y ~ x + B + N * P`                 with covariate
`y ~ . -x1`                         All variables except `x1` 
`y ~ . + A:B`                       Add interaction (`update`)
`Nitrogen ~ Times*(River/Site)`     More complex design

The items in *Expression* column show several examples of formulas.  
An expression of the form `y ~` `model` is interpreted as a specification that the response `y` is modelled by a linear predictor specified symbolically by `model`. Such a model consists of a series of terms separated by `+` operators.

In next paragraphs and chapters, many examples of use of these functions will be then presented.

#### Common R functions for inference

In following table the main functions used for linear model analysis are presented.

All the functions have to be applied on objects of class `lm` or class `aov`, obtained by calling, respectively,

  `obj = lm(formula)` or `obj = aov(formula)`

**Expression**                  **Description** 
------------------------------  -----------------------------
`coef(obj)`                     regression coefficients
`resid(obj)`                    residuals 
`fitted(obj)`                   fitted values 
`summary(obj)`                  analysis summary 
`predict(obj, newdata = ndat)`  predict for newdata 
`deviance(obj)`                 residual sum of squares


\newpage

Applied Examples
----------------

### Introduction
In this section we will explore linear models in R.
Let us first clean our workspace, load the needed libraries and set some general options
```{r}
rm(list=ls())

library(MASS)
library(lattice)
library(nortest)
library(car)

options(stringsAsFactors = FALSE)
```
After opening the project file (with the ".Rproj" extension), let us set the working directory

```{r eval=FALSE}
setwd("data/allData")
```



### Example 1: Reaction temperature (1-sample t-test)

#### Data description
A chemical theory suggests that the temperature at which a certain chemical reaction occurs is 180 °C.
Data in reaction.Rda contain the results of 10 independent measurements.  

#### Data loading
Let us load and have a look at the data
```{r,split=TRUE}
load("reaction.Rda")
head(reaction)
str(reaction)
```

#### Descriptives
First of all, let us calculate the main descriptive statistics on data vector:
```{r}
stt=c(summary(reaction),sd(reaction))
names(stt)[7]="sd"
print(stt)
rm(stt)
```
Sample mean and median are close to the hypothesized value of 180, and the data point values are close to 180 too.  
Let us plot data (with collection time if meaningful) and add, with `abline()` a reference line for temperature of 180 °C (red) and for the mean temperature (179.36 °C, green) 
```{r,fig.cap="Plot of reaction values",tidy=FALSE,comment='tidy=FALSE forza a-capo nel punto in cui si trova'} 
plot(reaction, pch = 16, ylab = "Reaction temperatures (°C)",
  ylim = c(177,183))
abline(h = 180, col = "red")
abline(h = mean(reaction), col = "darkgreen")
```

The plot shows the temperature (y-axis) on the index of observations (x-axis).
Our objective is to verify if the distance between the two lines (red and green) may be due to chance or if it is unlikely from a statistical point of view. Remember that the main objective of a statistical test is not to accept the null hypothesis, but to find if enough evidence appear to refuse it.  
Now let us plot data without time (no index) by building a stripchart, which directly shows the points and is a good alternative to the boxplot:
```{r,fig.cap="Stripchart of reaction values",tidy=FALSE}
stripchart(reaction)
abline(v = 180, col = "red")
abline(v = mean(reaction), col = "darkgreen")
legend("topleft", legend = c("reaction mean", "target"), lty = 1,
       lwd = 2, col = c("darkgreen", "red"), bty = "n")
```

Note that `abline()`, differently from `plot` or `stripchart`, is a low level function: it indeed adds something to an already drawn plot.  

#### Inference and models

##### One-sample Student's $t$-test

Standard instruments, as one-sample Student's $t$-test, do not use linear models at appearance. Is it true?  
First let us check if the normality assumption of the $t$-test are plausible: let us check if data does not depart too much from normality by using the R function `ad.test` of the `nortest` library: 
```{r}
ad.test(reaction)
```
Anderson-Darling's test tests the normality vs. the non-normality of data distribution. The p-value of the test is fully larger than 0.05, the usual standard for the first type $\alpha$ error probability: therefore there is no evidence of non-normality.  
The q-q plot (_Normal Probability Plot_ in this case) confirms the Anderson-Darling's test result:
```{r,fig.cap="Normal probability plot of reaction"}
qqnorm(reaction)
qqline(reaction, col = "red")
```

The plot compares the observed quantiles with the corresponding quantiles of a standard Normal distribution.  
If data points come from a Normal distribution, then they will tend to follow a straight line, which has been drawn by calling `qqline()`.  
In this case this plot is "nice".  
Now, let us test the hypothesis $H_0$ that the "true mean is equal to 180 °C".
```{r}
t.test(reaction, mu = 180)
```
We cannot refuse $H_0$ since the p-value is `r round(t.test(reaction, mu = 180)$p.value,4)`.  

##### Linear models

How can we think to an alternative which uses a model? Since normality assumption seems satisfied, we could fit a linear model with reaction as response variable and the only intercept as parameter (the model will then report only the mean):
```{r}
mod = lm(formula = reaction~1)
```
In above line of code, the `formula` argument of `lm` function gives the "structure" of the relation between dependent and independent variables: the `~` symbol separates the dependent variable from the model formulation, and the `reaction ~ 1` formula means that the dependent variable `reaction` is modeled by the intercept term only (the `1`) , or, equivalently in this case, by the mean.  
The output of linear model `mod` is
```{r}
mod
```
By applying the `summary()` R function to `lm`-type objects we get more information:
```{r}
summary(mod)
```
The very small p-value ($<2^{-16}$) tells us that the intercept is not 0; also, the intercept estimate (i.e., the mean) is close to 180.  
A way to check if the mean can be considered equal to 180 by using linear models is:
```{r}
reactionZ=reaction-180
fm = lm(reactionZ~1)
```
or, equivalently:
```{r}
fm = lm(I(reaction-180)~1)
```
If the null hypothesis is true, then `reaction-180` has 0 mean.  
The output of the linear model `fm` is
```{r}
summary(fm)
```
The estimated coefficient of the model, in this case the mean of reaction minus 180, is `r coefficients(fm)`, indeed
```{r}
mean(reaction) - 180
```
The p-value, `r round(summary(fm)$coefficients[4],3)`, is the same of the $t$-test: in this case it means that the intercept is not significantly different from 0 (that is the mean of original data can be 180).  
This particularly simple regression model is then equivalent to one-sample $t$-test.  
Let us check for normality of residuals, even if in this particular case is redundant, since it is equivalent to check the normality of `reaction`.
```{r,fig.cap="Normal probability plot of residuals"}
res = as.vector(fm$residuals)
ad.test(res)
qqnorm(res)
qqline(res, col = "red")
```

Indeed, the p-value of Anderson-Darling's test is the same as before. The q-q plot also confirms that the normality assumption is plausible.



### Example 2: Chicken hormones (2-sample $t$-test)

#### Data description
To compare two growth hormones, 18 chicken were tested: 10 being assigned to hormone A and 8 to hormone B. The gains in weights (grams) over the period of experiment were measured. Let us load the data:

#### Data loading
Let us clean the workspace and load the data:
```{r load2}
rm(list = ls())
load("hormones.Rda")
head(hormones)
str(hormones)
```

#### Descriptives
We begin by calulating some descriptive statistics:
```{r,tidy=FALSE}
aggregate(x=hormones$gain,
          by=list(hormone=hormones$hormone),
          FUN=function(x){
            dtt=c(summary(x),sd(x))
            names(dtt)[7]="sd"
            return(dtt)
            }
)
```

And now we draw a boxplot and a stripchart (in this case in vertical) of the data:
```{r preplot2a,tidy=FALSE,fig.cap="Box-and-whiskers plot of gain by hormone"}
boxplot(gain ~ hormone, data = hormones)
```
```{r preplot2b,tidy=FALSE,fig.cap="Stripchart with connect line of gain by hormone"}
stripchart(gain ~ hormone, data = hormones, vertical = T,  
           xlim = c(0.75, 2.25))
points(1:2, tapply(hormones$gain, hormones$hormone, mean),  
       cex = 1.5 , col = "red", pch = 16)
segments(x0 = 1, y0 = mean(hormones$gain[hormones$hormone == "A"]),  
         x1 = 2, y1 = mean(hormones$gain[hormones$hormone == "B"]), col = "red")
```

The `points` function (which requires first abscissae and then ordinates) and `segments` function (which requires the coordinates of two points to be linked), add the red points of two group averages to second graph and then link them with a segment.  
`tapply(gain, hormone, mean)` applies separately the `mean` function to values of `gain` corresponding to levels of the variable `hormone`. In this case, if the segment is near to parallel to the x-axis, then the two means are probably equal. 
Anyway, both graphs and statistics show that hormone B tends to generate heavier chickens.  

#### Inference and models
Let us use `qqmath` of `lattice` library to check the normality of `gain` in the two groups through q-q plots  
```{r qqmath2,fig.cap="",tidy=FALSE,fig.cap="Normal probability plot of gain by hormone using trellis"}
qqmath(~ gain | hormone, data = hormones, aspect = "xy",
  strip = strip.custom(strip.names = TRUE, strip.levels = TRUE),
  par.strip.text = list (cex = 2),
  prepanel = prepanel.qqmathline,
  panel = function(x, ...) {
    panel.qqmathline(x, col = "red", ...)
    panel.qqmath(x, pch = 16, cex = 1.2,...)
  }
)
```

`lattice` is a library that allows one to draw trellis plots, which have a different logic, alternative to R-base graphs. Trellis plots divide the graphical device into panels and are pehaps nicer, but potentially more complex than basic ones.

From graph, no evidence of non-normality is evident. Now let us perform Anderson-Darling's test for `gain` in the two groups, exploiting the function `tapply`:
```{r}
tapply(hormones$gain, hormones$hormone, ad.test)
```
The last line of above code, returns a 2-element list, with AD test for each sub-group. There is no evidence to reject the normality hypothesis within the two groups.  
The next line of code tests the homoscedasticity assumption, that is if the variances within the two groups are equal:
```{r}
var.test(gain ~ hormone, data = hormones)
bartlett.test(gain ~ hormone,data=hormones)
leveneTest(gain ~ hormone,data=hormones)
```
All three previous tests accept $H_0$, i.e., there is no evidence to say that the variability changes across levels.

Now the $t$-test compares the two group means.  

The null hypothesis is $H_0: \beta_1 = \beta_2$, where $\beta_1$ and $\beta_2$ are the expectations of dependent variable for hormones A and B, respectively.
Since the results obtained from above tests on homoscedasticity, the variance in subgroups will be considered equal.
```{r}
t.test(gain ~ hormone, data = hormones, var.equal = TRUE) 
```
The $t$-test rejects the null hypothesis.  

Now the same test may be conducted using a model approach.  
The following lines of code perform the model fitting and print the main results.
```{r}
mod = lm(gain ~ hormone, data = hormones) 
mod
```
The parameters shown are the model intercept and the "slope" coefficient for hormon `B`. The intercept in this case is the estimate of mean growth for `A` hormone, whereas the slope in this case is the estimate of difference between the mean growth obtained with hormon `B` and the mean growth obtained with hormone `A`.  
In other words, the mean growth estimate for `hormoneA` is `619`, while the mean growth estimate for `hormoneB` is `619 + 204 = 823`. We will return to parameterization issue later.

Using `summary` the significancy of parameters is also shown:
```{r}
summary(mod)
```
The test results are equal to the ones of $t$-test, and the $t$-test value is equal to minus $t$-test value of `hormoneB` coefficient. 

Why these results? "Behind the scene", the model has been built using the following **model matrix** ($\underline{X}$)
```{r}
model.matrix(mod)
```
What this matrix means?  
For modeling purposes, the independent variable `hormone` has been "split" in two dummy variables: `hormoneA` and `hormoneB`:

* `hormoneA` is `1` when data contain measures relative to hormon `A`, and is `0` when data contain measures relative to hormon `B` 
* `hormoneB` is `1` when data contain measures relative to hormon `B`, and is `0` when data contain measures relative to hormon `A` 

Obviously, when `hormoneA` is `0`, `hormoneB` is `1` and vice versa. Then, one dummy variable (in this case, `hormoneA`), can be removed from the model, because it does not add any useful information, and makes the model mathematically non-manageable. Consequently, the model matrix will contains only the "all 1s" intercept column (that refers to the case when hormon `A` is used) and the "displacement" `hormoneB` column which represents the constant added to mean growth when using hormon `B` instead of hormon `A`.

In other words, the model actually estimated is the following:

$$
  growth_i = \beta_0 + \beta_1 \cdot \delta_B(hormone_i) + \varepsilon_i
$$

Where $\delta_B(hormone_i)$ represents the function that returns 1 only when the hormone of $i$-th unit is `B`, otherwise, returns 0.  
$\beta_0$ in this manner is the value of mean growth when using hormone `A` (i.e., when $\delta_B(hormone_i)==0$), while $\beta_1$ is the "jump" in mean of growth when using hormone `B` instead of hormone `A`.

Another approach available to manage such type of test is through the analysis of variance model, i.e. by using the `aov()` function, that directly performs one analysis of variance:
```{r}
fm = aov(mod) 
```
Printing a summary method for analysis of variance objects reflects the different approach
to what is essentially the same information as returned by `lm()`. While a printed linear-model
object shows individual coefficients, printed `aov` objects show terms, which may correspond
to several coefficients. However, methods for linear models can always be applied explicitly
on `aov` objects, such as those for the coefficients or residuals.
```{r}
summary(fm)
```
Above output shows that `hormone`, with all its levels, is globally significant (p=0.012) in explaining the dependent variable variability. 

In this case, since the independent variable (factor) contains two levels only, the resulting p-value for `hormone` 
obtained from `aov` is identical to the p-value of hormoneB obtained using `lm()` (actually, in this example they are the same test). 

If the independent variable (factor) contains more than two levels, then `aov()` returns the global significancy (not the "by level significancy") 
of independent variable effect on dependent variable.

A few more concise code to obtain the same results as above is
```{r}
fm = aov(gain ~ hormone, data = hormones) 
summary(fm) 
```

Next lines of code show that actually `aov` objects are `lm` objects with also analysis of variance results added.
```{r}
class(mod)
class(fm)
```

The following commands are useful to read/define the type of contrasts (i.e., the type of coding of regressors matrix $\underline{X}$) used when unordered or ordered factors are used.

```{r}
options("contrasts")
options(contrasts = c("contr.treatment", "contr.poly"))
```
The first line of code simply shows the current predefined type of contrasts for unordered and ordered factors, while the second line of code sets the predefined contrasts.

To extract coefficients from an already calculated (`lm` or `aov`) model, one must use the `coefficients` method:
```{r}
coefficients(fm)
```
Next line of code shows that the `hormoneB` coefficient is simply the difference between average growths for `hormoneA` and `hormoneB` chickens.
```{r}
diff(tapply(hormones$gain, hormones$hormone, mean))
```

R can use several types of contrasts: `contr.treatment` sets to zero the coefficient of first factor level, that becomes the intercept,
and consequently all the means of other levels are relative to the first level. Another option is `contr.sum`, that sets to zero the sum of coefficients, and then the reference value of coefficients becomes the gran mean, calculated on all factor levels. R uses `contr.treatment` as default.

Let's try to change type of contrasts:
```{r}
options(contrasts = c("contr.sum", "contr.poly"))
```
```{r}
fm = aov(gain~hormone, data = hormones)
```
The `aov()` results are the same: 
```{r}
summary(fm)
```
but, on the whole, the results are changed:
```{r}
coefficients(fm)
```

The values of the coefficients are not the same because with `contr.treatment` the intercept represents the mean in
the first group (hormone A) and the "slope" represents the difference between the mean in the second group (hormone B) and
the mean in the first group; conversely, with `contr.sum` the intercept represents the grand mean, and the slope represents the difference
between the mean in the first group and the grand mean.  
The differences in coefficients are perhaps more perceptible looking at the model matrix: 
```{r}
model.matrix(fm)
```

Now, firstly we will restore predefined contrasts

```{r}
options(contrasts = c("contr.treatment", "contr.poly"))
```

and then we will fit a model without intercept to data. In general, removing the intercept is a discouraged practice, but sometimes no intercept models can help to enhance coefficients meaning:
```{r}
mod = lm(gain ~ hormone-1, data = hormones)
coefficients(mod)
```
The above coefficients represent, respectively, the mean in the group `hormoneA` and in the group `hormoneB`.

The model matrix can be useful to understand the meaning of the coefficients:
```{r}
model.matrix(mod)
```

Next code shows what can happen when one uses a no-intercept model and does not address for that in analysis of variance:
```{r}
fm = aov(mod) 
summary(fm)
coefficients(fm)
```
The significancy of factor is much more strong than in previous calculations, but this is wrong, because this result actually compares the model with `0` intercept with the model that uses one mean for `hormoneA` and one mean for `hormoneB`.

Using the following code gives results equivalent to t-test:
```{r}
anova(mod,lm(gain ~ 1, data = hormones))
```
`anova()` performs an analysis of variance to compare two nested models (see the paragraph on _Comparing two nested models_ in _Some Theory on Linear Models_ paragraph).  
In this case we are comparing a model with the group variable `hormone` against the model with grand mean only.  
The result is the same as seen individually in `lm()` and `aov()` outputs.

#### Model diagnostics
Since the assumpions on linear models, the residuals should arise from a gaussian distribution; then, a Normal Probability Plot of residuals will be plotted to check this assumption.
```{r}
hormones$residuals = fm$residuals
```
```{r,fig.cap="",tidy=FALSE,fig.cap="Overlaid Normal probability plots of residuals using trellis"}
qqmath(~ residuals ,data = hormones, aspect = 1,
  prepanel = prepanel.qqmathline, groups = hormone,
  panel = function(x, ...) {
    panel.qqmathline(x, col = "darkgreen")
    panel.qqmath(x, pch = 16, cex = 1.5,...)
  }
)
```

The two colors correspond to two groups.  
Residuals seem gaussian, irrespective of group.

The Anderson-Darling (AD) test confirms that:
```{r}
ad.test(fm$residuals) 
```
This confirms that the analysis and its results may be considered correct.


### Example 3: Park assistant paired $t$

```{r,echo=FALSE}
rm(list = ls())
```

#### Data description

In an experiment to investigate if an optional equipment of cars may help
to park the cars themselves, 20 drivers were asked to park the car with and 
without the equipment. The parking time (secs) for each driver and equipment
is recorded.

#### Data loading
```{r}
load("carctl.Rda")
str(carctl)
```

#### Descriptives

A simple boxplot:
```{r,fig.cap="Box-and-whiskers plot of parking times for the two cars"}
boxplot(x=carctl)
```

A stripchart with mean points and mean line:
```{r,fig.cap="Stripchart with connect line of parking times for the two cars",tidy=FALSE}
stripchart(x=carctl, vertical = T, xlim = c(0.75, 2.25))
points(1:2, apply(X=carctl, MARGIN=2,FUN=mean), 
       cex = 1.5 , col = "red", pch = 16)
segments(x0 = 1, y0 = mean(carctl$Car_A), x1 = 2, 
         y1 = mean(carctl$Car_B), col = "red")
```

And now the $t$-test. 

Before testing the means, a check for normality is made by using qq-plot and Anderson-Darling test
```{r,fig.cap="Normal probability plot of parking times of two cars using trellis",tidy=FALSE}
qqmath(~ Car_A+Car_B, data = carctl, aspect = "xy",
         prepanel = prepanel.qqmathline,
         panel = function(x, ...) {
         panel.qqmathline(x, col = "red", ...)
         panel.qqmath(x, pch = 16, cex = 1.2,...)
       }
) 
```

There is not evidence of non-normality within groups, and the AD tests confirm above results:
```{r}
lapply(X=carctl, FUN=ad.test) 
```
and finally the two independent samples $t$-test:
```{r}
t.test(x=carctl$Car_A, y=carctl$Car_B) 
```
Following this result, the null hypothesis is not rejected (at a 5% level).

But if we produce a scatterplot of `Car_A` Vs `Car_B`,
```{r,fig.cap="Scattrplot of parking times of Car_A Vs. Car_B using trellis"}
xyplot(Car_A~Car_B,data=carctl)
```

we can see that data of two cars are clearly NOT independent!!  
Measures on `Car_A` are strongly related to measures on `Car_B`, because each driver drove either cars.

Then the correct test for such as data is:
```{r}
carctl$Car_diff=carctl$Car_A-carctl$Car_B
t.test(x=carctl$Car_diff,mu=0)
```
or, equivalently, a paired $t$:
```{r}
t.test(x=carctl$Car_A,y=carctl$Car_B,paired=TRUE)
```

In either cases the results are the same: a difference statistically significant exists between mean parking times of two cars.

In this case the Normality test must be performed on differences between measures within same driver.

Check of normality by qqplot:
```{r,fig.cap="Normal probability plot of difference of parking times of two cars using trellis"}
qqmath(~ Car_diff, data = carctl, aspect = "xy",
       prepanel = prepanel.qqmathline,
       panel = function(x, ...) {
         panel.qqmathline(x, col = "red", ...)
         panel.qqmath(x, pch = 16, cex = 1.2,...)
       }
) 
```

And the Anderson-Darling test:
```{r}
ad.test(x=carctl$Car_diff)  
```
For either tests, the Normality assumption is not rejected.

Now we try the model approach to the same test. The model approach in this case follows the same procedure seen for the one-sample t-test, but using the differences between times as dependent variable.

Fitting of model:
```{r}
mod = lm(Car_diff ~ 1, data = carctl) 
mod
summary(mod)
```
or, equivalently one may use also:
```{r}
mod = lm(I(Car_A-Car_B)~1, data=carctl)
summary(mod)
```

q-q plot (or Normal probability plot) of residuals shall be identical to previous one.  
The Anderson-Darling test too is identical to previuosly made test:
```{r}
ad.test(mod$residuals) 
```
equivalently:
```{r}
ad.test(residuals(mod))
```

Note that for repeated measures experiments, several analytical options are also available: for example through the `Error()` function in formula specification, or using mixed-model methods (`lme4` or `nlme` packages are the most used and known).  
In this manual we won't deep these arguments, because they would require a distinct section for each of them.

### Example 4: Tissues
```{r,echo=FALSE}
rm(list = ls())
```

#### Data description

Three quality inspectors of a plant, Henry, Anne, and Andrew (operators) measure the strenght of car seat tissues.
The company managers want to test the reproducibility, between operators, of company's measurement system.  
The main goal of study is then to verify if operators' measures are confrontable.  
Each operator measured 25 pieces of car seat tissues.  
Globally, 75 samples of tissue randomly chosen from the same production batch were measured.

#### Data loading
```{r}
load("tissue.Rda")
head(tissue)
str(tissue)
```
Data is already ordered by operator (`Anne`, `Henry`, and `Andrew`)

#### Descriptives
Plot of `Strengt` with `Operator` factor variable in abscissa:
```{r,fig.cap="Stripchart of Strenght by Operator with connect line"}
stripchart(Strength ~ Operator, data = tissue, vert = T, pch = 16)
tissue.mean = aggregate(Strength ~ Operator, FUN = mean, data = tissue)
within(tissue.mean, lines(1:3, Strength, col = "red", type = "b"))
```

A different way to show similar information is through a plot of univariate effects:
```{r,fig.cap="Plot of univariate effects of Operator on Strenght"}
plot.design(Strength ~ Operator, data = tissue)
```
This plot shows the mean for each level of grouping factor compared to the grand mean (the center wider line).

#### Inference and models
Now we will try models with different contrast types.  
Initially the model with default contrasts will be fitted to data, then the `contr.sum` and the `contr.helmert` contrast models will be shown.

Default contrasts:
```{r}
options("contrasts")
options(contrasts = c("contr.treatment", "contr.poly"))
fm.treatment = aov(Strength~Operator, data = tissue)
summary(fm.treatment)
summary.lm(fm.treatment)
```
`aov()`'s predefined output gives a global p-value of 0.026 on `Operator` effect.  

Note that `summary.lm()` shows the `aov` resulting object `fm.treatment` as if it simply were an `lm`-type object.

From its output, Henry's mean appears significantly different from 0 ($\beta_0$ = 9.7288, p = 2e-16), 
Andrew seems significantly different from Henry (0.708 bigger, p-value=0.0087), whereas Anne's "position" is not really clear (mean 0.5 bigger than Henry, but only marginally significant, if $\alpha$ = 0.05).  

Instead of `summary(<aov object>)` and `summary.lm(<aov object>)`, `summary.aov(<lm object>)` and
`summary(<aov object>)` may be used, obtaining the same outputs:
```{r}
fm.treatment = lm(Strength~Operator, data = tissue)
summary.aov(fm.treatment)
summary(fm.treatment)
```
In this sense, `aov()`,`lm()` and their resulting objects are substantially interchangeable.

Now let's see what changes if constrasts settings are modified. The first contrast setting is `contr.sum`:
```{r}
options(contrasts = c("contr.sum", "contr.poly"))
fm.sum = aov(Strength~Operator, data = tissue)
summary(fm.sum)
summary.lm(fm.sum)
```
The interpretation of the `contr.sum` coefficients is not very simple: contrasts are the $k-1$ differences (where $k$ is the number of factor levels) between each level and a reference level; the resulting estimated coefficients contain the differences of each level mean (excluded the reference level) with respect to the grand mean; in fact, the model intercept is equal to grand mean.  
`aov()`, however, gives the same results obtained with default contrasts.
<!-- Non a' una buona idea mostrarlo..
```{r}
model.matrix(fm.sum)
```
non esguiamo nemmeno questo
```{r}
options(contrasts = c("contr.treatment", "contr.poly"))
```
-->

Now let's change contrast settings by selecting Helmert contrasts
```{r}
options(contrasts = c("contr.helmert", "contr.poly"))
fm.helm = aov(Strength~Operator, data = tissue)
summary(fm.helm)
summary.lm(fm.helm)
```
<!-- Anche qui e' bene non mostrarlo..
model.matrix(fm.helm)
-->
Helmert's contrasts are even more complex than the others; they analyze:  

1. Initially, the difference between first level mean and the second level mean
2. Then, the difference between the mean of first two (pooled) levels and the third level mean
3. Then, the difference between the mean of first three (pooled) levels and the fourth level mean
4. And so on for $k-1$ contrasts ..

The main advantage of using Helmert contrasts is that they are orthogonal; the tests on linear model coefficients are then stocastically independent: the Type I and Type II error rate for the tests on Helmert coefficients are unrelated. The main disavantage, of course, is their complexity in coefficients explanations.

Now, let's restore predefined contrasts:
```{r}
options(contrasts = c("contr.treatment", "contr.poly"))
```
<!--
compare model matrices
```{r}
model.matrix(fm.treatment)
model.matrix(fm.sum)
model.matrix(fm.helm)
```
```{r}
#vcov(fm.treatment)
#vcov(fm.sum)
#vcov(fm.helm)
```
-->
Summarizing the results just seen about contrasts:

* `contr.treatment` makes easier to read results, but the tests on _coefficients_ are NOT independent
* `contr.sum` makes NO independent tests on _coefficients_, but compares pairs of means; also, the coefficients represent the distance of level's mean with respect to grand mean, however they are not always easily interpretable
* `contr.helmert` makes independent tests on _coefficients_, but the coefficients are more difficult to interpret
* anyway, until now, the global ANOVA (`aov()`)results are always the same: `aov()` analyzes effects globally, not single means

#### Residuals analysis
By plotting a model object (`lm` or `aov`) up to six residuals diagnostic plots may be shown.

By default, the `plot()` method applied on a model object shows four graphs:

1. A plot of raw residuals against fitted values
2. A Normal Q-Q plot on standardized residuals
3. A scale-Location plot of $\sqrt{\vert std. residuals \vert}$ against fitted values. Here, $std. residuals$ means raw residuals divided by $\hat\sigma \cdot \sqrt{1-h_{ii}}$, where $h_{ii}$ are the diagonal entries of the "hat" matrix (see Appendices chapter).
4. A plot of standardized Pearson residuals against leverages. If the leverages are constant (as is the case in the balanced design of this example) the plot uses factor level combinations instead of the leverages for the x-axis.

The next lines of code split the graphical device in 4 areas (2 x 2) and plot diagnostic graphs in only one display
```{r,fig.show='hold',fig.cap="Compound diagnostic residual plot of Strenght Vs. Operator ANOVA model"}
op = par(mfrow = c(2, 2))
plot(fm.treatment)
par(op)
```
The last line of above code restores the graphical device.

The residual plots confirm that normality and homoscedasticity assumptions are met, and no outliers appear. This confirms the model results.


### Example 5: Brake distances

#### Data description
Distance data contain measurements of brake distances on the same car equipped with several configurations of:

* Tire: Factor with 3 levels "GT","LS","MX"
* Tread: Factor with 2 levels "1.5","10"
* ABS: Factor with levels "disabled","enabled"

For each combination of three factor levels, 2 measurements of brake distance have been taken.

The objective of experiment is to find which factor(s) influence the brake distance, and in which direction.

```{r,echo=FALSE}
rm(list = ls())
```

#### Data loading
```{r}
load("distance.Rda")
head(distance)
str(distance)
```

#### Descriptives
Plot univariate effects
```{r,fig.cap="Plot of univariate factors effects on Distance response variable"}
plot.design(Distance ~ ., data = distance)
```
The means seem to change mainly with `Tire` type and `ABS` levels. `Tread` seems to not influence mean brake distance. 

Plot two-way interaction plot
```{r,fig.cap="Plots of two-way interaction effects of factors on Distance response variable"}
op = par(mfrow = c(3, 1))
with(distance, {
  interaction.plot(ABS, Tire, Distance)
  interaction.plot(ABS, Tread, Distance)
  interaction.plot(Tread, Tire, Distance)
  }
)
par(op)
```
The distance seems to decrease from ABS disabled to enabled independently of the tire type (no interaction). Perhaps, an interaction between `Tread` and `Tire` or between `ABS` and `Tire` might exist.
...

#### Inference and models

Now let's build a full model with all interactions:
```{r}
fm = aov(Distance ~ ABS * Tire * Tread, data = distance)
```
It is equivalent to:
```{r}
fm = aov(Distance ~ ABS + Tire + Tread + ABS:Tire + ABS:Tread + Tire:Tread
         + ABS:Tire:Tread, data = distance)
summary(fm)
```
Actually, only main effects `ABS` and `Tire` seem to influence the mean of brake distance.
Interactions seem not significant: we can start to drop them from the three-way interaction.

In general we are interested to find the most provident model which better explains the data, 
and in general we start to drop first higher-level interactions, because we "move" within the hierarchical models paradigm.

To obtain the model without three-way interaction, we can use `update()`:
```{r}
fm = update(fm, . ~ . -ABS:Tire:Tread)
summary(fm)
```
Since two ways interactions seem to not influence mean brake distance, we could try to remove all two-way interactions, always using `update()` function:
```{r}
fm1=update(fm, .~ABS+Tire+Tread)
summary(fm1)
```
The result seems not surprising. It is better to check if the three removed effects together are still not
significant using `anova()`:
```{r}
anova(fm, fm1)
```
The combined effect of the tree two-way interactions is not significant.  
The tables of effects from model follow below
```{r}
model.tables(fm1,type="effects")
```
And now the table of means from model
```{r}
model.tables(fm1,type="means")
```
Finally, we remove the `Tread` effect to obtain the final model with significant effects only:
```{r}
fm = aov(Distance ~ ABS + Tire, data = distance)
summary(fm)
```
#### Residual analysis
```{r,fig.cap="Compound residual plot for final brake distance model"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```
Since the leverages are constant (as is typically the case in a balanced `aov` situation) the 4th plot draws factor level combinations instead of the leverages for the x-axis.
(The factor levels are ordered by mean fitted value.)

Distance means for each `ABS` and `Tire` category follow:
```{r}
with(distance, tapply(Distance, list (ABS, Tire), FUN = mean))
```

The table of grand means from model:
```{r}
model.tables(fm,type="means")
```

And some predicted values from ANOVA model:
```{r}
dfPred=data.frame(ABS=c("enabled","disabled"),Tire=c("GT","LS"))
predict(object=fm,newdata=dfPred)
```
Question: predicted values are different from the above calculated sampling averages. Why?


###  Example 6: Brake distance 1 (unbalanced anova with one obs dropped)

#### Data description 
Same example as above, but with one observation (row) removed

```{r,echo=FALSE}
rm(list = ls())
```

#### Data loading
```{r}
load("distance.Rda")
head(distance)
str(distance)
```

Drop one observation for dealing with an unbalanced anova with one obs dropped (one "LS 10 enabled")
```{r}
distance1 = distance[-24,]
```

#### Descriptives
Plot of univariate effects
```{r,fig.cap="Univariate effects plot of unbalanced model"}
plot.design(Distance ~ ., data = distance1)
```
Plot two-way interaction plot
```{r,fig.cap="Two-way interaction effects plots of unbalanced model"}
op = par(mfrow = c(3, 1))
with(distance1, {
  interaction.plot(ABS, Tire, Distance)
  interaction.plot(ABS, Tread, Distance)
  interaction.plot(Tread, Tire, Distance)
  }
)
par(op)
```
All the effects do not seem to change with respect to previous example

#### Inference and models

Model with all interactions
```{r}
fm = aov(Distance ~ ABS * Tire * Tread, data = distance1)
summary(fm)
```

Model without three-way interaction
```{r}
fm = update(fm, . ~ . -ABS:Tire:Tread)
summary(fm)
```

Model without two-way interactions
```{r}
fm1=update(fm, .~ABS+Tire+Tread)
summary(fm1)
```

Check significancy of two-way interactions:
```{r}
anova(fm, fm1)
```
They are not significant

The final model with significant effects is the same of model of previous example, but:
```{r}
fm = aov(Distance ~ ABS + Tire, data = distance1)
summary(fm)
fminv = aov(Distance ~ Tire + ABS, data = distance1)
summary(fminv)
```
Since `aov()` performs Type I SS ANOVA (see *Types of Sum of Squares* in the Appendix) and this example uses data from unbalanced design, the previous 2 models give different results in terms of SS and respective p-values.

`fm` returns SS(ABS) and SS(Tire | ABS), whereas `fminv` returns SS(Tire) and SS(ABS | Tire).  

In order to get Type II ANOVA SSs and p-values, one can consider SS(Tire | ABS) and SS(ABS | Tire).  
Otherwise, `drop1()` function can also be used.

```{r}
drop1(object=fm,test="F")
drop1(object=fminv,test="F")
```
In this case, the results are obviously equal to those of Type II SS

Alternatively, the function `Anova()` of the package `car` is available. `Anova()` allows Type II and III Sum of Squares too.  

Notice that, until now, at least six types of sum of squares have been introduced in literature.  
However, there are open discussions among statisticians about the use and pros/cons of different Types of SS.

#### Residual analysis
```{r,fig.show='hold',fig.cap="Residual plots of unbalanced model"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```
In this case, since the leverages are not constant (unbalanced design) 4th plot draws the leverages in x-axis.


### Example 7: Fixed effects Nested ANOVA

#### Data description

The dataframe contains data collected from five different lathes, 
each of these used by two different operators.  
The goal of study is to evaluate if differences appear in mean diameter of pins between lathes and/or operators.  
Notice that here we are concerned with the 
effect of operators, so the layout of experiment is nested. If we were concerned with 
shift instead of operator, the layout would be crossed. The measurement is the diameter of turned pins.

```{r, echo=FALSE}
rm(list = ls())
```
#### Data loading

```{r}
load("diameters.Rda")
str(diameters)
```

#### Descriptives
Next lines of code show descriptives for each variable and the mean for each crossing of `Lathe` and `Operator` factor levels.
```{r}
summary(diameters)
xtabs(formula=Pin.Diam~Lathe+Operator,data=diameters)
````
Above statistics are not completely well-advised, since the operators working in the same
part of the day (day or night) are different (nested anova) for different lathes.

Let us draw a box-plot for each combination of `Late` and `Operator` factor levels.
```{r,fig.cap="Boxplot of Pin.Diam by Lathe x Operator"}
boxplot(Pin.Diam ~ Lathe*Operator, data = diameters)
```

And now we try a first (incorrect) ANOVA to analyze diameters Vs. `Lathe` and `Shift` (i.e., considering `Operator` levels as equivalent to different shifts of working days) in a classical factorial layout.
```{r}
fm1=aov(formula=Pin.Diam~Lathe*Operator,data=diameters)
summary(fm1)
```
As previously said, the above result is incorrect. 

Actually, the data structure is the following:
```{r}
diameters$LatheOp = factor(diameters$Lathe:diameters$Operator)
xtabs(formula=Pin.Diam~Lathe+LatheOp,data=diameters)
```

And then, correct ANOVA is:
```{r}
fm1=aov(formula=Pin.Diam~Lathe+Lathe/Operator,data=diameters)
summary(fm1)
```

The `/` formula operator means that the levels of `Operator` factor are nested within the levels of `Lathe` factor.
Using this model, lathes seem produce different products (in mean), whereas no significant difference between operators appears.  

Nested anova is useful for reducing the general variability of the plan and getting more important
differences among the levels of the main factors.

Equivalent model formulations for nested ANOVAs are:
```{r}
fm1a=aov(formula=Pin.Diam~Lathe+Operator:Lathe,data=diameters)
summary(fm1a)
```

or 

```{r}
#alternative correct model
fm1b=aov(formula=Pin.Diam~Lathe+Operator:LatheOp,data=diameters)
summary(fm1b)
```

#### Residuals analysis
```{r,fig.cap="Residual plot of Late by Operator nested model"}
op = par(mfrow = c(2, 2))
plot(fm1)
par(op)
```


```{r, echo=FALSE, results='hide', eval=FALSE}

###  Example 7:  Random effects 1 (one way random effects anova)


rm(list = ls())

#- lme only accepts nested random effect; lmer handles crossed random effects 
#- lme has a convenient method of handling heteroscedasticity; lmer doesn't. 
#- lme gives you p-values; lmer doesn't (there is explanation of why at 
#  https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html which, I guess, should not be all that reassuring for lme users either) 


# lmer is newer, much faster, handles crossed random effects well (and 
# generalized linear mixed models), has some support for producing 
# likelihood profiles (in the development version), and is under rapid development. 
# It does not attempt to estimate residual degrees of freedom and 
# hence does not give p-values for significance of effects. 
# lme is older, better documented (Pinheiro and Bates 2000), more 
# stable, and handles 'R-side' structures (heteroscedasticity, 
#                                          within-group correlations). 
# 
# r-sig-mixed-models is a good place for questions about these packages. 
# 
# 
# Yield of dyestuff (Napthalene Black 12B) for 5 preparations from each of 6
# batches of an intermediate product (H-acid). 
# In the experiment six samples of the intermediate, representing different batches 
# of works manufacture, were obtained, and five preparations of the dyestuff were 
# made in the laboratory from each sample. The equivalent yield of each preparation 
# as grams of standard colour was determined by dye-trial.”

load("dyestuff.Rda")
str(dyestuff)
summary(dyestuff)
library(nlme)

boxplot(Yield ~ Batch, data = dyestuff)
# better stripchart

stripchart(Yield ~ Batch, data = dyestuff, vertical = T, xlim = c(0.75, 6.25))
# add mean and mean line
means=tapply(dyestuff$Yield, dyestuff$Batch, mean)
points(1:6, means, cex = 1.5 , col = "red", pch = 16)
segments(x0 = 1:5, y0 = as.vector(means)[1:5], x1 = 2:6, y1 = as.vector(means)[2:6], col = "red")



# In this case (one-way ANOVA), significancy may be calculated by using a simple fixed effects ANOVA  (results are identical to those of random effects ANOVA) 

dye.fm=lm(Yield ~ 1 + Batch, dyestuff)
dye.fm
summary(aov(dye.fm))


# Is the same of:

dye.fm0=lm(Yield ~ 1 , dyestuff)
dye.fm0
anova(dye.fm,dye.fm0,test="F")


# estimated sigmaBatch ("Satterthwaite"):

tbl=summary(aov(dye.fm))[[1]]
sigmaBatch=(tbl[1,3]-tbl[2,3])/replications(Yield ~ ., dyestuff)
sigmaBatch



# Now calculates the components of variance

dye.fme= lme(fixed=I(Yield-19.38) ~ 1,data=dyestuff,random=~1|Batch) 
summary(dye.fme)


dye.fmr=lmer(formula=Yield ~ 1 + (1|Batch),data=dyestuff)
summary(dye.fmr)



# Other example (simulated)

load("dyestuff2.Rda")
str(dyestuff2)
summary(dyestuff2)



boxplot(Yield ~ Batch, data = dyestuff2)

# better stripchart

stripchart(Yield ~ Batch, data = dyestuff2, vertical = T, xlim = c(0.75, 6.25))
# add mean and mean line
means=tapply(dyestuff2$Yield, dyestuff2$Batch, mean)
points(1:6, means, cex = 1.5 , col = "red", pch = 16)
segments(x0 = 1:5, y0 = as.vector(means)[1:5], x1 = 2:6, y1 = as.vector(means)[2:6], col = "red")



# Also in this case, significancy may be clculated by using a simple ANOVA fixed effect 

dye.fm=lm(Yield ~ 1 + Batch, dyestuff2)
dye.fm
summary(aov(dye.fm))


# estimated sigmaBatch (Satterthwaite):

tbl=summary(aov(dye.fm))[[1]]
sigmaBatch=(tbl[1,3]-tbl[2,3])/replications(Yield ~ ., dyestuff)
sigmaBatch



dye2.fm= lme(fixed=Yield ~ 1,data=dyestuff2,random=~1|Batch) 
summary(dye2.fm)



dye2.fmr= lmer(formula=Yield ~ 1+ (1|Batch),data=dyestuff2) 
summary(dye2.fmr)




##  PGONEW Random effects 4 (two random factors ANOVA) #### S ####
rm(list = ls())


# assess the variability between samples of penicillin by the B. subtilis method.
# In this test method a bulk-innoculated nutrient agar medium is poured into
# a Petri dish of approximately 90 mm. diameter, known as a plate. When the
# medium has set, six small hollow cylinders or pots (about 4 mm. in diameter)
# are cemented onto the surface at equally spaced intervals. A few drops of the
# penicillin solutions to be compared are placed in the respective cylinders, and
# the whole plate is placed in an incubator for a given time. Penicillin diffuses
# from the pots into the agar, and this produces a clear circular zone of inhibition
# of growth of the organisms, which can be readily measured. The diameter of
# the zone is related in a known way to the concentration of penicillin in the
# solution.


data(Penicillin)

str(Penicillin)
summary(Penicillin)


# This is a crossed and balanced  design

xtabs(~ sample + plate, Penicillin)


# Simple ANOVA model (in general, incorrect; in this case (two main random effects ANOVA), correct):

Pen.lm=lm(diameter~plate+sample,data=Penicillin)
summary(aov(Pen.lm))


# Now the RE model 

(Pen.lmr <- lmer(diameter ~ 1 + (1|plate) + (1|sample), Penicillin))


# And now test the significancy of effects

(Pen.lmrML0 <- lmer(diameter ~ 1 + (1|plate) + (1|sample), Penicillin,REML=FALSE))

(Pen.lmrMLp <- lmer(diameter ~ 1 + (1|plate) , Penicillin,REML=FALSE))

(Pen.lmrMLs <- lmer(diameter ~ 1 + (1|sample), Penicillin,REML=FALSE))


# significancy of sample effect

anova(Pen.lmrMLp,Pen.lmrML0)


# significancy of plate effect

anova(Pen.lmrMLs,Pen.lmrML0)




##  PGONEW Random effects   (nested  random effects anova) #### S ####

rm(list = ls())


# Ch2.pdf
# deliveries of a chemical paste product contained in casks where, in addition to
# sampling and testing errors, there are variations in quality between deliveries
# ... As a routine, three casks selected at random from each delivery were sampled
# and the samples were kept for reference.... Ten of the delivery batches were
# sampled at random and two analytical tests carried out on each of the 30
# samples.


load("pastes.Rda")

str(pastes)
summary(pastes)


# This might seem that this crossed random design:

  xtabs(~ batch + cask , pastes, drop = TRUE, sparse = TRUE)

# but, each cask "a" in batch A is different from cask "a" in batch B, and so on...
# A "sample" column can be built from scratch by using:

  pastes$sample = factor(pastes$batch:pastes$cask)

# or

# pastes = within(pastes, sample <- factor(batch:cask))



xtabs(~ batch + sample , pastes, drop = TRUE, sparse = TRUE)
summary(pastes)


# lmer model

(Past.lmr = lmer(formula=strength ~ 1 + (1|batch/cask),data=pastes))

# alternative notation (only for this case)

(Past.lmr2= lmer(formula=strength ~ 1 + (1|batch)+(1|sample),data=pastes))

# Other notation

(Past.lmr3= lmer(formula=strength ~ 1 + (1|batch)+(1|batch:cask),data=pastes))


# To test significancy of effects, ML estimation is required:

# lmer model

(Past.lmrML0 = lmer(formula=strength ~ 1 + (1|batch)+(1|sample),data=pastes,REML=F))

(Past.lmrML1 = lmer(formula=strength ~ 1 + (1|sample),data=pastes,REML=F))

(Past.lmrML2 = lmer(formula=strength ~ 1 + (1|batch),data=pastes,REML=F))


# Now the test

anova(Past.lmrML0,Past.lmrML1)
anova(Past.lmrML0,Past.lmrML2)

# It seems that the differences are only on casks, not between batches 
# 
# tentative normal plot

# qqnorm(residuals(Past.lmrML2))
# qqline(residuals(Past.lmrML2))
# ...
```

\newpage

### Example 8: Depressant drug and rats regression

```{r,echo=FALSE}
rm(list = ls())
```

#### Data description

In an experiment to investigate the effects of a depressant drug, the
reaction times of ten male rats to a certain stimulus were measured
after a specified dose of the drug have been administered to each rat.

#### Data loading
```{r}
load("drug.Rda")
str(drug)
head(drug)
```

#### Descriptives
Since we are mainly interested in relation between `dose` of drug and reaction `time`, we start the descriptive analysis plotting a graph that relates the two variables.
```{r,tidy=FALSE,fig.cap="Scatterplot of time Vs. dose with fitted linear regression and lowess line"}
plot(time ~ dose, data = drug, pch = 16,  xlab = "dose   (mg)",
     ylab = "reaction time   (secs)")
#Add a smoothing
lines(lowess(drug$dose, drug$time), col = "violet", lwd = 2)
fm = lm(time ~ dose, data = drug) 
# add the regression line to the plot
abline(fm, col = "red")
```

In above code, `lines()` is a function to draw lines in a plot, while `lowess()` is a function to perform local regression (lots of regressions put together to 
describe data in a smooth way) method.

Perhaps a simple straight linear regression is a good choice to describe the relation.

#### Inference and models

We then produce a summary of the simple linear model $E(y_i \vert xi) = \beta_0 + \beta_1 \cdot x_i$ between reaction time and dose of drug.
```{r}
summary(fm) 
```
Reading the results, it appears that if we increase `dose` by one unit, `time` increases by 0.48 on average, and this growth is significant.

Another way to obtain the p-value for `dose` parameter is comparing the `time ~ dose` model with `time ~ 1` model,  and testing significancy of slope via `anova()`
```{r}
anova(fm,update(fm,.~.-dose))
```
The results are the same.

#### Residual analysis
```{r,fig.show='hold',fig.cap="Residual plot of simple linear model"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```
Plots do not evidence particular problems.

\newpage
<!--
In the fourth plot there are no points outside Cook's bands.

Plot all diagnostic graphics
```{r,fig.show='hold',fig.cap="Residual plot of simple linear model"}
op = par(mfrow = c(2, 3))
plot(fm, which = 1:6)
par(op)
```
-->


### Example 9: Laboratory and on-line tools

#### Data description
Two pH measurement tools are compared: one instrument is a "gold standard", 
and is the "laboratory" tool, the other is an on-line (on the field) tool.  
Same samples are measured by using both instruments to see if on-line tool performs as laboratory tool. 

```{r,echo=FALSE}
rm(list = ls())
```

#### Data loading
```{r}
load("labonline.Rda")
str(labonline)
```

#### Descriptives
Since in this example we are simply interested in the relation between `Lab` and `Online` tool readings, we perform the descriptive analysis with a scatterplot only.

```{r,tidy=FALSE,fig.cap="Scatterplot of Online Vs. Lab"}
summary(labonline)
plot(Online ~ Lab, data = labonline, pch = 16,  
     xlab = "Laboratory   (pH)", ylab = "On-line   (pH)")
lines(lowess(labonline$Lab, labonline$Online), col = "violet", lwd = 2)
abline(lm(Online ~ Lab, data = labonline), col = "red")
```

#### Inference and models
Let us produce a summary of a simple linear model $E(y_i \vert xi) = \beta_0 + \beta_1 \cdot x_i$
```{r}
fm = lm(Online ~ Lab, data = labonline) 
fm1=summary(fm) 
fm1
```
Only slope is significant.  
Intercept not significant means that when laboratory tool returns 0, on-line tool returns a mean value that can be 0.  
This behavior is correct.  
Another test to perform is to verify if the slope may be equal to 1.  
The Wald test to check this may be performed through:
```{r,tidy=FALSE}
2*pt(abs(fm1$coefficients[2,1]-1)/fm1$coefficients[2,2],
     df=fm1$df[2],
     lower.tail=FALSE)
```
Since the returned value (p-value) is greater than $\alpha$=0.05, the null hypothesis that $\beta_1$=1 is accepted.

#### Residual analysis
```{r,fig.show='hold',fig.cap="Residual plot of Online Vs. Lab model"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```
The residuals are not really "nice", but their behavior might be due to rounding of pH values to the first decimal place only. 


### Example 10: Polyesterification  regression
```{r,echo=FALSE}
rm(list = ls())
```

#### Data description

In the study of the polyesterification of fatty acids with glycols, the
effect of temperature ($^◦$C) on the percentage conversion of the
esterification process was investigated.  
Data are the results of an experiment using a catalyst of $4 \cdot 10^{−4}$
mole zinc chloride per 100 grams of fatty acid.

#### Data loading
```{r}
load("polyester.Rda")
```

```{r}
str(polyester)
head(polyester)
```

#### Descriptives
Let us plot the relation between dependent and independent variables.

```{r,tidy=FALSE,fig.cap="Scatterplot of conversion Vs. temperature"}
plot(conversion ~ temperature, data = polyester , pch = 16,  
     xlab = "temperature   (°C)", ylab = "percentage conversion (%)")
```
A quadratic function seems to fit the data. 

#### Inference and models
Despite above results, we initially try to estimate a simple linear model, and to produce an illustrative graph of the model.
```{r,tidy=FALSE,fig.cap="Scatterplot of conversion Vs. temperature with linear fit"}
fm0 = lm(conversion ~ temperature, data = polyester) 
summary(fm0)
plot(conversion ~ temperature, data = polyester , pch = 16, 
     xlab = "temperature   (°C)", ylab = "percentage conversion (%)")
abline(fm0$coefficients, col=2)
```
Now the graph of residuals.
```{r,fig.show='hold',fig.cap="Residual analysis of simple linear model conversion Vs. temperature "}
op=par(mfrow=c(2,2))
plot(fm0)
par(op)
```

Either graphs suggest that a curve effect may exist.  
Another plot to check this hypothesis, is the plot of residuals Vs. independent variable:
```{r,fig.cap="Scatterplot of residuals Vs. temperature"}
plot(polyester$temperature,residuals(fm0))
abline(h=0, col="blue")
```
A clear curvature appears.

At this point, a quadratic model can be tried:
```{r}
fm = lm(conversion ~ poly(temperature, 2, raw = TRUE), data = polyester) 
```
or, alternatively:
```{r}
fm=update(fm0,.~. + I(temperature^2))
```

Now we see the model summaries
```{r}
summary(fm) 
```
All parameters are significant, and this confirms the initial observation of a curvilinear relation.

#### Residual analysis
```{r,fig.show='hold',fig.cap="Residual plot of final model"}
op=par(mfrow=c(2,2))
plot(fm)
par(op)
```

The residual plots seem "regular" enough.

As an additional check, we can try also to add a term of cube of temperature.
```{r}
(fm1=update(fm,.~. + I(temperature^3)))
summary(fm1)
anova(fm1,fm)
```
This cubic model does not add any useful information with respect to quadratic one.  
Now, let's use the quadratic model to produce a graph of predicted values:
```{r,tidy=FALSE,fig.cap="Plot of predition from quadratic model"}
newdata = data.frame(temperature = seq(min(polyester$temperature), 
                                       max(max(polyester$temperature)), 
                                       length = 100))
newdata$predict = predict(fm, newdata = newdata)
plot(predict ~ temperature , data = newdata , type = "l", 
     col = "violet", lwd = 2,  xlab = "temperature   (°C)", 
     ylab = "percentage conversion (%)")
points(conversion ~ temperature, data = polyester , pch = 16, 
       col = "darkgrey", cex = 1.5)
grid()
```

\newpage

### Example 11: Hardness and Density regression
```{r, echo=FALSE}
rm(list = ls())
```

#### Data description

Janka Hardness is an importance rating of Australian hardwood timbers. The test itself
measures the force required to imbed a steel ball into a piece of wood and therefore 
provides a good indication to how the timber will withstand denting and wear.
Janka hardness is strongly related to the density of the timber and can usually be 
modelled using a polynomial relationship.  
The dataset consists of density and hardness measurements from 36 Australian Eucalypt hardwoods.  
Variables in dataframe are:

* Density: Density measurements
* Hardness: Janka hardness

#### Data loading
```{r}
rm(list=ls())
load("janka.Rda")
head(janka)
str(janka)
```

#### Descriptives

Let us begin with a "base" plot showing the relation between `Hardness` and `Density`
```{r,fig.cap="Scatterplot of Hardness Vs. Density with lowess line"}
plot(Hardness ~ Density, data = janka, col = "darkblue", pch = 16)
lines(lowess(janka$Hardness ~ janka$Density), col="red")
```

And now a similar graph, but made with trellis, and with lowess line added:
```{r,fig.cap="Scatterplot of Hardness Vs. Density with lowess linear fit using trellis "}
xyplot(Hardness ~ Density, data = janka,
       panel = function(x, y, ...){
         panel.xyplot(x, y, col = "gray", pch = 16, ...)
         panel.lmline(x, y, col = "red", lwd = 2)
         panel.loess(x, y, degree = 2, span = 0.8, col = "blue", lwd = 2)
       }
)
```
It seems that a simple linear model, or, at most, a slightly curved model, could be used to fit the data.

#### Inference and models

Let we start with a simple linear model
```{r}
fm1 = lm(Hardness ~ Density, data = janka)
summary(fm1)
```
Parameters are all highly significant.

Now a residual plot to check correctness of model:
```{r,fig.show='hold',fig.cap="Residual plot of straight linear model"}
op=par(mfrow=c(2,2))
plot(fm1)
par(op)
```

One observation (32) seems to be an outlier. Residuals would also suggest to add the square of explanatory variable to the model.  
We will try to "increase" the model until a good result is obtained.

Zoom with trellis to first plot on residuals:
```{r,tidy=FALSE,fig.cap="Residuals Vs. fitted values using trellis"}
xyplot(residuals(fm1) ~ fitted(fm1)  | "Hardness ~ Density Residuals Plot",
       xlab = "Fitted Values", ylab = "Residuals",
       panel = function(x, y, ...){
         panel.xyplot(x, y, col = "gray", pch = 16)  
         panel.abline(0, 0, col = "red", lty = 2)
         panel.loess(x, y, span = .8, degree = 2, col = "blue", lwd = 2)
       }
)
```

Since a curvature (and, perhaps, heteroscedasticity, but we will be analyze this later) appear in residuals, a quadratic term is added to model:
```{r}
fm2 = update(fm1, . ~ . + I(Density^2))
```

Wald's $t$-test p-value's are:
```{r}
summary(fm2)
```
We can ask R to show only a part of the summary
```{r}
summary(fm2)$coef
```

And the ANOVA for nested models is:
```{r}
anova(fm2,fm1)
```
The contribution of quadratic term is significant.

First degree coefficient is not significant: the axis of parabola may be in 0.  
Let us try to add a cubic term:
```{r}
fm3 = update(fm2, . ~ . + I(Density^3))
summary(fm3)$coef

anova(fm2,fm3)
```
Adding third degree term does not add significant contributions to model.

In this case intercept is not significant.  
However, because of marginality principle, when a p-_th_ degree term of an explanatory variable is significant, statisticians tend to keep all the lower-degree terms (intercept, second degree, third degree, ..., (p-1)-_th_ degree) of the same exploratory variable.  
The same for interactions: if we keep a three-way interaction, also all two-way interactions and main effects of variables included in three-way interaction should we keep in the model.

If, for example, a linear effect within a more complex model is not significant, this does not necessarily mean that the linear effect does not exist. Sometimes (or, perhaps better, frequently), the non-significancy of a term could be simply due to the design or to the chosen measurement unit.  
Consequently, based on marginality principle, it does not make sense to check significancy of main effects (or lower order effects) if interaction effects (or higher order effects) involving main effects are significant.

In this case, if we try to transform the independent variable centering it around its median with:
```{r}
janka = transform(janka, normDensity = Density - median(Density))
```
or, equivalently, with:
```{r}
janka$normDensity = janka$Density - median(janka$Density)
```
<!--- Non funziona
Or, again, with:
```{r}
with(janka, normDensity <- Density - median(Density))
head(janka)
```
--->

This is the plot of relation with trellis:
```{r,tidy=FALSE,fig.cap="Hardness Vs. normalized (shifted on median) Density plot using trellis"}
xyplot(Hardness ~ normDensity, data = janka,
       panel = function(x, y, ...){
         panel.xyplot(x, y, col = "gray", pch = 16, ...)
         panel.lmline(x, y, col = "red", lwd = 2)
         panel.loess(x, y, degree = 2, span = 0.8, col = "blue", lwd = 2)
       }
)
```

The general "behavior" of relation is mainly the same, but the "normalized" model estimates are:
```{r}
fmd = lm(Hardness~normDensity+I(normDensity^2), janka)
summary(fmd)$coef
```
Here, the second degree coefficient is identical to the previous one, 
whereas intercept and first degree coefficient are now significant.  
This modified result is obtained only shifting the explanatory variable.
Significativity of lower degree terms may then depend on measurement scale or design.

Finally, notice also that centering the explanatory variables is a tool to "reduce collinearity", and then to reduce correlation between coefficient estimates:
```{r}
summary(fm2,correlation=TRUE)$correlation
summary(fmd,correlation=TRUE)$correlation
```
Now, we make other checks with studentized residuals (see Appendix):
```{r}
pl1 = xyplot(studres(fmd) ~ fitted(fmd) | "Residuals vs Fit",
             aspect = 1.6,
             panel = function(x, y, ...) {
               panel.xyplot(x, y, col = "darkgray", pch = 16,cex = 1.2,...)
               panel.loess(x, y, col = "navy", lwd = 2, degree = 2, span = .8)
               panel.abline(h = 0, lty = 4, col = "red")
             }, 
             xlab = "Fitted values", ylab = "Residuals"
)
```

```{r}
pl2 = qqmath(~ studres(fmd) | "Residuals Normal Probability Plot", aspect = 1.6,
             panel =  function(x, y, ...) {
               panel.qqmath(x, y, col = "darkgray", pch = 16, cex = 1.2,...)
               panel.qqmathline(x, dist = qnorm, col = "red", lty=4)
             }, 
             xlab = "Normal scores",ylab = "Sorted studentized residuals"
)
```

```{r,fig.cap="Studentized residuals Vs. fitted and Normal probability plot using trellis"}
print(pl1, split= c(x=1,y=1,nx=2,ny=1), more = T)
print(pl2, split= c(x=2,y=1,nx=2,ny=1), more = T)
```

Residual variability seems non constant (heteroscedasticity exists), and seems to increase as the response variable increases. We should search for a transformation of the response variable which produces models with "more normal" (and with reduced heteroscedasticity) residuals.

We will search for the transformation within the Box-Cox family (see the Appendix):
```{r,fig.cap="Box-Cox log-likelihood Vs. Lambda"}
bxcx=boxcox(fmd,lambda = seq(-0.25, 1, len=20))
```
`boxcox()` returns a plot with the MLE of $\lambda$ and a confidence interval for it, identified by the intersections of horizontal dotted line with log-likelihood.
When 0 is included in the confidence interval, often $\lambda$=0 (the logarithm) is an advisable choice.

We then re-analyze the model using the log-transform of the dependent variable:
```{r}
lfmd = lm(log(Hardness) ~ normDensity + I(normDensity^2), janka)
```
or, equivalently,
```{r}
lfmd = update(fmd, log(.) ~ .) 
```

The coefficients significances:
```{r}
round(summary(lfmd)$coef, 4)
```
The coefficients are all significant.

And finally (studentized) residual check (in trellis style)
```{r,tidy=FALSE,fig.cap="Studentized residual plot using trellis"}
pl1 = xyplot(studres(lfmd) ~ fitted(lfmd) | "Residuals vs Fit",
             aspect = 1.6,
             panel = function(x, y, ...) {
               panel.xyplot(x, y, col = "darkgray", pch = 16,cex = 1.2,...)
               panel.loess(x, y, col = "navy", lwd = 2, degree = 2, span = .8)
               panel.abline(h = 0, lty = 4, col = "red")
               panel.grid()
             }, 
             xlab = "Fitted values", ylab = "Residuals"
)

pl2 = qqmath(~ studres(lfmd) | "Residuals Normal Probability Plot", aspect = 1.6,
             panel =	function(x, y, ...) {
               panel.qqmath(x, y, col = "darkgray", pch = 16, cex = 1.2,...)
               panel.qqmathline(x, dist = qnorm, col = "red", lty=4)
               panel.grid()
             }, 
             xlab = "Normal scores",ylab = "Sorted studentized residuals"
)

graphics.off()
print(pl1, split= c(x=1,y=1,nx=2,ny=1), more = T)
print(pl2, split= c(x=2,y=1,nx=2,ny=1), more = T)
```
Plots are improved and this last model seems the best one.  
Now, since the response is the logarithm of y, contributions of the explanatory variables are multiplicative with respect to y.

Now residual check in "classic" style:
```{r,fig.show='hold',fig.cap="Studentized residuals Vs. Fit and Normal Probability Plot of studentized residuals"}
op = par(mfrow = c(1,2))
plot(fitted(lfmd) , studres(lfmd) , main =  "Residuals vs Fit", pch = 16, col = "darkgray",
     xlab = "Fitted Values", ylab = "Studentized Residuals")
abline(h = 0, col = "red", lty = 4)
loe = predict (loess(studres(lfmd)~fitted(lfmd)))
lines(fitted(lfmd), loe, col = "darkblue", lty = 3, lwd = 2)
qqnorm(studres(lfmd))
qqline(studres(lfmd))
par(op)
```

And standard residual check:
```{r,fig.show='hold',fig.cap="'Standard' residual plot on final model"}
op = par(mfrow = c(2,2))
plot(lfmd)
par(op)
```

Now the plot of models predictions (in log-scale and in original scale) with 95% Prediction Intervals.  
```{r,fig.show='hold',tidy=FALSE,fig.cap="Final model predictions with 95% intervals in log-scale and original scale"}
d = janka$normDensity
h = log(janka$Hardness)
pred = data.frame(predict(lfmd, interval = "prediction"))
op = par(mfrow = c(1, 2))
plot(d, h, pch = 16, col = "darkgray", cex = 1.5, 
     xlab = "Normalized Density", ylab = "log (Hardness)") 

lines(d, pred$fit, col = "red")
lines(d, pred$lwr, col = "green", lty = 4)
lines(d, pred$upr, col = "green", lty = 4)
grid(col = "darkgray")
plot(d, exp(h), pch = 16, col = "darkgray", cex = 1.5, 
     xlab = "Normalized Density", ylab = "Hardness")
lines(d, exp(pred$fit), col = "red")
lines(d, exp(pred$lwr), col = "green", lty = 4)
lines(d, exp(pred$upr), col = "green", lty = 4)
grid(col = "darkgray")
par(op)
```

\newpage

### Example 12: Oxidant Vs. meteo vars regression
```{r,echo=FALSE}
rm(list = ls())
```

#### Data description

Following data returns levels of an air pollutant, "Oxidant",
together with levels of four meteorological variables
recorded on 30 days during one summer.
Which, if any, of the four indipendent variables seem to be related to
levels of oxidant?

#### Data loading
```{r}
load("oxidant.Rda")
str(oxidant)
head(oxidant)
```
#### Descriptives
Let us start producing a matrix of scatterplots:
```{r,fig.cap="Matrix of scatterplots between metereological variables and also Oxidant"}
pairs(oxidant, panel = panel.smooth, pch = 16)
```
The above line of code draws a matrix of scatterplots between all possible pairs of variables.  
Some relations between `oxidant` and the independent variables appear.  
We will use the models to asses which independent variables actually influence the dependent variable.

#### Inference and models
We begin estimating a linear model with all the independent variables, excluding `day`. 
```{r}
fm1 = lm(oxidant ~ windspeed+temperature+humidity+insolation, data = oxidant)
summary(fm1)
```
The model seems to fit data well enough (R-square equal to 0.789). Now let us try to "expand" the initial model adding all two-way interactions.
```{r}
fm2 = lm(oxidant ~ (windspeed+temperature+humidity+insolation)^2, data = oxidant)
summary(fm2) 
```
The Wald statistics of two-way interactions are all non significant. Now we will check if all interactions together are not significant, comparing the two above models:
```{r}
anova(fm2, fm1, test = "F")
```
There is a non-significant difference between the two models (adding all interaction effects seems to not produce better fit).
`fm1` seems the better model.
```{r}
summary(fm1)
```

Let us verify if the `humidity:insolation` interaction (the one with smaller p-value) added alone is always non-significant.
```{r}
fm3=update(fm1,.~.+humidity:insolation)
summary(fm3)
```
The `humidity:insolation` interaction is significant!
`fm3` could be the best model.

Now the usual residuals checks:
```{r,fig.show='hold',fig.cap="Residual plot of first model rescribing Oxidant"}
op = par(mfrow = c(2, 2))
plot(fm3)
par(op)
```

In case of models with many explanatory variables a useful tool to check residuals is to compare the residuals also with the explanatory variables.  
Now, plot all pairs with residuals
```{r,fig.cap="Matrix of scatterplot between all variables and also residuals"}
pairs(cbind(oxidant,residuals(fm3)), panel = panel.smooth, pch = 16)
```
Looking at the residuals versus the explanatory variables scatterplots (last row of matrix), it seems that `humidity^2` may be useful to explain `oxidant`:
```{r}
fm4=update(fm3,.~.+I(humidity^2))
summary(fm4)
```
Maybe, many other coefficients (humidity, insolation and the interaction between them) were significant
only because the square of humidity was not included in the model.  
Let us first try to drop the interaction:
```{r}
fm4=update(fm4,.~.-humidity:insolation)
summary(fm4)
```
Now the model is simpler, and `insolation` seems not significant.

Now let us try to remove non-significant effects.  
Always remove terms one-by-one :
```{r}
fm = update(fm4, .~.-insolation)
summary(fm)
```
Now all coefficients (except for intercept) are significant.

We make further checks with overparametererization, adding an interaction:
```{r}
anova(fm, update(fm , .~.+windspeed:temperature), test = "F")
```
Adding the new term does not add any useful information to model.
<!---
```{r}
dfpred = data.frame(
  windspeed = rep(mean(oxidant$windspeed), 100),
  temperature = rep(mean(oxidant$temperature), 100),
  humidity = 1:100
)

resp = predict(fm, newdata = dfpred)

plot(resp ~ dfpred$humidity, type = "l")

class(resp)
```

su richiesta dei corsisti (da intervento di Nicola): come vedere il singolo effetto dell'umidità
```{r}
dfpred = data.frame(windspeed=rep(mean(oxidant$windspeed),20),
                    temperature=rep(mean(oxidant$temperature),20),
                    humidity=seq(min(oxidant$humidity),max(oxidant$humidity),length=20))
(resp = predict(object=fm, newdata=dfpred))
dfpred$resp = resp
plot(resp ~ humidity, type="l", data=dfpred)
```
--->

`fm` seems the best model. Let us check the model with usual residuals plots:
```{r,fig.show='hold',fig.cap="Residual plot for second model"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```
Residuals plots seem better than before.

And now plot all pairs to check if some other relation may emerge.
```{r,fig.show='hold',fig.cap="Matrix of scatterplot between variables and also residuals"}
pairs(cbind(oxidant,residuals(fm)), panel = panel.smooth, pch = 16)
```
The model seems good enough.

<!--- More analysis could be performed to deep relations.. --->
\newpage

### Example 13: Calories Vs. categorical and continuous indep. vars regression
```{r,echo=FALSE}
rm(list = ls())
options(contrasts = c("contr.treatment", "contr.poly"))
```

#### Data description

Data contains calories and sodium content for three types of hot dogs:
20 Beef, 17 Meat and 17 Poultry hot dogs.  
This data is a sample from much larger populations.
We want to find if `type` of hotdog and `sodium` content may influence the `calories` contents in hot dogs themselves.

#### Data loading
```{r}
load("hotdogs.Rda")
str(hotdogs)
head(hotdogs)
```
#### Descriptives
Now let us produce a boxplot for each type of hot dog:
```{r,fig.cap="Box and whisker plot of calories by type of hot dog"}
boxplot(calories~type, data=hotdogs)
```
Some difference appears between types of hot dogs. `Beef` and `meat` hot dogs seem similar.  
Following graphs represent the relation between `calories` and `sodium` without and with categorization per `type`. Splitting on categorization is then performed in several ways.
```{r,tidy=FALSE,fig.cap="Simple scatterplot of calories Vs. sodium"}
plot(calories~sodium,data=hotdogs)
```
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium with types with different colors"}
plot(calories~sodium, data=hotdogs, col=type)
legend("topleft",legend=unique(hotdogs$type),pch=rep(1,3),
       col=as.numeric(unique(hotdogs$type)))
```
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium with types with different symbols"}
plot(calories~sodium, data=hotdogs, pch=as.numeric(type))
legend("topleft",legend=unique(hotdogs$type),
       pch=as.numeric(unique(hotdogs$type)))
```
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium with types with different colors and symbols"}
plot(calories~sodium,data=hotdogs,col=type,pch=as.numeric(type))
legend("topleft",legend=unique(hotdogs$type),
       pch=as.numeric(unique(hotdogs$type)),
       col=as.numeric(unique(hotdogs$type)))
```
An increasing relationship between `calories` and `sodium` seems to exist with differences with respect to `type`s.

Plot `calories` vs `sodium` by `type` using trellis graph:
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium by type using trellis"}
xyplot(calories ~ sodium | type, data = hotdogs, 
  layout = c(3, 1), aspect = 1.4,
  strip = strip.custom(strip.names = TRUE, 
                       strip.levels = TRUE),
  par.strip.text = list (cex = 2),
  panel = function(x, y , ...){
    panel.xyplot(x, y, pch = 16, cex = 1.2, col = "darkgray", ...)
    panel.loess(x, y, ...)
    panel.lmline(x, y, col = "violet", lty = 2, ...)
    panel.grid()
  }
)
```

#### Models and inference
Let us start with the estimation of full model, with main effects and interactions:
```{r}
fm = lm(calories ~type*sodium, data = hotdogs) 
summary(fm)
summary.aov(fm) 
```
The model includes the intercept, a coefficient for the dummy variable for `meat` intercept, a coefficient for the dummy variable for `poultry` intercept, the
`sodium` variable coefficient, the interaction coefficients between `type` and `sodium` which tell us how much `sodium` slope coefficient varies when `type` is `meat` or `poultry` instead of `beef`.  
`sodium` and `type` coefficients are significant, the interaction between `sodium` slope and `type` is not.  
p-values of `aov` table suggest a three parallel regression lines model.

Notice that, since the design in unbalanced, we obtain different results switching the order of independent variables in model formulation.
```{r}
fm = lm(calories ~ sodium * type, data = hotdogs) 
summary(fm)
summary.aov(fm) 
```
p-value for interaction remains always the same in this case, and is non significant. The p-values of main effects, however, are always less than $\alpha=$ 0.05.  
Now we will remove the interaction term:
```{r}
fm = update(fm, .~.-sodium:type) 
summary.aov(fm)
summary(fm)
```
<!--- Mettere anche formula modello? # E(Yi) = mu + deltaMeat*x1 + deltaPpoultry*x2 --->
The difference in calories contents between `meat` and `beef` hot dogs is non significant.  
However, now we will try to change parameterization to make the model more readable:
```{r}
fm = lm(calories ~ sodium + type - 1, data = hotdogs)
```
This parameterization for `fm` may generally be used to better understand the meaning of model and parameters values.  
In such as formulation, `summary.aov(fm)` usually does not make sense because it compares the model with 0 gran mean with the analyzed model.
```{r}
summary(fm)
```
The parameters of model specified in this manner are more easily interpretable: there are three parallel straight lines with slope equal to 0.2022, and intercepts equal to 75.7350, 74.0767, 25.9521 for `beef`, `meat`, and `poultry`, respectively.

Since `beef` and `meet` hot dogs seem similar, we try to compare their intercepts using Helmert-type contrasts:
```{r}
contrasts(hotdogs$type)=matrix(c(-1,-1,1,-1,0,2),ncol=2,nrow=3,byrow=TRUE) 
fm3 = lm(calories ~ sodium + type, data = hotdogs)
summary(fm3)
```
<!--- model.matrix(fm3) --->
`type1` coefficient represents the comparison between `beef` and `meet` intercepts.  
Since `type1` coefficient p-value is greater than $\alpha$=0.05, `beef` and `meet` hot-dogs can be considered equal.  
On the contrary, `type2` coefficient is the difference between `poultry` intercept and `meet` and `beef` pooled intercept effect. `type2` is quite significant, and then `poultry` hot dog type may be considered different from the other (pooled) types.

Let us try to recode factor type to obtain only two types of hot-dogs.
```{r}
hotdogs$type2 = as.character(hotdogs$type)
hotdogs$type2[hotdogs$type2 == "beef"] = "meat"
hotdogs$type2 = factor(hotdogs$type2, levels = c("meat", "poultry"))
```

<!---
or:
```{r}
levels(hotdogs$type)
hotdogs$type3 = hotdogs$type
levels(hotdogs$type3)[1] = "meat"
levels(hotdogs$type3)
table(hotdogs$type)
table(hotdogs$type3)

levels(hotdogs$type)[1] = "meat"
```
--->

And now the new model is fitted to data:
```{r}
fm1 = lm(calories ~ sodium + type2, data = hotdogs)
summary(fm1)
```

Next two lines of code compare residual standard deviation of model with three types of hot-dogs with residual standard deviation of model with two types of hot-dogs:
```{r}
summary(fm)$sigma
summary(fm1)$sigma
```
They are very similar (the residual standard deviation of "reduced" model is actually smaller), and this confirms that `meat` and `beef` hot-dogs are really similar, and may be considered equal in terms of relation between calories and sodium.

#### Model diagnostics
The usual diagnostic plots for the model with three and two types of hot dogs, respectively, are reported below. 
```{r,fig.show='hold',fig.cap="Residual plot of model with distinct types for meat and beef"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```

```{r,fig.show='hold',fig.cap="Residual plot of model with pooled types for meat and beef"}
op = par(mfrow = c(2, 2))
plot(fm1)
par(op)
```

Either graphs report that the residuals are "very good".

\newpage

### Example 14: Toy model with Istat data
```{r,echo=FALSE}
rm(list = ls())
```

#### Data description

Data contain weight, height, gender and geographical area ("Nord", "Centro",
"Sud" and "Isole") from 1806 Italian people.  
The main goal of this analysis is on finding a simple model that fits the relation between height and weight of italian people.

#### Data loading
```{r}
load("istat.Rda")
str(istat)
head(istat)
```

#### Descriptives
Initially, we produce a trellis plot that shows the relationship between `Weight` and `Height`:
```{r,fig.cap="Scatterplot of Weight Vs. Height"}
xyplot(Weight ~ Height, data = istat, pch = 16)
```
And then the above trellis plot by gender with regression lines:
```{r,tidy=FALSE,fig.cap="Scatterplot of Weight Vs. Height by Gender using trellis"}
xyplot(Weight ~ Height | Gender, data = istat,
       panel = function(x, y , ...){
         panel.xyplot(x, y, pch = 16, ...)
         panel.lmline(x, y, col = "red", lwd = 2, ...)
       }
)
```
Now, a Trellis plot by area and gender with regression lines:
```{r,tidy=FALSE,fig.cap="Scatterplot of Weight Vs. Height by Area and Gender using trellis"}
xyplot(Weight ~ Height | Area*Gender, data = istat,
       layout = c(4, 2), aspect = 1.3,
       panel = function(x, y , ...){
         panel.xyplot(x, y, pch = 16, ...)
         panel.lmline(x, y, col = "red", lwd = 2, ...)
       }
)
```

#### Inference and models

After having seen the above graphs, we will try a model with all two-way interactions.
```{r}
fm = lm(Weight ~ (Height + Gender + Area)^2, data = istat)
summary.aov(fm)
```

Now we simplify the model by removing all non significant interactions and checking if their global contribution is significant:
```{r}
fm = lm(Weight ~ Height + Gender + Area + Height:Gender, data = istat)
summary.aov(fm)

anova(fm,update(fm, .~. +Height:Area+Gender:Area))
```

The simplified model assesses that the slope between `Height` and `Weight` is modified by `Gender` only. Differences in intercepts appear, due to `Area` and `Gender`.

Now we plot the diagnostic graphs.
```{r,fig.show='hold',fig.cap="Residual plot of first model"}
op = par(mfrow = c(2, 2))
plot(fm)
par(op)
```
The residuals are not symmetric.

We re-plot trellis graph by gender with smoothing as reference for modelling.
```{r,fig.cap="Scatterplot of Weight Vs. Height by Gender with lowess and fitted line using trellis"}
xyplot(Weight ~ Height | Gender, data = istat,
       panel = function(x, y , ...){
         panel.xyplot(x, y, pch = 16, ...)
         panel.lmline(x, y, col = "red", lwd = 2, ...)
         panel.loess(x, y, degree = 2, span = .5, col = "green", lwd = 2, ...)
       }
)
```

Now we try to transform data by using Box-Cox to correct skewness of residuals, and an apparent non-linearity in relation.  
This is the model:
```{r}
fm = lm(Weight ~ Area + Height * Gender, data = istat)
```

And then the Box-Cox transformation analysis:
```{r,fig.cap="Box-Cox likelihood graph for Istat toy model"}
boxcox(fm, lambda = seq(-0.5, 0.5, 1/20))
grid()
```
A transformation with $\lambda=-\frac{1}{5}$ seems a good choice.  
This is the model on transformed scale:
```{r}
fmL = update(fm, Weight^(-0.2)~.)
summary.aov(fmL)
```
Let us remove `Area`, that seems not significant, to complete the analysis.
```{r}
fmL = update(fmL, .~.-Area)
```

(Note: Is this choice completely correct? Is Area surely non significant?)

#### Model diagnostics
```{r,fig.show='hold',fig.cap="Resdial plot for Istat toy model"}
op = par(mfrow = c(2, 2))
plot(fmL)
par(op)
```

Residuals are not really normal, but as a "toy" model, this result can be considered acceptable enough.  
Now we calculate the predicted values on transformed scale:
```{r}
newdata = data.frame(Height = c(150:199, 150:199), Gender = factor(rep(c("Female", "Male"),
                                                                       each = 50)))
pred = predict(fmL, newdata = newdata, se = T, interval = "prediction")
```

And the predicted values with prediction interval on transformed scale:
```{r}
newdata$fit = pred$fit[,1] # predicted values
newdata$lwr = pred$fit[,2] # UPL
newdata$upr = pred$fit[,3] # LPL
ylim = range(istat$Weight^(-0.2))*c(0.95, 1.05) # compute y range
```

Now the predictions on transformed scale are plotted:
```{r,fig.show='hold',tidy=FALSE,fig.cap="Plot of model predictions by Gender with 95% prediction intervals on transformed scale"}
op = par(mfrow = c(1, 2))
# plot for male
plot(Weight^(-0.2) ~ Height, data = istat[istat$Gender == "Male",], 
     pch = 16, col = "gray", main = "Male", ylim = ylim)
# add prediction confidence interval for male
with(newdata, lines(Height[Gender == "Male"], fit[Gender == "Male"], 
                    col = "blue")) # predicted values
with(newdata, lines(Height[Gender == "Male"], upr[Gender == "Male"], 
                    col = "blue", lty = 2)) # UPL
with(newdata, lines(Height[Gender == "Male"], lwr[Gender == "Male"], 
                    col = "blue", lty = 2)) # LPL
# plot for female
plot(Weight^(-0.2) ~ Height, data = istat[istat$Gender == "Female",], 
     pch = 16, col = "gray", main = "Female", ylim = ylim)
# add prediction confidence interval for female
with(newdata, lines(Height[Gender == "Female"], fit[Gender == "Female"], 
                    col = "red")) # predicted values
with(newdata, lines(Height[Gender == "Female"], upr[Gender == "Female"], 
                    col = "red", lty = 2)) # UPL
with(newdata, lines(Height[Gender == "Female"], lwr[Gender == "Female"], 
                    col = "red", lty = 2)) # LPL
par(op)
```

Now we prepare some data to plot preditions on original scale
```{r}
ylim = range(istat$Weight * c(0.95, 1.05))
```

And then the predictions are actually plotted:
```{r,fig.show='hold',tidy=FALSE,fig.cap="Plot of model predictions by Gender with 95% prediction intervals on original scale"}
op = par(mfrow = c(1, 2))
# plot for male
plot(Weight ~ Height, data = istat[istat$Gender == "Male",], 
     pch = 16, col = "gray", main = "Male", ylim = ylim)
with(newdata, lines(Height[Gender == "Male"], (fit[Gender == "Male"])^(-5), 
                    col = "blue")) # predicted values
with(newdata, lines(Height[Gender == "Male"], (upr[Gender == "Male"])^(-5), 
                    col = "blue", lty = 2)) # UPL
with(newdata, lines(Height[Gender == "Male"], (lwr[Gender == "Male"])^(-5), 
                    col = "blue", lty = 2)) # LPL
# plot for female
plot(Weight~Height, data = istat[istat$Gender == "Female",], 
     pch = 16, col = "gray", main = "Female", ylim = ylim)
with(newdata, lines(Height[Gender == "Female"], (fit[Gender == "Female"])^(-5), 
                    col = "red")) # predicted values
with(newdata, lines(Height[Gender == "Female"], (upr[Gender == "Female"])^(-5), 
                    col = "red", lty = 2)) # UPL
with(newdata, lines(Height[Gender == "Female"], (lwr[Gender == "Female"])^(-5), 
                    col = "red", lty = 2)) # LPL
par(op)
```

### Example 15: Iowa example
```{r,echo=FALSE}
rm(list = ls())
```
#### Data description
The Iowa dataset is a toy example that summarizes the yield of wheat (bushels per acre)
for the state of Iowa between 1930-1962. In addition to yield, year, rainfall and temperature were recorded as the main predictors of yield:  
`Year`:  Year of harvest  
`Rain0`:	Pre-season rainfall  
`Temp1`:	Mean temperature for growing month 1  
`Rain1`:	Rainfall for growing month 1  
`Temp2`:	Mean temperature for growing month 2  
`Rain2`:	Rainfall for growing month 2  
`Temp3`:	Mean temperature for growing month 3  
`Rain3`:	Rainfall for growing month 3  
`Temp4`:	Mean temperature for harvest month  
`Yield`:	Yield in bushels per acre  

We want to find a first descriptive model that approximates the relation between Yield and the other variables.

#### Data loading
```{r}
load("iowheat.Rda")
str(iowheat)
head(iowheat)
```

#### Descriptives
We begin the analysis producing a matrix of scatterplots:
```{r,fig.cap="Matrix of scatterplot between dataframe variables"}
pairs(iowheat, pch = 16, col = "darkblue")
```
Some relations seem to appear. Now we will try to produce some models:

#### Inference and models
Let us start with a second degree model:
```{r}
fmA = lm(Yield ~ .^2, data = iowheat)
fmA
```
Unfortunately, there are more parameters than observations:
```{r}
dim(iowheat)
```
and then we cannot estimate all the parameters.

We then try with a second model that uses first degree effects only:
```{r}
fmA = lm(Yield ~ ., data = iowheat)
summary(fmA)
```

Now let us try to use automatic model selection procedures.

In a first attempt a lower stepwise procedure is used. Such a procedure tries to build an "optimum" model starting from a (almost) complete model and then removing the non significant terms with a step-wise fashion, one by one.  
During the process, if some removed term become significant, it is re-inserted in model.

In this case, the model must always include `Year` as an explicative variable (see the `scope` parameter).
```{r}
lowerStepAICfm = stepAIC(fmA, scope = list(lower  =  ~Year))
summary(lowerStepAICfm)
```
The final model has the terms: `Year`, `Rain0`, `Rain2`, and `Temp4`.

A second attempt with a upper stepwise procedure is then tried. Such a procedure tries to build an "optimum" model starting from a "only mean" model and then adding significant terms, one by one, with a step-wise fashion.  
During the process, if some added term become non significant, it is removed from model.  
In this case, the maximal model has all two-way interactions (see the `scope` parameter again):
```{r}
upperStepAICfm = stepAIC(lowerStepAICfm, scope = list(upper  =  ~.^2))
summary(upperStepAICfm)
```
The final model has the terms: `Year`, `Rain0`, `Rain2`, `Temp4`, `Rain2:Temp4`. It is slightly more complex than the previous one.

#### Diagnostic residual analysis
The model arising from the upper stepwise process is then the final one, and then residual analysis follows:
```{r,fig.show='hold',fig.cap="Residual plot of model"}
op = par(mfrow = c(2,2))
plot(upperStepAICfm)
par(op)
```

\newpage

Some Theory on Linear Models
----------------------------
Next paragraphs contains some basic information to understand the theory of Linear Models, and their application in R.

### Smoothing
Regression is the study of the change of the distribution of one variable, $y$, according to the value of another variable, $x$.
Both continuous and discrete variables can be included in regression problems.

Formally, a relationship between $x$ and $y$ usually means that the expected value of $y$ is different for different values of $x$.
At this stage, changes in $\sigma^2$ (the variance) or other aspects of the distribution are not considered.

When $y$ is continuous, changes in $y$ are smooth (continuous and differentiable), and the model used is:
$$
E(y \vert x) = g(x)
$$
for some unknown smooth function $g(\cdot)$.

### Regression
A variety of functions can be used to estimate $g(\cdot)$, called **regression function**. These functions are called **scatterplot smoothers**.

For a pair $(x_i, y_i)$, $i = 1, \dots, n$ the model states that:
      $$
      y_i = g(x_i) + \varepsilon_i
      $$

Where:

* $n$ is the sample size, or the number of measured units used to estimate the model. 
* $\varepsilon_i \; (i = 1, \dots, n)$ are $n$ $\text{ i.i.d.}$ random values arising from a Normal distribution with 0 as expected value and $\sigma$ as standard deviation ($\varepsilon_i \sim N(0,\sigma^2)$)

### Linear model
In general, a linear model may be expressed in the form:  
      $$
      E(y_i)\equiv E(y_i\vert\underline{x}_i) = \beta_0 + \displaystyle\sum_{j=1}^p \beta_j x_{ij}
      $$
And the above formula can be rewritten as  
      $$
      \underline{\mu} = E(\underline{y}) = \underline{X} \underline{\beta}
      $$
where $\underline{X}$ is usually a $n \times (p+1)$ matrix which includes all ones in the first column (the intercept column) and the $p$ explanatory variables in the other columns; $\underline{\beta}$ is a vector of $\beta_0, \beta_1, \dots, \beta_p$.

### Deterministic and random components
The general form of the linear model is then:
      $$
      \underline{y} = \underline{X} \underline{\beta} + \underline{\varepsilon}
      $$
or    
      $$
      y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \dots + \beta_p \cdot x_{ip} + \varepsilon_i
      $$
or
      $$
      y_i \sim N(\mu_i, \sigma^2)
      $$  
with 

$$
\mu_i = \beta_0 + \sum_j \beta_j x_{ij} \text{ and } \varepsilon_i \sim N(0, \sigma^2) \text{ i.i.d.}.
$$

This model has then $p+2$ unknown parameters: $\beta_0, \beta_1, \dots, \beta_p, \sigma^2$, and:

$$
\underline{X}\underline{\beta} = \underline{\mu}
$$  

is the **systematic** part, or **deterministic** part, or **signal**; it represents the explanaible differences between populations.  
The value:

$$\underline{\varepsilon}=(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)^T$$

is the **random** or **noise** component. 

Finally, following relationship between _response_, _signal_ and _noise_ holds:

 $\phantom{is the random or noise comp}$ **response** $=$ **signal** $+$ **noise**.

**Summarizing**: models where

* signal is a linear function of parameters
* response is a linear function of signal and noise

are said **linear models**. 

### Maximum Likelihood estimate
The likelihood function is the joint probability density function of observed values:  
       $\phantom{aaaaaaaaaaaa}L(\beta_0, \dots, \beta_p, \sigma^2) = \displaystyle\prod_{i=1}^n p(y_i\vert\beta_0, \dots, \beta_p, \sigma^2)$  
            $\phantom{aaaaaaaaaaaaL(\beta_0, \dots, \beta_p, \sigma^2) }= \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2}\left(\frac{y_i-\mu_i}{\sigma}\right)^2}$  
            $\phantom{aaaaaaaaaaaaL(\beta_0, \dots, \beta_p, \sigma^2) }= \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}{\left[y_i-\left(\beta_0+\sum_j \beta_j x_{ij}\right)\right]}^2}$  
            $\phantom{aaaaaaaaaaaaL(\beta_0, \dots, \beta_p, \sigma^2) }= (2\pi\sigma^2)^{-\frac{n}{2}} e^{-\frac{1}{2\sigma^2} \sum_i\left[y_i-\left(\beta_0+\sum_j \beta_j x_{ij}\right)\right]^2}$

but evaluated with respect to unknown parameters.

**MLE** (Maximum Likelihood Estimate) is obtained by taking the log of the likelihood expression  

$\phantom{aaaaaaaaaaaa}\ell(\beta_0, \dots, \beta_p, \sigma^2) = \log L(\beta_0, \dots, \beta_p, \sigma^2) \\  
      \phantom{aaaaaaaaaaaa\ell(\beta_0, \dots, \beta_p, \sigma^2) } = c - \frac{n}{2} \log\sigma^2 -\frac{1}{2\sigma^2} \sum_i\left[y_i-(\beta_0+\sum_j \beta_j x_{ij})\right]^2,$  
      
(the so-called **log-likelihood**), differentiating with respect to each parameter in turn, setting the derivative equal to 0 and solving.  
This obtains a linear equations system:  
      $$
      \begin{cases} 
          \frac{1}{\hat{\sigma}^2} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right] = 0 \\
          \frac{1}{\hat{\sigma}^2} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right] x_{i1} = 0 \\
          \hspace{2cm} \vdots \\
          \frac{1}{\hat{\sigma}^2} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right] x_{ip} = 0 \\
          -\frac{n}{\hat{\sigma}^2} + \frac{1}{\hat{\sigma}^3} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right]^2 = 0
        \end{cases}
      $$     

The first $p+1$ equations can be multiplied by $\hat\sigma^2$, yielding $p+1$ linear equations in the $p+1$ unknown $\hat\beta$'s.

Since they are linear, equations can be solved by linear algebra:
      $$
      \underline{\hat{\beta}} = (\underline{X}^T\underline{X})^{-1} \underline{X}^T \underline{y}
      $$
from which
      $$
      \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_p x_{ip}
      $$
where $\hat{y}_i$ are the **fitted values**.

Also, the values: 
$$
r_i = y_i - \hat{y}_i
$$
are said **residuals**, and are estimates of $\varepsilon_i$ (for this reason, sometime they are represented with $\hat\varepsilon_i$ symbol).

The MLE for $\sigma^2$, $\hat{\sigma}^2$, is found from the last equation, that can be rewritten as

$$
-\frac{n}{\hat{\sigma}} + \frac{1}{\hat{\sigma}^3}\sum_i{r_i^2} = 0
$$

from which  
$$
\hat{\sigma}^2_{ML} = \frac{1}{n} \sum_i r_i^2
$$  
Note that an unbiased, "non ML", and more often used  version of $\sigma^2$ estimate is: 
$$
\displaystyle{\hat{\sigma}^2 = \frac{1}{n-p-1} \sum_i r_i^2}.
$$

### Assumptions on linear model
As we have seen before, the main assumption on linear model is that the model itself is linear in parameters.

Other assumptions concerning the error terms $\underline{\varepsilon}$ and the "components" of linear models are then:  

* $E(\varepsilon_{i}) = 0 \,\,\,(i = 1, \dots, n)$
* $Var(\varepsilon_{i}) = \sigma^2  \,\,\,(i = 1, \dots, n )$ (homoscedasticity)
* $\underline{X}$ is not stochastic
* $\varepsilon_i \sim\; N(0,\,\sigma^2) \,\,\,(i = 1, \dots, n )$
* $Corr(\varepsilon_{i},\,\varepsilon_{i-j}) = 0 \,\,\,(i = 1, \dots, n;\, 0 < j < i)$ (incorrelation of residuals)

### Distribution of estimators
If linear model assumptions are met, $\underline{\hat{\beta}}$ are random variables with a known distribution (except for $\underline\beta$ and $\sigma$ parameter values).

Indeed, if the model is correctly specified, then 
$$
\underline{\hat\beta} \sim N\left(\underline\beta, \sigma^2{(\underline{X}^T\underline{X})}^{-1}\right).
$$

It is then possible:

1. To test hyphoteses of the form $H_0: \beta_j = \beta_j^*$.
2. To provide confidence intervals for the regression parameters $\beta_j$.

In particular, testing the hypothesis that $\beta_j = 0$ is of particular interest, as its rejection means that the $j$-th variable is significant, i.e. that it explains in some manner the behaviour of response variable, net of the other explanatory variables.

#### Test on one parameter

To test if $\beta_j$ is equal to some value $\beta_j^*$, $j = 0, 1, \dots, p$, the Wald $t$-test statistic can be used:
      $$
      t = \frac{\hat{\beta}_j-\beta_j^*}{\hat{se}(\hat\beta_j)} 
      $$
where $\hat{se}(\hat\beta_j)$ denotes the estimated standard error of the estimator $\hat{\beta}_j$ and is the square root of the $(j+1)$-th element of the diagonal of the matrix $\hat\sigma^2{(\underline{X}^T\underline{X})}^{-1}$, with $\hat\sigma^2 = \frac{1}{n-p-1} \sum_i r_i^2$, the unbiased estimate of $\sigma^2$.

When $H_0$ is true, $t$-test statistics follows a *Student's t* distribution with $n - p - 1$ degrees of freedom, where $p + 1$ is the number of estimated regression parameters.

#### Confidence interval on one parameter

Given the considerations about the distribution of previous $t$-test, the $(1 - \alpha)$ confidence interval for $\beta_j$ may then be calculated by replacing $\beta_j^*$ with $\beta_j$ in above Wald $t$-test formula, so obtaining the following:
  $$
  \left[\hat{\beta}_j - t_{n-p-1;\frac{\alpha}{2}} \cdot \hat{se}(\hat\beta_j);\, \hat{\beta}_j + t_{n-p-1;\frac{\alpha}{2}} \cdot \hat{se}(\hat\beta_j)\right]
  $$
where $t_{k;\gamma}$ is the $\gamma$-th right percentile of Students-t distribution with $k$ degrees of freedom.

### Comparing two nested models: $F$ test

An $F$ test can be used to compare two models and to choose one of them too: suppose we are interested to compare the "complete" model ($p +1$ parameters)
  $$
  y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_px_{ip}
  $$
with the "reduced" model ($p_0 +1$ parameters)
  $$
  y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_{p_0}x_{ip_0}\,,\quad p_0 < p
  $$
that is to verify the hypothesis $H_0: \beta_{p_0+1} = \beta_{p_0+2} = \ldots = \beta_{p} = 0$.  
The following $F$ test can be calculated:
  $$
  F = \frac{(SS_{R_0} - SS_{R_1})/(p-p_0)}{SS_{R_1}/(n-p-1)}
  $$
Where $SS_{R_0}$ and $SS_{R_1}$ are the residual Sum of Squares (see the next paragraph for definitions of Sum of Squares) for the reduced and the complete model, respectively.

If the null hypothesis holds, then $F \sim F_{p-p_0, n-p-1}$,

where $F_{a, b}$ is the Fisher-Snedecor distribution with $a$ (numerator) and $b$ (denominator) degrees of freedom.

If the previous $F$ test refuses $H_0$, the complete model is maintained; otherwise, if the $F$ test does not refuse $H_0$, the reduced model is used, since the last $p-p_0$ variables result not significant in order to explain the response variable.

Notice that using the formula above, the $F$ test simply establishes if the difference (in terms of residual variability $SS_R$) between nested models is significant or not.

### Determination coefficient $R^2$
The **Determination Coefficient** is defined as:
$$
R^2 = 1 - \frac{SS_{R}}{SS_{TOT}} = \frac{SS_E}{SS_{TOT}}\,,
$$

where $SS_R = \sum_i r_i^2$ is the residual sum of squares (residual deviance), $SS_E = \sum_i {(\hat{y}_i - \overline{y})^2}$ is the regression sum of squares (explained deviance) and $SS_{TOT} = SS_E + SS_R = \sum_i {(y_i - \overline{y})^2}$ is the total sum of squares (total deviance).

$R^2$ can take values in the $[0,1]$ interval and represents the percentage of response variability explained by explanatory variables. 

If $R^2=0$, then the explanatory (independent) variables don't add any information to explain the dependent variable; if $R^2=1$, then the explanatory (independent) variables explain all the variability of dependent variable.

If $p=1$, then $R^2 = \widehat{[Corr(\underline{x}, \underline{y})]}^2$

where $\widehat{Corr(\underline{x}, \underline{y})}$ is the estimate of Pearson linear correlation index using $\underline{x}$ and $\underline{y}$ data.

A so-called "adjusted" version of $R^2$ also exists. It is the $R^2$ corrected by the degrees of freedom of sums of squares:

$$
\overline{R}^2 = 1 - \frac{n-1}{n-p-1}\frac{SS_R}{SS_{TOT}}=1 - \frac{n-1}{n-p-1}\left(1-R\right)
$$

And then it can assume values in the $[- \frac{p}{n-p-1}, 1]$ interval. 

#### Comparing two nested models: alternative formula

Returning back to the nested models hypothesis testing formula, one can see that $F$ may be rewritten as:
$$
  F = \frac{(R_1^2-R_0^2)/(p-p_0)}{(1-R_1^2)/(n-p-1)}
$$

Where $R_0^2$ and $R_1^2$ are the determination coefficients for the reduced and the complete model, respectively.

In this case, using the formula with the $R^2$ coefficients, $R_1^2-R_0^2$ is in the numerator, and this because when two models are nested, the "parent" model always has a greater $R^2$. Also, the $F$ test as seen with this formulation, establishes if the difference (in terms of explained variability, $R^2$) is significant or not.
<!--- 
% ### Inference using estimable functions (generalization of tests)
As previously seen, it may be shown that 
$$
Var(\underline{\hat\beta}) = \sigma^2 {\left( \underline{X}^T \underline{X} \right)}^{-1} 
$$ 
and then
$$ 
Var \left( \underline{K}^T \underline{\hat\beta} \right) = \sigma^2 \underline{K}^T {\left( \underline{X}^T \underline{X} \right)}^{-1} \underline{K}
$$
where $\underline{K}$ is a $((p+1) \times k)$ matrix, and $\underline{K}^T \underline{\beta}$ is a **estimable function**.

The statistic for $H_0 = \underline{K}^T \underline{\beta} = \underline{K}^T \underline{\beta_0}$ is
$$
\dfrac{\left( \underline{K}^T \underline{\hat\beta} - \underline{K}^T \underline{\beta_0} \right)^T \left[ \underline{K}^T \left( \underline{X}^T \underline{X} \right)^{-} \underline{K} \right] \left( \underline{K}^T \underline{\hat\beta} - \underline{K}^T \underline{\beta_0} \right)/\nu}{SSR/(n-\nu)}
$$
Where $\nu = \text{rank}(\underline{K})$.

The statistic, under $H_0$, is distributed as a $F_{\nu,n-\nu}$ 

As a particular case, the scalar form is the quare of Wald statistic shown above:
$$
\frac{(\hat\beta - \beta_0)^2}{Var(\hat\beta - \beta_0)}
$$
--->

### A particular case of $F$ test for nested models
When $p_0 = 0$, that is when the reduced model has only the intercept, we get the model goodness-of-fit $F$ test for the complete model.
It tests  
  
$H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0$,  
  
that is if $H_0$ is refused, at least one parameter included in the complete model is significant. Since in this case $R^2_0 = 0$, and $SS_{R_0} = \sum_i {(y_i - \overline{y})}^2 = SS_{TOT}$ we call $R^2_1 = R^2$ and $SS_{R_1} = SS_R$:
  $$ 
  F = \frac{(SS_{TOT} - SS_R)/p}{SS_R/(n-p-1)} =
      \frac{R^2/p}{(1-R^2)/(n-p-1)}\,,
  $$

$F \sim F_{p, n-p-1}$ under $H_0$.
This $F$ test is also a way to associate a p-value to the coefficient $R^2$

In **R** 
```
summary(lm(formula))
``` 
reports both the value of $F$, its p-value and the value of $R^2$.

### $t$-test and ANOVA are regressions!

#### One sample $t$-test
One-sample Student's $t$-test is equivalent to the linear regression of the intercept-only model.  
The one-sample $t$-test p-value is equal to the intercept regression parameter ($\beta_0$) p-value, where the response variable minus $\beta^*_0$ is measured (see examples in above paragraphs).

#### Two independent samples $t$-test
Student's $t$-test for independent samples is equivalent to the linear regression of the response variable on the grouping variable.  
The $t$-test for indipendent samples p-value is equal to the slope regression parameter ($\beta_1$) p-value.  
The linear regression model says data are normally distributed about the regression line with constant standard deviation. The grouping variable takes on only two values.  
Therefore, there are only two locations along the regression line where data are. "Homoscedastic normally distributed values about the regression line" is equivalent to "two normally distributed populations with equal variances".  
Thus, the hypothesis of equal means ($H_0: \mu_A - \mu_B = 0$) is equivalent to the hypothesis that the regression coefficient of $x$ is 0 ($H_0: \beta_1 = 0$). That is, the population means are equal if and only if the regression line is horizontal (see examples).

#### ANOVA
More than two samples can be compared using ANOVA. In the ANOVA, the categorical variable is effect coded, which means that each category's mean is compared to the grand mean.  
In the regression, the categorical variable is dummy coded, which means that each category's intercept is compared to the reference group's intercept.  
So an ANOVA reports each mean and a p-value that says at least two are significantly different. A regression reports only one mean (as an intercept), and the differences between that one and all other means, but the p-values evaluate those specific comparisons.  
Thus, the hypothesis of equal means ($H_0: \mu_0 = \mu_1 = \dots = \mu_p$) of the ANOVA is equivalent to the hypothesis that the regression dummy coefficients $\beta_1, \beta_2, \dots, \beta_p$ are all equal to zero (see examples).
