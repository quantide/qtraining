---
title:
author:
date:
output:
  html_document:
    self_contained: no
---

```{r setup, include=FALSE}
library(knitr)
# La directory di lavoro va definita all'inizio in questo modo per non doverla ripetere in
# ogni chunk! (con echo=F si eseguono i comandi del chunk senza mostrarli)
#root.dir = "~/Dropbox/quantide/int/corsi/corsiR/03.rModels/v01/data/allData",
opts_chunk$set(list(dev = 'png',fig.cap='',fig.show='hold',dpi=100,fig.width=7, fig.height=7,fig.pos='H!',fig.path="figure/glm-"))
```
```{r, echo=FALSE}
# La directory di lavoro va definita all'inizio in questo modo per non doverla ripetere in
# ogni chunk! (con echo=F si eseguono i comandi del chunk senza mostrarli)
opts_knit$set(root.dir = "../data/allData")
```

Generalized Linear Models
============================================


```{r,echo=FALSE}
rm(list=ls())
```
```{r,eval=FALSE,echo=FALSE}
setwd("data/allData")
```


Some illustrative examples
--------------------------

### Example 1

Collett (1991, p. 75) reports an experiment on the toxicity to the tobacco budworm _Heliothis Virescens_ of doses of the pyrethroid trans-cypermethrin to which the moths were beginning to show resistance. 

Batches of 20 moths of each sex were exposed for three days to the pyrethroid and the number in each batch that were dead or knocked down was recorded.

The results were:

Dose ($\mu g$)    1    2    4    8    16    32
--------------   ---  ---  ---  ---  ----  ----
    Sex: Male     1    4    9    13   18    20
  Sex: Female     0    2    6    10   12    16

<!---
% \begin{frame}
%   \begin{itemize}
%   \vspace{.5cm}
%    \item Structure of the model: $$ Y_i = \frac{Z_i}{m_i}\,,\quad Z_i \sim Bin\left( m_i, \frac{e^{\eta_i}}{1+e^{\eta_i}} \right)\,,\quad i=1,\ldots,n $$
%    In the example $m_i = 20$ $\forall i$ 
%   \vspace{.25cm}
%    \item Deterministic component of the model (without interaction): $$ \eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} $$
%    $x_{i1}$ is the dose applied to the $i$-th observation; $x_{i2}=1$ if the $i$-th observation is a male batch and $x_{i2}=0$ if the $i$-th observation is a female batch.
% %   \vspace{.25cm}
%    \item Link function: $$ \eta_i = g(\mu_i) = \log\frac{\mu_i}{1-\mu_i}\,,\quad \mu_i = E(Y_i) $$
%   \end{itemize}
% \end{frame}
--->

To analyze such a type of relation, a "classic" linear model is inadequate, because:

* Dependent variable of model is clearly non-Normal.
* The error term of model cannot be thought as homoscedastic, because if the true ratio of death mots is close to 0.5, then the variance of dependent variable is close to 0.25; whereas, if the true ratio of death mots is close to 0 or 1, then the variance of dependent variable may be much less than 0.25.
* The predicted values produced by a "classic" linear model may almost surely fall oustide the allowed range for the dependent variable in this case; consequently, and also, the relation between independent and dependent variables is almost surely non-linear.


To analyze such a problem, a model structured as below could be considered: 
$$
Z_k \sim Bin(1,\mu_k)\,,\quad \mu_k = \frac{e^{\eta_k}}{1+e^{\eta_k}}\,,\quad k=1,\ldots,m
$$
where $Bin(a,b)$ means a Binomial distribution with parameters $a$ (sample size) and $b$ (probability of success); $Z_k$ is equal to 1 if the $k$-th worm is dead or knocked out, $Z_k=0$ otherwise; $\mu_k = Pr(Z_k = 1)$; $\eta_k$ is a linear function of independent variables (the **linear component** of model);  $m = 240$ $(2 \times 6 \times 20)$. 

When observations are grouped (in the example budworms are grouped in batches) it is more convenient considering $Z_{ij}$, $i=1,\ldots,n$, $j=1,\ldots,n_i$, $\sum_{i=1}^n n_i = m$, and dealing with
   $$
   Y_i = \frac{1}{n_i}\sum_{j=1}^{n_i} Z_{ij} 
   $$
In the example $n_i=20$ $\forall i$ and $n=12$.
  
The deterministic (linear) component of the base model, if no interaction between `Sex` and  `Dose` is considered, may be stated as: 
$$ 
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}
$$

where $x_{i1}$ is the dose applied to the $i$-th grouped observation; $x_{i2}=1$ if the $i$-th grouped observation is a male batch and $x_{i2}=0$ if the $i$-th grouped observation is a female batch.

The inverse of formula which returns $\mu_i$ as a function of $\eta_i$ (in this case a logistic-type function) is called **Link function**, and in this case (as inverse of logistic function) is called **logit**: 
$$
\eta_i = g(\mu_i) = \textrm{logit}(\mu_i) = \log\left(\frac{\mu_i}{1-\mu_i}\right)\,,\quad \mu_i = E(Y_i)
$$

As a consequence: 
$$
\mu_i = g^{-1}(\eta_i) = \frac{e^{\eta_i}}{1+e^{\eta_i}} =
   \frac{\exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+\exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}
$$
The resulting model could be graphically summarized, for this specific example, with the following graph:

![Example of model for Binomial data](./figure/glm-budwormsFinalModelGraph.png)

#### How to specify response variable for binomial data in R
In the case of binomial data, in **R** response variable may be specified in three different ways:

* If the response is a numeric vector it is assumed to hold the data in ratio form, $y_i = s_i / a_i$, in which case the $a_i\text{'s}$ must be given as a vector of weights using the `weights` argument. If the $a_i\text{'s}$ are all one, the default `weights` suffices.
* If the response is a logical vector or a two-level factor it is treated as a 0/1 numeric vector and handled as previously.
* If the response is a two-column matrix it is assumed that the first column holds the number of successes, $s_i$, and the second holds the number of failures, $a_i - s_i$, for each trial. In this case, no `weights` argument is required.

### Example 2
An experiment recorded the counts of chromosomal abnormalities observed for various amounts and intensities of gamma radiation.
The number of cells per measurement varied.
The results contains, for each one of 27 measurements:

* The number of cells per measurement;
* The numbers of chromosomal abnormalities;
* The amount of gamma radiation;
* The intensity of gamma radiation.   

In such a case, again the "classical" linear model is inadequate, since the dependent variable is discrete, clearly non-Normal, and cannot be less than zero. 

In this case, a Poisson distribution may be assumed for the dependent variable. The parameter (the mean) of Poisson distribution could be a non-linear transformation of a linear combination of explicative (independent) variables. In this case, since the mean of Poisson distribution must be greater than zero, the non-linear transformation could be the Exponential one: 
$$
Y_i \sim \text{Poi}(\mu_i)\,,\quad \mu_i = E(Y_i) = e^{\eta_i}\,,\quad i=1,\ldots,n
$$
$Y_i$ is the number of chromosomal abnormalities for the $i$-th observation; $n =$ 27.  
The deterministic (linear) component of the base model could be: 
$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4}
$$
where $x_{i1}$ is the number of cells, $x_{i2}$ is intensity of gamma radiation, $x_{i3}$ is the amount of gamma radiation, $x_{i4} = x_{i2}x_{i3}$.  
The link function is a **log link**: 
$$
\eta_i = g(\mu_i) = \log(\mu_i)
$$
As a consequence: 
$$
\mu_i = g^{-1}(\eta_i) = e^{\eta_i} = \exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4})
$$


\newpage

Applied Examples
----------------

Following lines of code ares used to load all necessary libraries to perform the example analyses:  

```{r,results='hide',message=FALSE}
library(MASS)
library(lattice)
library(surveillance)
```

<!--- library(wle) 

Setting working directory

```
setwd("data/allData")
```
--->

### Example 1: Budworms

#### Data description
A study to analyze the effect of a toxic product on survival of tobacco _Heliothis Virescens_ budworms.  
Each of 5 different doses of the toxic product has been tested on 20 male 
and 20 female worms to see the survival of worms.

#### Data loading
Let us create the dataframe
```{r,split=TRUE}
ldose = rep(0:5, 2)
numdead = c(1, 4 ,9 ,13, 18 ,20, 0, 2, 6, 10, 12, 16)
sex = factor(rep(c("M", "F"), each = 6))
rate = numdead/20
SF = rbind(numdead, numalive = 20 - numdead)
Budworms = data.frame(ldose, sex, rate)
Budworms$SF = t(SF)

rm(sex, ldose, SF, rate)

head(Budworms)
str(Budworms)

class(Budworms)
```

In this case, the dependent variable (`SF`), is made by a matrix embedded within the dataframe, and read as an object with two columns.

#### Descriptives
```{r,tidy=FALSE,fig.cap="Plot of mortality rate Vs. dose and lowess lines by sex using trellis"}
xyplot(rate~ldose | sex, data = Budworms,
       strip = strip.custom(strip.names = TRUE, strip.levels = TRUE),
       par.strip.text = list (cex = 2),
       panel = function(x, y, ...){
         panel.xyplot(x, y, pch = 16, type  = "p", cex = 2,...)
         panel.loess(x, y, lty = 1 , lwd = 2, col = "violet",...)
         panel.grid()
       }
)
```

It seems that a difference exists between male and female moths, but it is not really clear if this difference is only in "intercept" or in "slope" too.

#### Inference and models

Let us try to fit a complete model to data:
```{r,tidy=FALSE}
fm0 = glm(SF ~ sex + ldose + sex:ldose, family = binomial(), 
          data = Budworms, trace = T)
```
The `glm()` function is used to generate glm-type objects; the `family` parameter is a new parameter, with respect to `lm()` function. In this specific example, `family` is `binomial()`, which means that the family of dependent variable is Binomial. Since no other parameters are given to ```binomial()```, the link function used is the default one for the Binomial family: the ```logit``` link function.  
Finally, the `formula` parameter, the first parameter in above call of `glm()`, contains the linear component of the model.
```{r}
summary(fm0)
```
The output of `summary` function is similar to the one obtained by applying it to `lm` objects.  
The returned parameters are, of course, relative to linear portion of model.  
`ldose` is significant, and growing values of `ldose` produce higher probabilities of death of moths (positive coefficient); `sex` and interaction between `sex` and `ldose` are not significant.  

The output reports also an indication about the dispersion parameter $\phi$: in the binomial case it is
fixed to one, since the relationship between mean and variance is entirely ruled by $V(\underline\mu)$.

Now a different parameterization of the same model is tried. Maybe this parametrization will give parameter values more easily to interpret.  
```{r,tidy=FALSE}
fm1 =  glm(SF ~ sex + sex:ldose - 1,family = binomial(), 
           data = Budworms, trace = T)
summary(fm1)
```
This parameterization actually shows the parameters as if two distinct regression models were calculated on data: one for male and one for female moths.  

Then, `sexF` and `sexM` parameters represent, respectively, the intercept for female and male moths, whereas `sexF:ldose` and `sexM:ldose` parameters represent, respectively, the slopes of linear models for female and male moths.  

Obviously in this case p-values of parameters and of model are in general different, with respect to previous parameterization, and take a different meaning, because the tests performed are actually different.

Now the predictions for a set of values for independent variables are calculated.

Here the dataframe of independent variables values for the predictions is created:
```{r,tidy=FALSE}
newdata = data.frame(
  ldose = rep(seq(0, 5, length = 100), 2),
  sex = factor(rep(0:1, each = 100), labels = c("F","M"))
)
```
And then, the predictions are actually calculated:
```{r}
newdata$fit = predict(fm1, type = "response", newdata = newdata)
```
The function to make predictions is `predict`, the same used for linear models.

The following lines of code plot the observed mortality rate and corresponding model estimates, for male and female moths.
```{r,fig.cap="Plot of mortality rate Vs. dose by sex with first model fit"}
with(Budworms, {
  plot(rate ~ ldose, type = "n")
  points(0:5, rate[sex == "F"], pch = 4, col = "darkgray", cex = 2)
  points(0:5, rate[sex == "M"], pch = 5, col = "darkgray", cex = 2)
})
with(newdata, {
  lines(ldose[sex == "F"], fit[sex == "F"], col = "orange")
  lines(ldose[sex == "M"], fit[sex == "M"], col = "darkblue")
})
```

Now, in above `fm0` model, Sex seemed non-significant. But is this interpretation correct?  
A slightly modified model can be calculated, where a centered on 0 `ldose` variable is used:
```{r}
fm2 =  update(fm0, . ~ sex + I(ldose-3) + sex:I(ldose-3))
summary(fm2, cor = F)$coefficients
```
In `fm0` `sexM` parameter is not significant; this tells us only that male and female moths tend to have the same
values of mortality rate when `ldose` is zero. However, by only shifting data values of `ldose` by a fixed quantity (in this case 3) the coefficient becomes significant!  
This may happen when including an interaction term, and this is a reason for which removing non significant lower degrees effects from a model if their interaction with other effects is significant is generally discouraged.

Now a check for curvature in relationship between logit of mortality rate and `ldose` is performed.
```{r}
fm3 = glm(SF ~ sex + ldose + sex:ldose + sex:I(ldose^2), 
          family = binomial(), data = Budworms, trace = T)
anova(fm0, fm3, test = "Chisq")
```
With `anova()` the reference distribution is Chi-Square (see the `test="Chisq"` parameter in above line of code) for binomial and Poisson models ($\phi$ known);
with gamma and Gaussian model ($\phi$ unknown) the reference distribution is $F$. Also, instead of `test="Chisq"`, `test="LRT"` can be used.  
`anova()` p-value "says" that the presence of quadratic effect on `ldose` is not significant.


Now the model without interaction is fitted to data; `anova()` is then used to test for parallelism:
```{r}
fm4 = glm(SF ~ sex + ldose, family = binomial(), Budworms, trace = T)
anova(fm0, fm4, test = "Chisq")
summary(fm4)
```
The model with parallel regression lines seems to fit well the data, and the interaction effect is non significant.  
`ldose` affects positively the death probability of moths, while male moths have a greater probability of death than female moths.

Next lines of code produce a graph showing the fitted probability for male and female moths, along with the observed mortality rates.

```{r,tidy=FALSE}
newdata = data.frame(
  ldose = rep(seq(0, 5, length = 100), 2),
  sex = factor(rep(0:1, each = 100), labels = c("F","M")))
```
```{r}
newdata$fit = predict(fm4, type = "response", newdata = newdata)
```
```{r budwormsFinalModelGraph,fig.cap="Plot of mortality rate Vs. dose by sex with final model fit"}
with(Budworms, {
  plot(rate ~ ldose, type = "n",xlab="Dose", ylab="Rate")
  points(0:5, rate[sex == "F"], pch = 4, col = "darkgray", cex = 2)
  points(0:5, rate[sex == "M"], pch = 5, col = "darkgray", cex = 2)
})
with(newdata, {
  lines(ldose[sex == "F"], fit[sex == "F"], col = "orange")
  lines(ldose[sex == "M"], fit[sex == "M"], col = "darkblue")
})
```

And then the diagnostic graphs on model residuals. The four graphs produced shall be read roughly in the same manner of residual plots for linear models. However, since the characteristics of dependent variable, more "flexibility" is required to confirm the adequacy of model. 
```{r,fig.cap="Residual plot for the model"}
op = par(mfrow = c(2, 2))
plot(fm4)
par(op)
```
Standardized, for residuals, means divided by an estimate of their variability.
The plots do not show particular patterns.

<!---
Anscombe residuals should be distributed "more Normally" than "classic" residuals when the model is correctly
specified.
Anscombe residuals (trial version)
```{r ,eval=FALSE}
res=anscombe.residuals(fm4,phi=1)
res1=residualsAnscombe(y=fm4$y,mu=fm4$fitted.values,family=binomial())
op=par(mfrow=c(1,2))
plot(predict(fm4,type="response"),res)
lines(lowess(predict(fm4,type="response"),res))
grid()
qqnorm(res)
qqline(res)
grid()
par(op)
```
--->

\newpage

### Example 2: Low birthweight

#### Data description
The low birthweight data contains information on low birth weight in infants born at a US hospital. 
A number of variables were collected that might explain the cause of the low birth weight.

Variable description:

* `low`: indicator of birth weight less than 2.5 kg.
* `age`: mother's age in years.
* `lwt`: mother's weight in pounds at last menstrual period.
* `smoke`: smoking status during pregnancy.

```{r,echo=FALSE}
rm(list=ls())
```

#### Data loading
```{r}
load("bwt.Rda")
head(bwt)
str(bwt)
```
Set R contrasts default
```{r}
options(contrasts = c("contr.treatment", "contr.poly"))
```


#### Descriptives

Next lines of code produce a plot showing the relation between mother's `age` and `low` by smoking status, with lowess lines:
```{r}
pl1 = xyplot(jitter(low, .05) ~ age | smoke, data = bwt, layout = c(2, 1),
  panel = function(x, y, ...){
    panel.xyplot(x, y, pch = 16, col = "red", ...)
    panel.loess(x,y, degree = 2, span = .8, col = "darkblue", lwd = 2)
  }
)
```
And now a plot of mother's weight vs ```low``` by smoking status, with lowess lines:
```{r}
pl2 = xyplot(jitter(low, .05) ~ lwt | smoke, data = bwt, layout = c(2, 1),
  panel = function(x, y, ...){
    panel.xyplot(x, y, pch = 16, col = "red", ...)
    panel.loess(x,y, degree = 2, span = .8, col = "darkblue", lwd = 2)
  }
)
```
Now the two plots are actually printed in only one display:
```{r,fig.cap="Graph of low Vs. age and Vs. lwt by smoke with lowess using trellis"}
print(pl1, position=c(0, 0.5, 1, 1), more = TRUE)
print(pl2, position=c(0, 0, 1, 0.5))
```

```low``` does not seem to depend from mother's age, while a relation between ```low``` and mother's low weight seems to exist. It is not clear if mother's smoking status affects ```low``` probability.

Now, a first simple model with main effects only is tried:
```{r}
fm1 = glm(low ~ age + lwt + smoke, data = bwt, family = binomial())
summary(fm1)
```

From the results of this model, a marginally significant relation between `low` and `age` and between `low` and `smoke` seems to exist. Low mother's weight seems to reduce the probability of infant low weight, while mother's smoking seems to increase the probability of low weight.

Now, let us try a more complex model, with all main effects and second degree interactions:
```{r}
fm2 = update(fm1, . ~ .^2)
summary(fm2, corr = F)$coefficients
```
Try to find useless (non significant) terms using the `dropterm()` function.  
This function performs a "Type II Sum of Squares"-like significancy test on all removable model parameters.  
After calling `dropterm()`, the less significat term is removed by using `update()`.
```{r}
dropterm(fm2, test = "Chisq")
fm3 = update(fm2, . ~ . -age:lwt)
anova(fm3, test = "Chisq")
```
Now, since only main effects are significant, a test is performed to check if, globally, all remaining interaction terms are significant:
```{r}
anova(fm1, fm3, test = "Chisq")
```

Removing first-order interactions seems to not affect the goodness of model in terms of explicative power.

Now, a main-effects model is tried, but adding some second-degree terms on quantitative preditors.  
This should allow checking if some curvature exists in `age` and `lwt`. 
```{r}
fm4 = update(fm1, . ~ . + I(age^2) + I(lwt^2))
summary(fm4)
```
Now let us check if curvature effects are globally significant:
```{r}
anova(fm1,fm4,test="Chisq")
```
Curvature effects are not significant.

Next code line compares the two-way interactions complete model with the model with main effects only, to see if all two-way interactions are globally significant:
```{r}
anova(fm1,fm2,test="Chisq")
```
Two-way interactions are globally non significant.

Finally, check significancy of main effects by using "Type II Sum of Squares"-like tests:
```{r}
dropterm(fm1,test="Chisq")
```

And then `age` is removed from model:
```{r}
fm0 = update(fm1, . ~ . -age)
summary(fm0)
```
Now the diagnostic graphs on model residuals. 
```{r,fig.cap="Residual plots of final model"}
op = par(mfrow = c(2, 2))
plot(fm0)
par(op)
```

The diagnostic graphs are not really nice, but similar configurations of points is not infrequent, when the response variable is a Bernoulli (not Binomial) one.

And now the prediction for the observed range of prediction data values
```{r}
bwt$prob = predict(fm0, type = "response")
bwt = bwt[order(bwt$prob),]
```
And the plot of model predictions: 
```{r,tidy=FALSE,fig.cap="Plot of final model predictions with data points"}
bwtPred  = reshape(bwt, varying = list(c(1,5)), direction = "long")
xyplot(low ~ lwt| smoke, layout = c(2,1), aspect = 1.5,
  groups =  time, data = bwtPred, 
  type = c("p", "l"), distribute.type = TRUE, 
  pch = 16, lwd = 2, col = c("darkblue", "red"), grid = T)
```

\newpage

### Example 3: Chromosomal abnormalities

#### Data description

Numbers (`ca`) of chromosomal abnormalities observed for various amounts (`doseamt`) and intensities (`doserate`) of gamma radiation. 
The numbers of cells per measurement varied.

```{r,echo=FALSE}
rm(list=ls())
```

#### Data loading
```{r}
load("dicentric.Rda")
head(dicentric)
str(dicentric)
```

Chromosomal abnormalities is a non-continuous count variable. In this case, of course, the
linear model is generally not adequate to describe data. A Poisson model is the typical GLM useful for this
purpose.  
The default link function for the Poisson model is the natural logarithm.  
Note: in some cases a linear model could even work well, but inference on parameters would not be
reliable.

#### Data management
Let us create a `conc` variable containing the concentration of chromosomal abnormalities, calculated with respect to the number of cells analyzed.
```{r}
dicentric$conc = dicentric$ca/dicentric$cells
```

#### Descriptives

Plot of `ca/cells` vs `doseamt` by `doserate` (one plot)
```{r,tidy=FALSE,fig.cap="Plot of doseamt Vs. ca/cells by doserate using trellis"}
xyplot(conc ~ doseamt, groups = doserate, data = dicentric,
  type = "b", auto.key = list(space = "right", lines = TRUE, points = FALSE))
```
Plot of `ca/cells` vs `doserate` by `doseamt` (one plot)
```{r,tidy=FALSE,fig.cap="Plot of doserate Vs. ca/cells by doseamt using trellis"}
xyplot(conc ~ doserate, groups = doseamt, data = dicentric,
  type = "b", auto.key = list(space = "right", lines = TRUE, points = FALSE))
```
Plot of `ca/cells` vs `doseamt` by `doserate` (separate plots)
```{r,tidy=FALSE,fig.cap="Plot of doseamt Vs. ca/cells by doserate (separate plots) using trellis"}
xyplot(conc ~ doseamt | factor(doserate), data = dicentric,
  type = "b", layout = c(9, 1), aspect = 1.25,
  main = "doseamt vs ca/cells given doserate")
```
Plot of `ca/cells` vs `doserate` by `doseamt` (separate plots)
```{r,tidy=FALSE,fig.cap="Plot of doserate Vs. ca/cells by doseamt (separate plots) using trellis"}
xyplot(conc ~ doserate | factor(doseamt), data = dicentric,
  type = "b", layout = c(3,1), aspect = 1.25,
  main = "dosereate vs ca/cells given doseamt")
```
Plot of `ca/cells` vs logarithm of `doserate` by `doseamt` (separate plots)
```{r,tidy=FALSE,fig.cap="Plot of doserate logarithm Vs. ca/cells by doseamt (separate plots) using trellis"}
xyplot(conc ~ log(doserate) | factor(doseamt), data = dicentric,
  type = "b", layout = c(3,1), aspect = 1.25,
  main = "log(dosereate) vs ca/cells given doseamt")
```
Plot of `ca/cells` vs log(`doserate`) by `doseamt` (one plot)
```{r,tidy=FALSE,fig.cap="log(doserate) vs ca/cells by doseamt using trellis"}
xyplot(conc ~ log(doserate), groups = doseamt, data = dicentric,
       type = "b", auto.key = list(space = "right", lines = TRUE, 
                                   points = FALSE))
```
The pictures show that a linear relation between `ca/cells` and `doserate` or log(`doserate`) seems to exist. Also, this relation seems to change by varying levels of `doseamt`. 

Next level plot tries to represent the relation in a three-dimensional-like fashion:
```{r,tidy=FALSE,fig.cap="Level plot of conc Vs. doseamt and doserate"}
levelplot(conc ~ factor(doseamt) + factor(doserate), data = dicentric, 
          aspect = 3, region = T, col.regions = terrain.colors(27))
```

#### Inference and models

**A first simple model**  
Now let us estimate a Poisson model. The default link is the logarithm. This "forces" the estimated mean being greater than 0.
Even here (like in Binomial case) the dispersion parameter is known and equal to 1. 
If Poisson model with log link holds, then the number of chromosomal anomalies should be proportional to the number of cells analyzed, and then the `log(cells)` variable should be included in the model.

Model estimate:
```{r}
fm0 = glm(ca ~ log(cells) + doserate * doseamt, family = poisson(), data = dicentric)
summary(fm0)
```

The interaction between `doserate` and `doseamt` is significant, while main effect of `doserate` is not.

Plot of diagnostic graphs
```{r,fig.cap="Residual plot of first model"}
op = par(mfrow = c(2, 2))
plot(fm0)
par(op)
```
A coulpe of observations (25 and 27) seem outliers or leverages.

<!---
Anscombe residuals (trial version)
```{r}
res=anscombe.residuals(fm0,phi=1)
par(mfrow=c(1,2))
plot(predict(fm0,type="response"),res)
lines(lowess(predict(fm0,type="response"),res))
grid()
qqnorm(res)
qqline(res)
grid()
```
--->
**Second model: adding an offset term**  
If the Poisson distributional assumption is true, then the number of cells analyzed can be thought as an offset: it simply multiplies the mean of `ca` in one cell by the number of cells analyzed.  
In other words, if the expected number of chromosomal abnormalities for a cell is $\mu$, and the distribution is a Poisson($\mu$) then the expected number of chromosomal abnormalities for $k$ cells is $k \mu$, and the distribution is a Poisson($k \mu$).  
In this case `cells`  is not an explicative variable whose parameter has to be estimated, but simply a multiplicative constant for the mean. This is accomplished by adding an `offset` term to model depending from `cell`, when calling `glm()`. Since the link function is the logarithm, `cell` must be included in model as a log-transformed offset.  
Also, to simplify the interpretation of resulting model, and to obtain a more powerful model, that better accomodate possible non linearities in variable, `doseamt` is treated as a factor.

Model estimate:
```{r}
fm1 = glm(ca ~ offset(log(cells)) + factor(doseamt) + doserate:factor(doseamt) - 1,
          family = poisson(), data = dicentric)
summary(fm1)
```
Now there is no parameter referring to cells. Also, Akaike Information Criterion (AIC) is much less than the one of first model.

Plot of diagnostic graphs
```{r,fig.cap="Residual plot of second model"}
op = par(mfrow = c(2, 2))
plot(fm1)
par(op)
```

<!---
Anscombe residuals (trial version)
```{r}
res=anscombe.residuals(fm1,phi=1)
par(mfrow=c(1,2))
plot(predict(fm1,type="response"),res)
lines(lowess(predict(fm1,type="response"),res))
grid()
qqnorm(res)
qqline(res)
grid()
```
--->

**Considering the log of doserate**

In descriptive graphs, it appeared that the relation betwen `log(doserate)` and concentration is linear.  
So, we will try a different model, that uses `log(doserate)` as explicative variable instead of simpler `doserate`.

Model estimate
```{r,tidy=FALSE}
fm2 = glm(ca ~ offset(log(cells)) + factor(doseamt) + 
            log(doserate):factor(doseamt) - 1, family = poisson(), data = dicentric)
summary(fm2)
```
Plot of diagnostic graphs
```{r,fig.cap="Residual plot of third model"}
op = par(mfrow = c(2, 2))
plot(fm2)
par(op)
```
This model performs better than the previous ones, since the residual plots seem more regular (at least, with respect to the first model), and the AIC is less than in previous two models.
<!---
Anscombe residuals (trial version)
```{r}
res=anscombe.residuals(fm2,phi=1)
par(mfrow=c(1,2))
plot(predict(fm2,type="response"),res)
lines(lowess(predict(fm2,type="response"),res))
grid()
qqnorm(res)
qqline(res)
#abline(0,1,col="red")
grid()
```
--->

Next, a plot of prediction of mean number of chromosomal anomalies per cell is produced. Here the predictions. 
```{r}
preddata = dicentric[, c("cells", "doseamt", "doserate")]
preddata$cells = 1
dicentric$pred = predict(fm2, newdata = preddata, type = "response")
```
A plot for `log(doserate)` as independent variable:
```{r,tidy=FALSE,fig.cap="Plot of predictions of model with doserate log scale using trellis"}
df = reshape(dicentric[,3:6]  ,varying = list(3:4), direction = "long")
xyplot(conc ~ log(doserate) | factor(doseamt), group = time, data = df, 
  strip = strip.custom(strip.names = TRUE, strip.levels = TRUE, bg = "gray"),
  par.strip.text = list (cex = 1, col = "darkblue"),
  type = c("p", "l"), distribute.type = TRUE,  layout = c(3,1), aspect = 1.25,
  pch = 3, lwd = 2, col = c("darkblue", "violet"), cex  = 2,
  main = "log(dosereate) vs ca/cells given doseamt \n data = dicentric")
```
And a plot for `doserate` as independent variable:
```{r,tidy=FALSE,fig.cap="Plot of predictions of model with doserate in normal scale using trellis"}
xyplot(conc ~ doserate | factor(doseamt), group = time, data = df, 
       strip = strip.custom(strip.names = TRUE, strip.levels = TRUE, bg = "gray"),
       par.strip.text = list (cex = 1, col = "darkblue"),
       type = c("p", "l"), distribute.type = TRUE,  layout = c(3,1), aspect = 1.25,
       pch = 3, lwd = 2, col = c("darkblue", "violet"), cex  = 2,
       main = "dosereate vs ca/cells given doseamt \n data = dicentric")
```
\newpage

### Example 4: Education
```{r,echo=FALSE}
rm(list = ls())
```

#### Data description
The Raftery and Hout Irish education data.  
Data on educational transitions for a sample of 500 Irish schoolchildren aged 11 in 1967.  
The data were collected by Greaney and Kelleghan (1984), and reanalyzed by Raftery and Hout (1985, 1993).

Variable description:

1. `Sex`: 1 = male; 2 = female.
2. `DVRT`: Drumcondra Verbal Reasoning Test Score.
3. `edlevel`: Educational level attained:
    1. Primary terminal leaver
    2. Junior cycle incomplete: vocational school
	  3. Junior cycle incomplete: secondary school
	  4. Junior cycle terminal leaver: vocational school
	  5. Junior cycle terminal leaver: secondary school
	  6. Senior cycle incomplete: vocational school
	  7. Senior cycle incomplete: secondary school
	  8. Senior cycle terminal leaver: vocational school
	  9. Senior cycle terminal leaver: secondary school
	 10. 3rd level incomplete
	 11. 3rd level complete
4. `lvcert`: Leaving Certificate. 1 if Leaving Certificate not taken; 2 if taken.
5. `fathocc`: Prestige score for father's occupation (calculated by Raftery and Hout, 1985). <!--- 0 if missing. --->
6. `schltype`: Type of school: 1 = secondary; 2 = vocational; 9 = primary terminal leaver.

The goal of study is to find a model that predicts `lvcert` (Leaving Certificate), given the explanatory variables values.

#### Data loading
```{r}
load("irished.Rda")
head(irished)
str(irished)
```

#### Inference and models
We begin with a simple logistic regression model in which the leaving certificate indicator (`lvcert`) is the response
variable and the student's `DVRT` test result is the sole explanatory variable
```{r}
fm1 = glm(lvcert ~ DVRT, family = binomial(), data = irished)
summary(fm1)
```
Here we see a very significant bivariate association between `DVRT` and leaving certificate indicator.
Is this magnitude of this association substantively large? Let's plot the fitted values against `DVRT`.

Plot DVRT vs leaving certificate (with fitted values)
```{r,fig.cap="jittered lvcert Vs. DVRT with fitted means"}
with(irished, {
  plot(jitter(lvcert, 0.05) ~ DVRT, type = "n")
  abline(h = 0, col = "darkgray"); abline(h = 1, col = "darkgray")
  points(jitter(lvcert, 0.05) ~ DVRT, pch = 3, col = "red")
  lines(sort(DVRT), fitted(fm1)[order(DVRT)], col = "darkblue", lwd = 2)
  grid()
})
```
Here we see that a student at the low end of
the distribution of DVRT scores has about a 0.10 probability of
getting a leaving certificate while a student at the hight end of
the distribution of DVRT scores has about a 0.90 probability of 
getting a leaving certificate. This is a sizable effect.

Now we try to add `sex` to model, and to add the prestige score of the father's education as covariate

```{r}
fm3 = glm(lvcert ~ DVRT + fathocc + sex, family = binomial(), data = irished)
summary(fm3)
```
And now a test to check significancy of `sex`.
```{r}
anova(fm3, update(fm3, . ~ . -sex), test = "Chisq")
```
`sex` looks as significant. This confirms the results of Wald tests on same parameter.

And a significancy test on `fathocc`
```{r, error=TRUE}
anova(fm3, update(fm3, . ~ . -fathocc), test = "Chisq")
```
Test on `fathocc` returns and error because of presence of `NA` in `fathocc` data.  
To be able to test `fathocc`, missing data should be removed prior to model data (see next computations).

Anyway, to check adequacy of model, a test based on Deviance can be produced:
```{r}
pchisq(q=fm3$deviance,df=fm3$df.residual,lower.tail=FALSE)
```

Following computations produce a three-dimensional graph of estimated model.  
Initially the preditions on observed independent variables range values are calculated:
```{r,tidy=FALSE}
newdata = with(irished, 
  expand.grid(
    sex = unique(sex),
    DVRT = min(DVRT):max(DVRT),
    fathocc = min(fathocc, na.rm = T):max(fathocc, na.rm = T)
  )
)
newdata$pred = predict(fm3, newdata = newdata , type = "response")
```
And then a categorized surface plot is drawn:
```{r,tidy=FALSE,fig.cap="Categorized surface plot of model predictions Vs. DVRT and fathocc by sex using trellis"}
wireframe(pred~DVRT+fathocc | sex, data = newdata, 
          drape = T, layout = c(2,1))
```
Here we see that DVRT, sex, and fathocc exert significant effects on
the probability of obtaining a leaving certificate. 

If we are interested in testing whether the inclusion of these
additional covariates improves the fit over the original logistic
regression model with DVRT as the sole covariate, we could perform a
likelihood ratio test. However, to do this correctly we need to
refit the original model to the dataset in which the observations 
with missing values of `fathocc` have been dropped.

Model without `fathocc`:
```{r}
fm4 = glm(lvcert ~ DVRT+sex, data = na.omit(irished), family = binomial())
summary(fm4)
```
And then the model comparison:
```{r}
anova(fm3, fm4, test = "Chisq")
```
Model without `sex` 
```{r}
fm5 = glm(lvcert ~ DVRT+fathocc, data = na.omit(irished), family = binomial())
summary(fm5)
```
And then the model comparison:
```{r}
anova(fm3, fm5, test = "Chisq")
```
Model without `sex` and `fathocc`:
```{r}
fm6 = glm(lvcert ~ DVRT, data = na.omit(irished), family = binomial())
summary(fm6)
```
And then the model comparison:
```{r}
anova(fm3, fm6, test = "Chisq")
```

Clearly, new variables do improve the fit. Significancy of `fathocc` is much bigger than significancy of `sex`.

Last check on model
```{r}
pchisq(q=fm4$deviance,df=fm4$df.residual,lower.tail=FALSE)
```

The p-value obtained is small, but it is not "extremely" small. Anyway, some improvements could be obtained by adding other variables or by conducting more deep studies.

\newpage

### Example 5: Boiling

#### Data description

In an attempt to resolve a domestic dispute about which of two pans was the quicker pan for cooking, the following data were obtained.
Various measured volumes (pints) of cold water were put into each pan and geated using the same setting of the cooker.
The response variable was the time in minutes until the water boiled.

Time is a non-negative variable; the gamma distribution is then probabily more appropriate than gaussian distribution.
Finally, keep in mind that with glm 6 observations are few, since most of statistical results about GLM are asymptotic.

```{r,echo=FALSE}
rm(list=ls())
```

#### Data loading
```{r}
load("boiling.Rda")
boiling
```

#### Descriptives
```{r,tidy=FALSE,fig.cap="time Vs. volume plot for the 2 pans using trellis"}
xyplot(time ~ volume, data = boiling, groups = pan , type = "b", 
  auto.key = list (space = "right", lines = F, points = T))
```
A difference in slope appears between the two lines, but the difference is not too big. We want to test if the difference is really significant.

#### Models and inference
Initially, a simple linear model is tried; only main effects are estimated. Note that a general linear model with Gaussian error and Identity link is actually a simple linear model. 
```{r}
fm1 = glm(time ~ volume + pan - 1, data = boiling, family = gaussian)
summary(fm1)
```
All parameters are clearly significant. We may want to check for a difference in slope between pots.

Now we then estimate a model with an additional parameter to model difference in slopes:
```{r}
fm2 = glm(time ~ pan + volume:pan - 1, data = boiling, family = gaussian)
summary(fm2)
```
Actually, the model contains two distinct linear models: one for `panA`, and one for `panB`.  
Now a test on significancy of difference is performed (note that the test distribution in this case is F, because $\phi$, dispersion parameter, is estimated).
```{r}
anova(fm1,fm2,test="F")
```
Note that in this case of normal model, the `test=F` option is perfectly appropriate with theory of linear models (LM).

A difference in slope only marginally exists.  
However, this model could be unsatisfactory in two respects:

* boiling time cannot be negative 
* the variance of boiling time might be expected to increase with its expectation;

The natural candidate for these problems is the gamma distibution, as it respects the increasing relation between mean and variance as shown in the next graphical example.

Next plot produce a comparison between Gaussian and Gamma means and variances while means vary.
```{r,fig.cap="How dispersion varies with means for Gaussian models and Gamma models"}
set.seed(2000)
means=(1:300)/100
x1 = apply(as.matrix(means),MARGIN=1,FUN=function(x){rnorm(1,mean=x)})
x2 = apply(as.matrix(means),MARGIN=1,FUN=function(x){rgamma(1,shape=x, scale = 1)})


# Plot Gaussian data
pl1 = xyplot(x1 ~ means, pch = 16, main = "Gaussian", aspect = 1.5)

# Plot Gamma data
pl2 = xyplot(x2 ~ means, pch = 16, main = "Gamma", aspect = 1.5)

# Plot print
print(pl1, position=c(0, 0, 0.5, 1), more = TRUE)
print(pl2, position=c(0.5, 0, 1, 1))
```

The plot just created compare a linear model with Gaussian error distribution, with a linear model with Gamma error distribution and identity link.
Two things are evident:

* Gaussian linear model can return values less than zero, whereas Gamma model does not.
* Gaussian linear model has a substantially constant variability while Gamma linear model is clearly heteroscedastic.

Now, a model from Gamma family and identity link will be estimated on data:
```{r,tidy=FALSE}
fm3 = glm(time ~ pan + volume:pan - 1, data = boiling,
          family = Gamma(link = "identity"))
summary(fm3)
```
Comparing AIC stats
```{r}
fm2$aic ; fm3$aic
```
Gamma model results slightly better than Gaussian one. 

Now let us check on significancy of differences between the two pans:
```{r,tidy=FALSE}
fm3a=glm(time ~ pan + volume + volume:pan, data = boiling, 
         family = Gamma(link = "identity"))
summary(fm3a)
fm4=glm(time ~ pan+ volume  , data = boiling, family = Gamma(link = "identity"))
summary(fm4)

anova(fm3a,fm4,test="F")
```
This test checked if the difference in slope between the two pans is significant. The test distribution is F, because the dispersion parameter is estimated from data.  
The difference in slope has a p-value a few less than in Gaussian model. Anyway, this difference is actually not significant.  
If one wants to check if the use of `pan` as independent variable is useful to explain difference in means, given that interaction is not significant, he/she may produce the following:
```{r}
fm4bis=glm(time ~ volume  , data = boiling, family = Gamma(link = "identity"))
anova(fm4, fm4bis, test="F")
```
And A and B seem not different.

Alternatively, same results can be obtained using the following:
```{r}
anova(fm4,update(fm4,.~.-pan),test="F")
fm5 = update(fm4, . ~ . -pan)
summary(fm5)

```
However, if a researcher wants to check global significancy of `pan`, he/she may compare the model that uses volume only with the complete model, obtaining:
```{r}
anova(fm3a,glm(time~volume,family=Gamma(link="identity"),data=boiling), test="F")
```
Significancy of "global" contribution of `pan` seems barely significant.
\newpage

### Example 6: Skin cancer

#### Data description

400 patients have been classified by cancer type and body site in which it has been detected.
The goal of analysis is to check if cancer type is independent from body site.

```{r,echo=FALSE}
rm(list=ls())
```
#### Data loading
```{r}
load("skin.Rda")
head(skin)
str(skin)
```
#### Descriptives
Data view as frequency table
```{r}
xtabs(Counts ~ Type + Site, data = skin)
```

A simple solution to answer to the main question is by using Pearson Chi-square test:
```{r}
chisq.test(matrix(skin$Counts,nrow=4,ncol=3))
```
The test says that there is no independence between cancer type and body site.

An alternative approach is to use models to conduct the same test, but with a likelihood (or deviance) approach.
Let us begin with main effects model using a Poisson-distributed dependent variable:
```{r}
fm0 = glm(Counts ~ Type + Site, family = poisson(), data = skin)
summary(fm0)
```
If the main effects model is correct, and then no interaction between cancer type and body site exists, then the residual deviance will follow a Chi-square distribution with `df.residual` degrees of freedom.  
Now we test the model:
```{r}
1 - pchisq(fm0$deviance, fm0$df.residual)
```
p-value is very small: the model does not fit well the data, and probabily there is interaction between `Type` and `Site`.  
This confirms the above results obtained with Pearson Chi-square.
<!---
```{r}
fm0a=glm(Counts ~ Type, family = poisson(), data = skin)
anova(fm0,fm0a,test="LRT")
```
--->
An alternative method to perform the same analysis is by estimate the saturated (full) model and then compare it with the main effects model with `anova()`.

```{r}
fm1 = glm(Counts ~ Type * Site, family = poisson(), data = skin)
summary(fm1)
```
The comparison between fm0 and fm1
```{r}
anova(fm0, fm1, test="LRT")
```
Notice the use of `test="LRT"`; this is equivalent to  `test="Chisq"`.  
Not surprisingly, the results are equivalent to the results of analysis performed on residual deviance only.
The only difference with respect to previous test is that in this case a p-value on coefficients of interaction term is reported. This could allow the analyst (along with appropriate contrasts), to evaluate the combinations of levels that have greater impact on significancy of interaction term. 

Finally, if one thinks that the main effects model correctly describe the dependent variable, since an overdispersion exists in main effects model residuals, he/she could think to use quasi-poisson models to check the significancy of effects:
```{r}
fm2 = glm(Counts ~ Type + Site, family = quasipoisson(), data = skin)
summary(fm2)

fm2a=glm(Counts ~ Type, family = quasipoisson(), data = skin)
anova(fm2,fm2a,test="F")
```
In this specific case, since the dispersion parameter is estimated, the `test="F"` option will be required for `anova()`.  
Notice that the parameter estimates of quasi-poisson model remain unchanged, with respect to Poisson model. Indeed, p-values of parameters and `anova()` change their values.
\newpage

### Example 7: Toxoplasmosis

#### Data description

In 34 cities of El Salvador, `num` people where examined and a `prop` proportion of them results affected by toxoplasmosis.
The main question is: toxoplasmosis incidence may be due to rain?

Variables description:

* `city`: city number
* `rain`: yearly rain (mm)
* `prop`: proportion of people affected by toxoplasmosis
* `num`: examined people
* `ill`: two-column dataframe containing the number of non-illness (illN) and the number of illness (illY)

```{r,echo=FALSE}
rm(list=ls())
```

#### Data loading
```{r}
load("toxo.Rda")
head(toxo)
str(toxo)
```

#### Descriptives
Let us try to plot the relation between proportion of cases of toxoplasmosis and rain.
```{r,tidy=FALSE,fig.cap="Rate of toxoplasmosys cases Vs. rain (point size proportional to sqrt(n)) "}
plot(toxo$prop~toxo$rain,cex=sqrt(toxo$num)/5,xlab="Rain",ylab="Proportion",
     main="Proportion of toxoplasmosis cases Vs. Rain 
     \n (Point size proportional to sqrt(n) )")
toxo$illY=toxo$ill[,1]
toxo$illN=toxo$ill[,2]
toxor=reshape(toxo[,c(1:4,6,7)],direction="long",varying=list(c(5:6)),
              v.names="cases",timevar="illness",times=c(1,0))
toxor=toxor[rep(row.names(toxor), toxor$cases), 1:7]
lines(lowess(toxor$illness~toxor$rain),col="red")
grid()
rm(toxor)
```
The plot seems to show a curvilinear relation between independent ad dependent variable

#### Inference and models
First attempts to produce models that roughly fit the relation:
```{r}
fm1 = glm(ill ~ rain, family = binomial(), data = toxo)
summary(fm1)
fm2 = glm(ill ~ rain + I(rain^2), family = binomial(), data = toxo)
summary(fm2)
fm3 = glm(ill ~ rain + I(rain^2) + I(rain^3), family = binomial(), data = toxo)
summary(fm3)
fm4 = glm(ill ~ rain + I(rain^2) + I(rain^3) + I(rain^4), family = binomial(), data = toxo)
summary(fm4)
```
The cubic polynomial (in the linear portion of model) seems to be able to describe in some manner the relation between toxoplasmosis and rain.
In fact, the coefficients of `fm3` model are all significant. 

With following lines of code, the overall significancy of rain to describe toxoplasmosis is tested:
```{r}
fm0 = glm(ill ~ 1, family = binomial(), data = toxo)
summary(fm0)
mc = anova(fm0, fm3)
1 - pchisq(mc$Deviance[2], mc$Df[2])
```
There is a significative influence of rain in the model.  
The above line of code is equivalent to: 
```{r,results='hide'}
anova(fm0, fm3,test="LRT")
```
or to:
```{r,results='hide'}
anova(fm0, fm3,test="Chisq")
```

The plot of estimated model:
```{r,fig.cap="Fitted means plot with data points"}
with(toxo, {
  plot(prop ~ rain, type = "n")
  abline(h = 0, col = "darkgray"); abline(h = 1, col = "darkgray")
  points(prop ~ rain, pch = 16, col = "red")
  lines(sort(rain), fitted(fm3)[order(rain)], col = "darkblue", lwd = 2)
  grid()
})
```
<!---   lines(lowess(toxor$illness~toxor$rain),col="red",lty=2)
  rm(toxor)
--->

And now the graph of diagnostic plots
```{r, fig.cap="Residuals plot of model"}
op = par(mfrow = c(2, 2))
plot(fm3)
par(op)
```
Indeed, residuals variability seems high:
```{r}
pchisq(fm3$deviance, fm3$df.residual,lower.tail=FALSE)
```
The model is not sufficient to explain data: if the model were totally correct, then the residual deviance of model should distribute asymptotically as a Chi-square with `fm3$df.residual` degrees of freedom.

Sometimes it is natural to describe data with Binomial (or Poisson) models, but, as in this case, they do not fit well (residual deviance is still to high).  
This may happen in case of overdispersion (or underdispersion) of data.  
In this case, we could need to estimate a dispersion parameter even in case of Binomial (or Poisson) data, using a so-called quasi-binomial (or quasi-poisson) model.
<!---
Anscombe residuals (trial version)
```{r}
res=anscombe.residuals(fm3,phi=1)
par(mfrow=c(1,2))
plot(predict(fm3,type="response"),res)
lines(lowess(predict(fm3,type="response"),res))
grid()
qqnorm(res)
qqline(res)
grid()
```
--->

Now let we try a quasi-binomial model:
```{r,tidy=FALSE}
fmQ = glm(ill ~ rain + I(rain^2) + I(rain^3), 
          family = quasibinomial(), data = toxo)
summary(fmQ)
```
Plot diagnostic graphs
```{r,fig.cap="Residual plot for overdispersed (quasi-binomial) model"}
op = par(mfrow = c(2, 2))
plot(fmQ)
par(op)
```
With quasi-likelihood, estimates of parameters are the same, whereas standard errors (and therefore p-values) change!

<!---
Anscombe residuals (trial version)
```{r}
res=anscombe.residuals(fm3,phi=1.940446)
par(mfrow=c(1,2))
plot(predict(fm3,type="response"),res)
lines(lowess(predict(fm3,type="response"),res))
grid()
qqnorm(res)
qqline(res)
grid()
```
--->
\newpage

### Example 8: Clotting time

#### Data description

From McCullagh and Nelder (1989, pp. 300-302)
Hurn et al. (1945) published data on the clotting time of blood,
giving clotting times in seconds (y) for normal plasma diluted
to nine different percentage concentration with prothrombin-free
plasma (u); clotting was induce by two lots of thromboplastin.
Data are analyzed using gamma errors and inverse link.

```{r,echo=FALSE}
rm(list=ls())
```

#### Data loading
```{r}
load("clotting.Rda")
head(clotting)
str(clotting)
```

#### Descriptives
```{r,fig.cap="Clotting time (y) Vs. percentage concentration (u) by lot using trellis"}
xyplot(y~u|lot, data=clotting)
```
```{r,fig.cap="Clotting time (y) Vs. log of percentage concentration log(u) by lot using trellis"}
xyplot(y~log(u)|lot, data=clotting)
```
It seems that a relation between `y` and the inverse of `u` or `log(u)` exists.  
Consequently, a model with Gamma dependent variable and default link function ("inverse", for Gamma distribution) could be effective.
Since the relation seem more regular in `log(u)` the model with `log(u)` will be tested

#### Inference and models
Now let's try a model with intercept only
```{r}
fm0 = glm(y ~ 1, data = clotting, family = Gamma())
summary(fm0)
```

Model with log(u)
```{r}
fm1 = glm(y ~ log(u), data = clotting, family = Gamma())
summary(fm1)
```

Model with log(u) and lot, without interaction
```{r}
fm2 = glm(y ~ log(u) + lot, data = clotting, family = Gamma())
summary(fm2)
```

Model with log(u) and lot and interaction
```{r}
fm3 = glm(y ~ log(u) + lot + log(u):lot, data = clotting, family = Gamma())
summary(fm3)
```

Now a test to see if the interaction between `lot` and `log(u)` can be considered significant:
```{r}
anova(fm2,fm3,test="F")
```
The interaction term is significant, and then the full model is retained.

Plot of diagnostic graphs
```{r,fig.cap="Residual plot of last model"}
op = par(mfrow = c(2, 2))
plot(fm3)
par(op)
```

Some little heteroscedasticity seems to appear, but globally the model seems good.
<!---
Anscombe residuals (trial version)
```{r}
res=anscombe.residuals(fm3,phi=0.002129707)
par(mfrow=c(1,2))
plot(predict(fm3,type="response"),res)
lines(lowess(predict(fm3,type="response"),res))
grid()
qqnorm(res)
qqline(res)
grid()
```
--->

\newpage

Some Theory on Generalized Linear Models
----------------------------------------

Generalized Linear Models (**GLM**) are extensions of fixed-effects linear models to cases where standard linear model assumptions are violated.  
The standard linear model states that:
    $$
    y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \dots + \beta_p \cdot x_{ip} + \varepsilon_i
    $$

The above formula can be written in a matrix format:
    $$
    \underline{y} = \underline{X} \underline{\beta} + \underline{\varepsilon}
    $$

or, in terms of expected values:
    $$
    E(\underline{y}) = \underline{X} \underline{\beta}
    $$

Estimation is done by least squares, or maximum likelihood, based on the assumption of normal errors. 

GLM uses a likelihood-based procedure to fit $\underline{X} \underline{\beta}$ to a function of $E(\underline{y})$, usually suggested by the distribution of data.

$\underline{y} = (y_1, \ldots, y_n)^T$ is a vector of indipendent observations, where $y_i$, $i=1,\ldots,n$, is drawn from a probability distribution, not necessarly Gaussian.  
$\underline{y}$ could be described in GLM, as in LM, as an additive function of **systematic** and **random** components. The first corresponds to $E(\underline{y})$, the later to error.
    $$
    \underline{y} = \underline{\mu} + \underline{\varepsilon}
    $$
Also, for the variance of $\underline{y}$ the following holds:

$Var(\underline{y}) = Var(\underline{\varepsilon}) = \underline{R}$

where $\underline{R}$ is a diagonal matrix.

Also, for systematic component of model, the following assumptions hold:

$\underline{\eta} = \underline{X} \underline{\beta}$, where $\underline{\eta} = g(\underline{\mu})$.

And where $g(\underline{\mu})$ is called the **link function**, because it links the linear model to the mean of $\underline{y}$.

Nelder and Wedderburn (1972) showed that the Maximum Likelihood Estimates (MLEs) for $\underline{\beta}$ can be obtained iteratively solving:
    $$
    \underline{X}^T \underline{W} \underline{X} \underline{\beta} =  \underline{X}^T \underline{W} \underline{y}^*
    $$
with
$\underline{W} = \underline{D} \underline{R}^{-1} \underline{D}$

* $\underline{y}^* = \underline{\hat{\eta}} + (\underline{y} - \underline{\tilde{\mu}}) \underline{D}^{-1}$
* $\underline{D} = \partial \underline{\mu} / \partial \underline{\eta}$
* $\underline{R} = Var(\underline{\varepsilon})$
* $\underline{\mu} = E(\underline{y})$

Estimates of $\underline{D}$ and $\underline{R}$ are used in place of $\underline{D}$ and $\underline{R}$.  
In the case of standard linear model, $\underline{\eta} = E(\underline{y}) = \underline{\mu}$, $\underline{R} = \underline{I}\cdot \sigma^2$, and $\underline{D} = \underline{I}$.  
Thus, $\underline{X}^T \underline{X} \underline{\beta} =  \underline{X}^T \underline{y}$, is the normal equation.


### Probability distribution
The elements for estimating $\underline{\beta}$ are:

* __link function__, which determines $\underline{\eta}$ and $\underline{D}$;
* __probability function__, which determines $\underline{\mu}$ and $\underline{R}$.

The process for selecting a link function and the structure of the mean and variance can be understood by looking at the probability distribution, or, better said, at the likelihood function.

Four cases will be here considered: the Binomial, the Poisson, the Gamma and the Normal. Notice that GLM's can be applied to other distribution families, such as, for example, Inverse Gaussian, Multinomial, and others.

### The Binomial case

<!--- VERIFICARE L'USO DI m, SOPRATTUTTO VISTO L'ESEMPIO INIZIALE! --->
$m$ Bernoulli trials are hypothesized, each trial with success probability $\pi$:
    $$
    f(y_{m}) = \binom{m}{y_{m}} \pi^{y_m} (1-\pi)^{m-y_{m}}, \phantom{spazi} y_m \in 0,\, 1,\, 2,\, \cdots,\, m-1, \, m
    $$

The log-likelihood function, then, is:
    $$
    \ell(\pi; y_{m}) = y_{m} \log \left(\frac{\pi}{1-\pi}\right) + m \log(1-\pi) + \log \binom{m}{y_{m}}
    $$
And a sample proportion, $y = y_{m}/m$, thus has the following log-likelihood function:
    $$
    \ell(\pi; y) = m y \log \left(\frac{\pi}{1-\pi}\right) + m \log(1-\pi) + \log \binom{m}{my}
    $$
And mean and variance of a sample proportion are:
      $$
      E(y) = \pi \hspace{2cm} Var(y) = \frac{\pi (1-\pi)}{m}
      $$

### The Poisson case
The probability function of the Poisson distribution is:
    $$
    f(y) = \frac{\lambda^y e^{-\lambda}}{y!} , \phantom{spazi} y \in 0,\, 1,\, 2,\, 3,\, \cdots
    $$
The log-likelihood function is (one-observation sample):
$$
\ell(\lambda; y) = y \log(\lambda) - \lambda - \log(y!)
$$

And mean and variance are:
      $$
      E(y) = \lambda \hspace{2cm} Var(y) = \lambda
      $$


### The Gamma case
The probability density function of the Gamma distribution is:
      $$
      f(y) = \frac{1}{\lambda^\nu \Gamma(\nu)} y^{\nu-1}e^{-\frac{y}{\lambda}}, \phantom{spazi} y \in \Re^+
      $$

The log-likelihood function is (one-observation sample):
      $$
      \ell(\nu,\lambda; y) = -\nu \log(\lambda) -\frac{y}{\lambda} + (\nu-1)\log(y) - \log(\Gamma(\nu))
      $$

And mean and variance  are:
      $$
      E(y) = \nu \lambda\hspace{2cm} Var(y) = \nu \lambda^2
      $$

### The Normal case
The probability density function of the Normal distribution is:
      $$
      f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{1}{2\sigma^2}(y-\mu)^2\right)}, \phantom{spazi} y \in \Re
      $$
The log-likelihood function is (one-observation sample):
      $$
      \ell(\mu, \sigma^2; y) = -\frac{1}{2\sigma^2} (y-\mu)^2 - \frac{1}{2} \log(2\pi\sigma^2)
      $$
And mean and variance  are:
      $$
      E(y) = \mu \hspace{2cm} Var(y) = \sigma^2
      $$

### Common features
The log-likelihood functions for these four distributions can be expressed in a common form,
      $$
      \ell(\theta, \phi; y) = \frac{y\theta - b(\theta)}{\phi} + c(y, \phi)
      $$
where

* $\theta$ is the so-called natural parameter,
* $\phi$ is the scale parameter.

Note that $\theta$ is a function of the mean, $\theta(\mu)$. 

It is also possible to express the variance of dependent variable ($y$) as a function of the mean and $\phi$:
$$
Var(y) = \phi V(\mu)
$$
where $V(\mu)$ is the so-called **variance function**.


                 **Sample proportion**     **Poisson**           **Gamma**                                 **Normal**
--------------  ------------------------  -----------------    -----------------------------------------  ------------
$E(y)$           $\pi$                     $\lambda$            $\lambda \nu$                              $\mu$ 
$\theta(\mu)$    $\log(\pi/(1-\pi))$       $\log(\lambda)$      $-\frac{1}{\lambda \nu}=-\frac{1}{\mu}$    $\mu$
$\phi$           $1$ *                     $1$                  $\frac{1}{\nu}$                            $\sigma^2$
$V(\mu)$         $\pi(1-\pi)$              $\lambda$            $\lambda^2 \nu^2 =\mu^2$                   $1$
$Var(y)$         $\pi(1-\pi)/m$            $\lambda$            $\lambda^2 \nu$                            $\sigma^2$


**Note**: Actually, to be coherent with formulas, $\phi$ for Binomial case for sample proportion should be $1/m$. However $m$ is not a parameter to be estimated and can be included simply as a weight using weighted Likelihoods. In such manner, $\phi$ may be considered equal to $1$.

### Exponential family of distributions

A distribution whose log-likelihood has the general form
    $$
    \ell(\theta, \phi; y) = \frac{y\theta - b(\theta)}{\phi} + c(y, \phi)
    $$
is a member of the **exponential family**.

The GLM can be applied to data distributed according to the exponential family.

If $y_1, \dots, y_n$ is a random sample from such a family, the log-likelihood of $y_i$ is
    $$
    \ell(\theta_i, \phi; y_i) = \frac{y_i\theta_i - b(\theta_i)}{\phi} + c(y_i, \phi)
    $$
The joint log-likelihood of $y_1, \dots, y_n$ is

$$
\ell(\underline{\theta}, \phi; y_1, \dots, y_n) = \sum_i \left(\frac{y_i\theta_i - b(\theta_i)}{\phi} + c(y_i, \phi)\right)
$$

### Link functions
Observations are linear in the **natural parameter** $\theta$, and:

* For normally distributed data: $\theta = \mu$.
* For Poisson distributed data: $\theta = \log(\lambda)$.
* For Binomial distributed data: $\theta = \log(\pi/(1-\pi))$.
* For Gamma distributed data: $\theta = -\frac{1}{\mu} = -\frac{1}{\lambda \nu}$.

In these examples, $\theta(\mu)$ is used as a link function, and the resulting link functions are called **canonical link functions**.

### Variance structure

The structure of the variance-covariance matrix of $\underline{y}$ can be described in terms of the scale parameter and variance function. Specifically,
    $$
    Var(\underline{y}) = \underline{R} = \underline{R}_\mu^{1/2} \underline{A} \, \underline{R}_\mu^{1/2}
    $$
where

* $\underline{R}_\mu$ is the diagonal matrix whose $i$-th diagonal element is $V(\mu_i)$, the variance function for the $i$-th observation;
* $\underline{R}_\mu^{1/2}$ is the diagonal matrix of square roots of the corresponding elements of $R_\mu$;
* $\underline{A}$ is the scale parameter matrix.


**Distribution**          $\underline{\mathbf{R}}_\mu$      $\underline{\mathbf{A}}$
-------------------  -----------------------------------   --------------------------
Normal                $\underline{I}$                       $\underline{I}_{\sigma^2}$
Poisson               $\text{diag}(\lambda_i)$              $\underline{I}$
Sample proportion     $\text{diag}(\pi_i/(1-\pi_i))$        $\text{diag}(1/m_i)$
Gamma                 $\text{diag}(\lambda_i^2 \nu^2)$      $\underline{I}_{1/\nu}$


### Predicting means

The **inverse link function** (sometimes referred to as the **mean function**) is defined as $h(\underline\eta) = \underline{\mu}$.  
The inverse link function can be used to predict $\underline{\mu}$ from $\underline{\hat{\beta}}$.

In general, the relationship between $\underline{\eta}$ and $\underline{\mu}$ is assumed one-to-one, and thus $h(\underline\eta) = g^{-1}(\underline\eta)$. However, for some complex GLMs, this may be false.

Since $\underline{\eta}$ is estimated by $\underline{X}\underline{\hat{\beta}}$, then $\underline{\hat{\mu}} = h(\underline{X}\underline{\hat{\beta}})$.

* Normal:  
          $\eta = \mu \Rightarrow \underline{\hat\mu} = h(\underline{X}\underline{\hat{\beta}}) = \underline{X}\underline{\hat{\beta}}$
* Poisson:  
          $\eta = \log{\lambda} \Rightarrow \underline{\hat\lambda} = h(\underline{X}\underline{\hat{\beta}}) = \exp({\underline{X}\underline{\hat{\beta}}})$
* Sample proportion:   
          $\eta = \log{(\pi/(1-\pi))} \Rightarrow \underline{\hat\pi} = h(\underline{X}\underline{\hat{\beta}}) = \exp({\underline{X}\underline{\hat{\beta}}})/(1+\exp({\underline{X}\underline{\hat{\beta}}}))$ 
<!--- Aggiungere Gamma? --->

### Deviance
Deviance is defined as:
$$
Dev(\underline{\hat{\mu}};\underline{y}) = 2\left(\ell(\underline{y};\underline{y}) - \ell(\underline{\hat{\mu}};\underline{y})\right)
$$
where

* $\ell(\underline{y};\underline{y})$ is the value of the maximum log-likelihood achievable in a full model; i.e., it is the value of the log-likelihood for which $\underline{\theta}$ is expressed as a function of $\underline{y}$;
* $\ell(\underline{\hat{\mu}};\underline{y})$ is the value of the log-likelihood over $\underline{\hat\beta}$, i.e., it is the value of the log-likelihood for which $\underline{\theta}$ is expressed as a function of $\underline{\hat{\mu}}$, the (Maximum Likelihood) estimate of $\underline{\mu}$.

The deviance is a generalization of the Sum of Squares of Residuals in the Analysis of Variance and the likelihood ratio $\chi^2$ in contingency tables:

Another quantity used is the __scaled deviance__ ($Dev^*$), defined as:
    $$
    Dev^*(\underline{\hat{\mu}};\underline{y}) = Dev(\underline{\hat{\mu}};\underline{y})/\phi 
    $$

Deviance and scaled deviance can be used to __test hypotheses__ and/or to evaluate __goodness-of-fit__ of models.


### Goodness-of-fit test

<!-- $Dev$ is a function of $\theta$ and $\phi$.
-->
When $\phi$ is known then:

* If the model is correctly specified, $Dev^*(\underline{\hat{\mu}};\underline{y}) \sim \chi^2_{(n-p)}$ where $n$ is the number of observations and $p$ is the number of parameters in $\underline{\beta}$ (including intercept, if any);
* therefore, $Dev$ can be used as a $\chi^2$-type statistics to test the goodness-of-fit (GoF) of the model.

When $\phi$ is unknown (for example with the gamma distribution), the GoF test is in general not applicable but it is possible to estimate $\phi$ as:
    $$
    \hat\phi = \frac{1}{n-p}\sum_i \frac{{(y_i - \hat\mu_i)}^2}{V(\hat\mu_i)}\,,\quad \hat\mu_i=h(\underline{x}_i^T\underline{\hat\beta})
    $$
(where $p$ is the number of parameters of model, intercept included) and then to calculate an estimate of scaled deviance:
    $$
    Dev^*(\underline{\hat{\mu}};\underline{y}) = Dev(\underline{\hat{\mu}};\underline{y})/\hat{\phi}\,, 
    $$

(Notice that $\phi$ might be estimated also as $\tilde\phi=Dev(\underline{\hat{\mu}};\underline{y})/(n-p)$, but this estimate of $\phi$ is generally less preferred, and it does not allow a "sensical" evaluation of $Dev^*$). 

### Hypothesis testing

If $\underline{\beta}$ is partitioned in $\underline{\beta}_1$ and $\underline{\beta}_2$ then
    $$
    \underline{X}\underline{\beta} = \underline{X}_1\underline{\beta}_1 + \underline{X}_2\underline{\beta}_2
    $$

When $\phi$ is known, the difference between the deviance of the full model and that of the model fitting $\underline{X}_1\underline{\beta}_1$ can be used as likelihood ratio (LR) to test
      $$
      \begin{cases}
        H_0: \underline{\beta}_2 = \underline{0}\\ 
        H_A: \overline{H}_0
       \end{cases}
      $$
with
$$
LR = \left[ Dev^*_0(\underline{\hat{\mu}}_0;\underline{y}) - Dev^*_A(\underline{\hat{\mu}}_A;\underline{y})\right] \sim \chi^2_{(p-p_1)} \text{ under } H_0
$$ 

where $p_1$ is the number of parameters in $\underline{\beta}_1$.

When $\phi$ is unknown, the same test may be performed with the following LR:
$$
LR = \dfrac{\left[ Dev_0(\underline{\hat{\mu}}_0;\underline{y}) - Dev_A(\underline{\hat{\mu}}_A;\underline{y})\right]/(p-p1)}{Dev_A(\underline{\hat{\mu}}_A;\underline{y})/(n-p)} \sim F_{p-p_1;n-p} \text{ under } H_0
$$ 

Where, also in this case, $p$ is the number of full model parameters, intercept included.  
Note also that the two above distributional results are asymptotic in nature, i.e., they are valid for $n$ large enough.

### Inference using estimable functions
It can be shown that 
      $$
      Var(\underline{\hat\beta}) = {\left( \underline{X}^T \underline{W} \underline{X} \right)}^{-1}
      $$ 
then
      $$
      Var \left( \underline{K}^T \underline{\hat\beta} \right) = \underline{K}^T {\left( \underline{X}^T \underline{W} \underline{X} \right)}^{-1} \underline{K}
      $$
where $\underline{K}^T \underline{\hat\beta}$ is an estimable function.  
The statistic for $H_0: \underline{K}^T \underline{\beta} = \underline{K}^T \underline{\beta}_0$ is
      $$
      \left( \underline{K}^T \underline{\hat\beta} - \underline{K}^T \underline{\beta}_0 \right)^T \left[ \underline{K}^T \left( \underline{X}^T \underline{W} \underline{X} \right)^{-1} \underline{K} \right] \left( \underline{K}^T \underline{\hat\beta} - \underline{K}^T \underline{\beta}_0 \right) 
      $$

The Wald test for an individual parameter is a particular case of above formula:
      $$
      \frac{(\hat\beta - \beta_0)^2}{Var(\hat\beta - \beta_0)}
      $$

The statistic is aspymptotically distributed as a $\chi^2_{\nu}$ where $\nu = \text{rank}(\underline{K})$.



### A possibility when Binomial or Poisson models fail

Sometimes the data $y_i$ should tipically follow a Binomial (sample proportion) or Poisson distribution, but Binomial or Poisson GLMs fail to perform well because of overdispersion or underdispersion in $y_i$.   
In fact, binomial or Poisson models are such that $\phi = 1$, but sometines data may suggest that $\phi > 1$ (overdispersion) or $\phi < 1$ (underdispersion).  

After trying to transform variables, adding raised-power terms to explanatory variables, adding other variables if available, etc., another option to fit such data is by using **quasi-likelihood** models; in particular, **quasi-binomial** or **quasi-poisson** ones.

Quasi-models, and particularly, quasi-binomial or quasi-poisson, estimate the dispersion parameter $\phi$, without assuming that it is equal to $1$.

As a result, the $\hat{\beta}$ parameter estimates remain usually unchanged, with respect to the "standard" GLM approach, but the variability of estimates, and then the significancy of tests, change with the value of $\hat{\phi}$, producing different inferential results.
