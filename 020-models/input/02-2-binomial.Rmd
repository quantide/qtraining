---
title: "Binomial Examples"
---

```{r setup, include=FALSE, purl=FALSE}
require(knitr)

opts_chunk$set(list(dev = 'png',fig.cap='',fig.show='hold',dpi=100,fig.width=7, fig.height=7,fig.pos='H!',fig.path="images/glm-"))
```

```{r, echo=TRUE, message=FALSE}
require(MASS)
require(ggplot2)
require(grid)
require(gridExtra)
require(qdata)
```

## Example: Budworms

### Data description
A study to analyze the effect of a toxic product on survival of tobacco _Heliothis Virescens_ budworms.  
Each of 5 different doses of the toxic product has been tested on 20 male 
and 20 female worms to see the survival of worms.

### Data loading
Let us create the dataframe
```{r,split=TRUE}
ldose <- rep(0:5, 2)
numdead <- c(1, 4 ,9 ,13, 18 ,20, 0, 2, 6, 10, 12, 16)
sex <- factor(rep(c("M", "F"), each = 6))
rate <- numdead/20
SF <- rbind(numdead, numalive = 20 - numdead)
Budworms <- data.frame(ldose, sex, rate)
Budworms$SF <- t(SF)

rm(sex, ldose, SF, rate)

head(Budworms)
str(Budworms)

class(Budworms)
```

In this case, the dependent variable (`SF`), is made by a matrix embedded within the dataframe,
and read as an object with two columns. Also, the dosage variable actually contains the logarithm 
of administered dose (`ldose`) 

### Descriptives
```{r,tidy=FALSE,fig.cap="Plot of mortality rate Vs. dose and lowess lines by sex"}
ggp <- ggplot(data = Budworms, mapping = aes(x = ldose, y = rate)) +
  geom_point(size=3, colour="darkblue") + 
  geom_smooth(method = "loess", colour="green",span=1, se = FALSE) +
  facet_wrap(facets = ~sex, ncol = 1,labeller = function(x){x$sex <- paste("sex:",x$sex); return(x)})
print(ggp)
```

It seems that a difference exists between male and female moths, but it is not really clear if this difference is only in "intercept" or in "slope" too.

### Inference and models

Let us try to fit a complete model to data:
```{r,tidy=FALSE}
fm0 <- glm(SF ~ sex + ldose + sex:ldose, family = binomial(), 
          data = Budworms, trace = T)
```
The `glm()` function is used to generate glm-type objects; the `family` parameter is a new parameter, with respect to `lm()` function. In this specific example, `family` is `binomial()`, which means that the family of dependent variable is Binomial. Since no other parameters are given to ```binomial()```, the link function used is the default one for the Binomial family: the ```logit``` link function.  
Finally, the `formula` parameter, the first parameter in above call of `glm()`, contains the linear component of the model.
```{r}
summary(fm0)
```
The output of `summary` function is similar to the one obtained by applying it to `lm` objects.  
The returned parameters are, of course, relative to linear portion of model.  
`ldose` is significant, and growing values of `ldose` produce higher probabilities of death of moths (positive coefficient); `sex` and interaction between `sex` and `ldose` are not significant.  

The output reports also an indication about the dispersion parameter $\phi$: in the binomial case it is
fixed to one, since the relationship between mean and variance is entirely ruled by $V(\underline\mu)$.

Now a different parameterization of the same model is tried. Maybe this parametrization will give parameter values more easily to interpret.  
```{r,tidy=FALSE}
fm1 <-  glm(SF ~ sex + sex:ldose - 1,family = binomial(), 
           data = Budworms, trace = T)
summary(fm1)
```
This parameterization actually shows the parameters as if two distinct regression models were calculated on data: one for male and one for female moths.  

Then, `sexF` and `sexM` parameters represent, respectively, the intercept for female and male moths, whereas `sexF:ldose` and `sexM:ldose` parameters represent, respectively, the slopes of linear models for female and male moths.  

Obviously in this case p-values of parameters and of model are in general different, with respect to previous parameterization, and take a different meaning, because the tests performed are actually different.

Now the predictions for a set of values for independent variables are calculated.

Here the dataframe of independent variables values for the predictions is created:
```{r,tidy=FALSE}
newdata <- data.frame(
  ldose = rep(seq(0, 5, length = 100), 2),
  sex = factor(rep(0:1, each = 100), labels = c("F","M"))
)
```
And then, the predictions are actually calculated:
```{r}
newdata$fit <- predict(fm1, type = "response", newdata = newdata)
```
The function to make predictions is `predict`, the same used for linear models.

The following lines of code plot the observed mortality rate and corresponding model estimates, for male and female moths.
```{r,fig.cap="Plot of mortality rate Vs. dose by sex with first model fit"}
ggp <- ggplot(data = Budworms, mapping = aes(x = ldose, y = rate)) +
  geom_point(size=3, mapping = aes(shape=sex, colour=sex)) + 
  geom_line(data=newdata, mapping = aes(x = ldose, y = fit, colour=sex)) +
  theme(legend.position="top")
print(ggp)
```

Now, in above `fm0` model, Sex seemed non-significant. But is this interpretation correct?  
A slightly modified model can be calculated, where a centered on 0 `ldose` variable is used:
```{r}
fm2 <-  update(fm0, . ~ sex + I(ldose-3) + sex:I(ldose-3))
summary(fm2, cor = F)$coefficients
```
In `fm0` `sexM` parameter is not significant; this tells us only that male and female moths tend to have the same
values of mortality rate when `ldose` is zero. However, by only shifting data values of `ldose` by a fixed quantity (in this case 3) the coefficient becomes significant!  
This may happen when including an interaction term, and this is a reason for which removing non significant lower degrees effects from a model if their interaction with other effects is significant is generally discouraged.

Now a check for curvature in relationship between logit of mortality rate and `ldose` is performed.
```{r}
fm3 <- glm(SF ~ sex + ldose + sex:ldose + sex:I(ldose^2), 
          family = binomial(), data = Budworms, trace = T)
anova(fm0, fm3, test = "Chisq")
```
With `anova()` the reference distribution is Chi-Square (see the `test="Chisq"` parameter in above line of code) for binomial and Poisson models ($\phi$ known);
with gamma and Gaussian model ($\phi$ unknown) the reference distribution is $F$. Also, instead of `test="Chisq"`, `test="LRT"` can be used.  
`anova()` p-value "says" that the presence of quadratic effect on `ldose` is not significant.


Now the model without interaction is fitted to data; `anova()` is then used to test for parallelism:
```{r}
fm4 <- glm(SF ~ sex + ldose, family = binomial(), Budworms, trace = T)
anova(fm0, fm4, test = "Chisq")
summary(fm4)
```
The model with parallel regression lines seems to fit well the data, and the interaction effect is non significant.  
`ldose` affects positively the death probability of moths, while male moths have a greater probability of death than female moths.

Next lines of code produce a graph showing the fitted probability for male and female moths, along with the observed mortality rates.

```{r,tidy=FALSE}
newdata <- data.frame(
  ldose = rep(seq(0, 5, length = 100), 2),
  sex = factor(rep(0:1, each = 100), labels = c("F","M")))
```
```{r}
newdata$fit <- predict(fm4, type = "response", newdata = newdata)
```
```{r budwormsFinalModelGraph,fig.cap="Plot of mortality rate Vs. dose by sex with final model fit"}
ggp <- ggplot(data = Budworms, mapping = aes(x = ldose, y = rate)) +
  geom_point(size=3, mapping = aes(shape=sex, colour=sex)) + 
  geom_line(data=newdata, mapping = aes(x = ldose, y = fit, colour=sex)) +
  theme(legend.position="top")
print(ggp)
```

And then the diagnostic graphs on model residuals. The four graphs produced shall be read roughly in the same manner of residual plots for linear models. However, since the characteristics of dependent variable, more "flexibility" is required to confirm the adequacy of model. 
```{r,fig.cap="Residual plot for the model"}
op <- par(mfrow = c(2, 2))
plot(fm4)
par(op)
```
Standardized, for residuals, means divided by an estimate of their variability.
The plots do not show particular patterns.


## Example: Low birthweight

### Data description
The low birthweight data contains information on low birth weight in infants born at a US hospital. 
A number of variables were collected that might explain the cause of the low birth weight.

Variable description:

* `low`: indicator of birth weight less than 2.5 kg.
* `age`: mother's age in years.
* `lwt`: mother's weight in pounds at last menstrual period.
* `smoke`: smoking status during pregnancy.

```{r,echo=FALSE}
rm(list=ls())
```

### Data loading
```{r}
data(bwt)
head(bwt)
str(bwt)
```
Set R contrasts default
```{r}
options(contrasts = c("contr.treatment", "contr.poly"))
```


### Descriptives

Next lines of code produce a plot showing the relation between mother's `age` and `low` by smoking status, with lowess lines:
```{r}
ggp1 <- ggplot(data = bwt, mapping = aes(x = age, y= low)) +
  geom_jitter(mapping = aes(y=low), colour="red",height = 0.01) +
  geom_smooth(method = "loess", colour="blue", se = FALSE, span=2/3) +
  facet_wrap(facets = ~smoke) +
  ggtitle("Low birthweight Vs Age of mother By Smoking status")
```
And now a plot of mother's weight vs `low` by smoking status, with loess lines:
```{r}
ggp2 <- ggplot(data = bwt, mapping = aes(x = lwt, y= low)) +
  geom_jitter(mapping = aes(y=low), colour="red",height = 0.01) +
  geom_smooth(method = "loess", colour="blue", se = FALSE, span=2/3) +
  facet_wrap(facets = ~smoke) +
  ggtitle("Low birthweight Vs Weight of mother By Smoking status")
```
Now the two plots are actually printed in only one display:
```{r,fig.cap="Graph of low Vs. age and Vs. lwt by smoke with loess"}
grid.arrange(ggp1, ggp2, nrow = 2)
```

`low` does not seem to have a strong dependence from mother's age, while a relation between `low` and mother's low weight seems to exist. It is not clear if mother's smoking status affects `low` probability.

Now, a first simple model with main effects only is tried:
```{r}
fm1 <- glm(low ~ age + lwt + smoke, data = bwt, family = binomial())
summary(fm1)
```

From the results of this model, a marginally significant relation between `low` and `age` and between `low` and `smoke` seems to exist. Low mother's weight seems to reduce the probability of infant low weight, while mother's smoking seems to increase the probability of low weight.

Now, let us try a more complex model, with all main effects and second degree interactions:
```{r}
fm2 <- update(fm1, . ~ .^2)
summary(fm2, corr = F)$coefficients
```
Try to find useless (non significant) terms using the `dropterm()` function.  
This function performs a "Type II Sum of Squares"-like significancy test on all removable model parameters.  
After calling `dropterm()`, the less significat term is removed by using `update()`.
```{r}
dropterm(fm2, test = "Chisq")
fm3 <- update(fm2, . ~ . -age:lwt)
anova(fm3, test = "Chisq")
```
Now, since only main effects are significant, a test is performed to check if, globally, all remaining interaction terms are significant:
```{r}
anova(fm1, fm3, test = "Chisq")
```

Removing first-order interactions seems to not affect the goodness of model in terms of explicative power.

Now, a main-effects model is tried, but adding some second-degree terms on quantitative preditors.  
This should allow checking if some curvature exists in `age` and `lwt`. 
```{r}
fm4 <- update(fm1, . ~ . + I(age^2) + I(lwt^2))
summary(fm4)
```
Now let us check if curvature effects are globally significant:
```{r}
anova(fm1,fm4,test="Chisq")
```
Curvature effects are not significant.

Next code line compares the two-way interactions complete model with the model with main effects only, to see if all two-way interactions are globally significant:
```{r}
anova(fm1,fm2,test="Chisq")
```
Two-way interactions are globally non significant.

Finally, check significancy of main effects by using "Type II Sum of Squares"-like tests:
```{r}
dropterm(fm1,test="Chisq")
```

And then `age` is removed from model:
```{r}
fm0 <- update(fm1, . ~ . -age)
summary(fm0)
```
This seems the final model.  
Now the diagnostic graphs on model residuals. 
```{r,fig.cap="Residual plots of final model"}
op <- par(mfrow = c(2, 2))
plot(fm0)
par(op)
```

The diagnostic graphs are not really nice, but similar configurations of points is not infrequent, when the response variable is a Bernoulli (not Binomial) one.

And now the prediction for the observed range of prediction data values
```{r}
bwt$prob <- predict(fm0, type = "response")
bwt <- bwt[order(bwt$prob),]
```
And the plot of model predictions: 

<!--

vecchia versione chunk (non funziona riga commentata)

```{r,tidy=FALSE,fig.cap="Plot of final model predictions with data points", purl = FALSE, eval=FALSE}
#bwtPred  <- reshape(bwt, varying = list(c(1,5)), direction = "long")
ggp <- ggplot(data = bwt, mapping = aes(x = lwt, y = low)) +
  geom_point(colour="blue") + 
  geom_line(mapping = aes(x = lwt, y = prob), colour="red") +
  facet_wrap(facets = ~smoke)
print(ggp)
```

-->

```{r,tidy=FALSE,fig.cap="Plot of final model predictions with data points"}
ggp <- ggplot(data = bwt, mapping = aes(x = lwt, y = low)) +
  geom_point(colour="blue") + 
  geom_line(mapping = aes(x = lwt, y = prob), colour="red") +
  facet_wrap(facets = ~smoke)
print(ggp)
```


## Example: Toxoplasmosis

### Data description

In 34 cities of El Salvador, `num` people where examined and a `prop` proportion of them results affected by toxoplasmosis.
The main question is: toxoplasmosis incidence may be due to rain?

Variables description:

* `city`: city number
* `rain`: yearly rain (mm)
* `prop`: proportion of people affected by toxoplasmosis
* `num`: examined people
* `ill`: two-columns matrix containing the number of non-illness (illN) and the number of illness (illY)

```{r,echo=FALSE}
rm(list=ls())
```

### Data loading
```{r}
data(toxo)
head(toxo)
str(toxo)
```

### Descriptives
Let us try to plot the relation between proportion of cases of toxoplasmosis and rain.
```{r,tidy=FALSE,fig.cap="Rate of toxoplasmosys cases Vs. rain (point size proportional to sqrt(n)) "}
ggp <- ggplot(data=toxo, mapping = aes(x=rain,y = prop)) +
  geom_point(mapping = aes(size=sqrt(num))) + 
  geom_smooth(method = "loess", mapping = aes(weight= num), col="red")
print(ggp)

```
The plot seems to show a curvilinear relation between independent ad dependent variable

### Inference and models
Firstly we attempt to produce some models that roughly fit the apparent relation between `rain` and `toxo`:
```{r}
fm1 <- glm(ill ~ rain, family = binomial(), data = toxo)
summary(fm1)
fm2 <- glm(ill ~ rain + I(rain^2), family = binomial(), data = toxo)
summary(fm2)
fm3 <- glm(ill ~ rain + I(rain^2) + I(rain^3), family = binomial(), data = toxo)
summary(fm3)
fm4 <- glm(ill ~ rain + I(rain^2) + I(rain^3) + I(rain^4), family = binomial(), data = toxo)
summary(fm4)
```
The cubic polynomial (in the linear portion of model) seems to be able to describe in some manner the relation between toxoplasmosis and rain.
In fact, the coefficients of `fm3` model are all significant. 

With following lines of code, the overall significancy of rain to describe toxoplasmosis is tested:
```{r}
fm0 <- glm(ill ~ 1, family = binomial(), data = toxo)
summary(fm0)
mc <- anova(fm0, fm3)
pchisq(mc$Deviance[2], mc$Df[2],lower.tail = FALSE)
```
There is a significative influence of rain in the model.  
The above line of code is equivalent to: 
```{r,results='hide'}
anova(fm0, fm3,test="LRT")
```
or to:
```{r,results='hide'}
anova(fm0, fm3,test="Chisq")
```

The plot of estimated model:
```{r,fig.cap="Fitted means plot with data points"}
ggp <- ggplot(data = data.frame(toxo, srt_rain=sort(toxo$rain), fit=fitted(fm3)[order(toxo$rain)]), mapping = aes(x=rain,y=prop)) +
  geom_point(colour="red", size=1.5) +
  geom_line(mapping = aes(x=srt_rain, y=fit), colour="darkblue", size=1.2) +
  geom_hline(yintercept = c(0,1), colour="darkgray") 
print(ggp)
```

And now the graph of diagnostic plots
```{r, fig.cap="Residuals plot of model"}
op <- par(mfrow = c(2, 2))
plot(fm3)
par(op)
```
Indeed, residuals variability seems high:
```{r}
pchisq(fm3$deviance, fm3$df.residual,lower.tail=FALSE)
```
The model is not sufficient to explain data: if the model were totally correct, then the residual deviance of model should distribute asymptotically as a Chi-square with `fm3$df.residual` degrees of freedom.

### "Quasi-binomial" model
Sometimes it is natural to describe data with Binomial (or Poisson) models, but, as in this case, they do not fit well (residual deviance is still to high).  
This may happen in case of overdispersion (or underdispersion) of data.  
In this case, we could need to estimate a dispersion parameter even in case of Binomial (or Poisson) data, using a so-called quasi-binomial (or quasi-poisson) model.

Now let we try a quasi-binomial model:
```{r,tidy=FALSE}
fm_q <- glm(ill ~ rain + I(rain^2) + I(rain^3), 
          family = quasibinomial(), data = toxo)
summary(fm_q)
```
In manner, the estimated dipersion parameter is $\phi=$ ```r summary(fm_q)$dispersion```
Notice that if we want to test the significancy of parameters with `anova()`, we have to use the `F` test:
```{r test_quasi}
fm_q_red <- update(fm_q, .~. -I(rain^3))
anova(fm_q, fm_q_red, test="F")
```
In this case, the third-degree coefficient is still significant.

Now the  diagnostic plots:
```{r,fig.cap="Residual plot for overdispersed (quasi-binomial) model"}
op <- par(mfrow = c(2, 2))
plot(fm_q)
par(op)
```
With quasi-likelihood, estimates of parameters are the same, whereas standard errors (and therefore p-values) change!

Of course, the global Chi-square test on residuals now becomes non significant:
```{r}
pchisq(fm_q$deviance/summary(fm_q)$dispersion, fm_q$df.residual,lower.tail=FALSE)
```

## Example: Education
```{r,echo=FALSE}
rm(list = ls())
```

### Data description
The Raftery and Hout Irish education data.  
Data on educational transitions for a sample of 500 Irish schoolchildren aged 11 in 1967.  
The data were collected by Greaney and Kelleghan (1984), and reanalyzed by Raftery and Hout (1985, 1993).

Variable description:

1. `Sex`: 1 = male; 2 = female.
2. `DVRT`: Drumcondra Verbal Reasoning Test Score.
3. `edlevel`: Educational level attained:
    1. Primary terminal leaver
    2. Junior cycle incomplete: vocational school
	  3. Junior cycle incomplete: secondary school
	  4. Junior cycle terminal leaver: vocational school
	  5. Junior cycle terminal leaver: secondary school
	  6. Senior cycle incomplete: vocational school
	  7. Senior cycle incomplete: secondary school
	  8. Senior cycle terminal leaver: vocational school
	  9. Senior cycle terminal leaver: secondary school
	 10. 3rd level incomplete
	 11. 3rd level complete
4. `lvcert`: Leaving Certificate. 1 if Leaving Certificate not taken; 2 if taken.
5. `fathocc`: Prestige score for father's occupation (calculated by Raftery and Hout, 1985). <!--- 0 if missing. --->
6. `schltype`: Type of school: 1 = secondary; 2 = vocational; 9 = primary terminal leaver.

The goal of study is to find a model that predicts `lvcert` (Leaving Certificate), given the explanatory variables values.

### Data loading
```{r}
data(irished)
head(irished)
str(irished)
```

### Inference and models
We begin with a simple logistic regression model in which the leaving certificate indicator (`lvcert`) is the response
variable and the student's `DVRT` test result is the sole explanatory variable
```{r}
fm1 <- glm(lvcert ~ DVRT, family = binomial(), data = irished)
summary(fm1)
```
Here we see a very significant bivariate association between `DVRT` and leaving certificate indicator.
Is this magnitude of this association substantively large? Let's plot the fitted values against `DVRT`.

Plot DVRT vs leaving certificate (with fitted values)
```{r,fig.cap="jittered lvcert Vs. DVRT with fitted means"}
ds <- cbind(irished, fit=fitted(fm1))
ds <- ds[order(ds$DVRT),]
ggp <- ggplot(data = ds, mapping = aes(x = DVRT, y = jitter(lvcert,factor = 0.05))) +
  geom_point(colour="red", shape=4) + 
  geom_line(mapping =  aes(x = DVRT, y=fit), colour="darkblue")
print(ggp)

```
Here we see that a student at the low end of
the distribution of DVRT scores has about a 0.10 probability of
getting a leaving certificate while a student at the hight end of
the distribution of DVRT scores has about a 0.90 probability of 
getting a leaving certificate. This is a sizable effect.

Now we try to add `sex` to model, and to add the prestige score of the father's education as covariate

```{r}
fm3 <- glm(lvcert ~ DVRT + fathocc + sex, family = binomial(), data = irished)
summary(fm3)
```
And now a test to check significancy of `sex`.
```{r}
anova(fm3, update(fm3, . ~ . -sex), test = "Chisq")
```
`sex` looks as significant. This confirms the results of Wald tests on same parameter.

And a significancy test on `fathocc`
```{r, error=TRUE}
anova(fm3, update(fm3, . ~ . -fathocc), test = "Chisq")
```
Test on `fathocc` returns and error because of presence of `NA` in `fathocc` data.  
To be able to test `fathocc`, missing data should be removed prior to model data (see next computations).

Anyway, to check adequacy of model, a test based on Deviance can be produced:
```{r}
pchisq(q=fm3$deviance,df=fm3$df.residual,lower.tail=FALSE)
```

Following computations produce a three-dimensional graph of estimated model.  
Initially the preditions on observed independent variables range values are calculated:
```{r,tidy=FALSE}
newdata <- with(irished, 
  expand.grid(
    sex = unique(sex),
    DVRT = min(DVRT):max(DVRT),
    fathocc = min(fathocc, na.rm = T):max(fathocc, na.rm = T)
  )
)
newdata$pred <- predict(fm3, newdata = newdata , type = "response")
```
And then a categorized surface plot is drawn:
<!--
```{r,tidy=FALSE,fig.cap="Categorized surface plot of model predictions Vs. DVRT and fathocc by sex using trellis", eval=FALSE, purl=FALSE}
require(lattice) # (For wireframe)
wireframe(pred~DVRT+fathocc | sex, data = newdata, 
          drape = T, layout = c(2,1))
```
-->
```{r,tidy=FALSE,fig.cap="Categorized surface plot of model predictions Vs. DVRT and fathocc by sex"}
ggp <- ggplot(data = newdata,mapping = aes(x=DVRT,y=fathocc,fill=pred))+
  geom_tile() + 
  facet_wrap(facets = ~sex)
print(ggp)

```
Here we see that DVRT, sex, and fathocc exert significant effects on
the probability of obtaining a leaving certificate. 

If we are interested in testing whether the inclusion of these
additional covariates improves the fit over the original logistic
regression model with DVRT as the sole covariate, we could perform a
likelihood ratio test. However, to do this correctly we need to
refit the original model to the dataset in which the observations 
with missing values of `fathocc` have been dropped.

Model without `fathocc`:
```{r}
fm4 <- glm(lvcert ~ DVRT+sex, data = na.omit(irished), family = binomial())
summary(fm4)
```
And then the model comparison:
```{r}
anova(fm3, fm4, test = "Chisq")
```
Model without `sex` 
```{r}
fm5 <- glm(lvcert ~ DVRT+fathocc, data = na.omit(irished), family = binomial())
summary(fm5)
```
And then the model comparison:
```{r}
anova(fm3, fm5, test = "Chisq")
```
Model without `sex` and `fathocc`:
```{r}
fm6 <- glm(lvcert ~ DVRT, data = na.omit(irished), family = binomial())
summary(fm6)
```
And then the model comparison:
```{r}
anova(fm3, fm6, test = "Chisq")
```

Clearly, new variables do improve the fit. Significancy of `fathocc` is much bigger than significancy of `sex`.

Last check on model
```{r}
pchisq(q=fm4$deviance,df=fm4$df.residual,lower.tail=FALSE)
```

The p-value obtained is small, but it is not "extremely" small. Anyway, some improvements could be obtained by adding other variables or by conducting more deep studies.

