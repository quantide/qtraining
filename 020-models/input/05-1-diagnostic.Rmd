---
title: "Diagnostics"
---


```{r setup, include=FALSE, purl=FALSE}
require(knitr)

opts_chunk$set(list(dev = 'png',fig.cap='',fig.show='hold',dpi=100,fig.width=7, fig.height=7,fig.pos='H!',fig.path="images/app-"))
```

 
Even a very high "adherence" of model to observed data (for example, when $R^2 = 0.9$ or greater in linear models) is not sufficient to say that the model is correctly specified and follows the assumptions of (generalized) linear models.

In order to complete the diagnostic analysis of the model, an analysis of residuals is needed too. The following plots are then crucial:

* standardized (Pearson) residuals $r_i^*$ versus fitted values $\hat{y}_i$, for verifying if no relation between residuals and fitted values exists;
* normal qq-plot of residuals, for evaluating the normality assumption (for LM) or approximate normality of pearson residuals (for GLM).

The function `plot(<object>)`, where `<object>` is a LM or GLM class object, by default returns:

* the two plots just described above;
* a plot of the square root of standardized residuals (or standardized deviance residuals for GLMs) versus fitted values, useful for evaluating homoscedasticity assumptions;
* a plot of standardized residuals versus leverage, useful for evaluating the presence of outliers (see below in this chapter).

## Residuals

*Basic residuals* are computed in linear and generalized linear models as:
$$
r_i = y_i - \hat{y}_i
$$

Other variants or transformations are:

* _Pearson residuals_ ("standardized" residuals: usually used in tests of assumptions)
* _Deviance residuals_ ($r_{Di}=\textrm{sign}(r_i) \cdot \sqrt{d_i}$, where $d_i$ is the contribution to total deviance of the $i$-th observation). This type of residual is used in GLM models
* _Anscombe residuals_ ("normalized" and "homoscedasticized" residuals, if model assumptions are met). This type of residual is also used with Generalized Linear Models, but no functions in R base are available to calculate them.
* _Studentized residuals_, computed by $\frac{r_i}{\hat{\sigma}_i \sqrt{1-h_{ii}}}$, (see below for a definition of $h_{ii}$) that account for greater variability of residuals at extremes of sampling space of independent variables.


_Standardized (Pearson)_ residuals are computed by 
      $$
      \frac{y_i - \hat{\mu}_i}{\hat{\sigma}_i}
      $$
where $\hat{\mu}_i$ is the estimated expected value of $y_i$, and $\hat{\sigma}_i$ is the estimated standard deviation of $y_i$ (in case of GLM, $\hat{\sigma}_i = \sqrt{\hat{\phi} V(\hat{\mu}_i)}$). 

_Anscombe_ residuals $r_{A_i}$ are calculated in the following way:
$$
r_{A_i} = \frac{A(y_i)-A(\hat\mu_i)}{A'(\hat\mu_i)\sqrt{V(\hat\mu_i)}}\,,
$$
except for binomial distribution with $n_i$ trials in the $i$-th observation, where the Anscombe residuals are:
$$
r_{A_i} = \sqrt{n_i}\frac{A(y_i)-A(\hat\mu_i)}{A'(\hat\mu_i)\sqrt{V(\hat\mu_i)}}
$$
$A(\cdot)$ is a proper function which makes the distribution of the response variable $y$ closer to the normal distribution and is given by
$$
A(\mu) = \int_{-\infty}^\mu V(t)^{-\frac{1}{3}}\textrm{d}t
$$

As a result, Anscombe residuals have a different formulation for each distribution.

In Normal case:
$$
r_{A_i} = y_i - \hat\mu_i
$$
In  Binomial case: 
$$
r_{A_i} = \sqrt{n_i}\left[B\left(y_i,\frac{2}{3},\frac{2}{3}\right) - B\left(\hat\mu_i,\frac{2}{3},\frac{2}{3}\right)\right]{[\hat\mu_i(1-\hat\mu_i)]}^{-\frac{1}{6}}\,,
$$
where $B(z,a,b)$ is the beta function

In Poisson case:
$$
r_{A_i} = \frac{3}{2}\frac{y_i^{\frac{2}{3}} - \hat\mu_i^{\frac{2}{3}}}{\hat\mu_i^{\frac{1}{6}}}
$$

In Gamma case: 
$$
r_{A_i} = 3\left[{\left(\frac{y_i}{\hat\mu_i}\right)}^{-\frac{1}{3}}-1\right]
$$

## Residual diagnostics
Residual diagnostic tests the fulfillment of the assumptions of the (generalized) linear model:

* testing the assumption of homogeneity of variance of standardized residuals;
* testing for normality of residuals (mainly with linear models);
* testing for outliers.

### Homogeneity of residual variance tests

The assumption of _homogeneity of error variance_ in simple linear models can also be performed with the Levene's test, if data are grouped.

Levene's test is performed by computing:
      $$
      Z_{ij} = \vert y_{ij} - E(y_{.j}) \vert
      $$
where $j$ represents the group and $i$ the replication within the group, and then computing an F test on the $j$ groups.
A nonsignificant result indicates no heteroscedasticity.

### Residual normality tests

The _residual normality_ can be also checked by means of normality tests, such as *Anderson-Darling test*.  
The Anderson-Darling test computes square mean difference between the empirical cumulative distribution (computed on residuals) and theoretical cumulative distribution of the standard normal distribution, via: 
      $$ 
      AD = \sum_{i=1}^n {\frac{1-2i}{n}}\{\ln(F_0[Z_{(i)}])+\ln(1-F_0[Z_{(n+1-i)}])\} - n
      $$
where $n$ is the sample size, $Z_{(i)}$ are observed ordered and standardized residuals and $F_0$ is the cumulative distribution function of the standard normal distribution.  
The value of $AD$ increases when the difference between residuals empirical distribution and gaussian cumulative distribution grows up.


### Checking for outlier presence

Outliers can be detected:

* By looking for standardized residuals greater than 3.5 or less than -3.5
* and by looking for high Cook's $D_i$, greater than $4p/(n-p-1)$.  
  For instance, if $n = 100$, $p = 5$ then high Cook's $D_i$ are high when higher than $4 \cdot 5 / (100-5-1) = 20/94$.

*Cook's* $D$ are useful to recognize outliers, and are computed by
      $$
      \text{Cook's }D_i = \left(\frac{1}{p}\right)\left(\frac{h_{ii}}{1-h_{ii}}\right)\left(\frac{r_i^2}{\hat{\sigma}_i^2(1-h_{ii})}\right)
      $$
where $h_{ii}$ is the $i$-th diagonal entry of the hat matrix $\underline{H}$ (see below).

Cook and Weisberg (1982) suggested that values of $D_i$ that exceed 50\% of the $F$ distribution with $p$ and $n-p$ degrees of freedom are large

The *hat matrix* $\underline{H}$ transforms $\underline{Y}$ into the predicted scores. It is computed by
      $$
      \underline{H} = \underline{X}(\underline{X}^T \underline{X})^{-1}\underline{X}^T
      $$
The trace of the hat matrix is equal to the number of variables in the model.  

The diagonal values of the hat matrix indicate which values could be outliers or not.
The diagonal values are therefore measures of so-called _Leverage_.

Leverage is a measure of influence of individual data point on global parameter estimates.  
Leverage is bounded by two limits: $1/n$ and $1$. The closer the leverage is to unity, the more leverage the value has, and then the more is the influence of specific data point on model estimate.  
In other words, if a data point has a very high leverage value, then the model estimate may change a lot using or not using that point.

When the leverage is greater than $2p/n$ there is high leverage according to Belsley et al. (1980), cited in Long J.F., Modern Methods in Data Analysis (page 262). For smaller samples, Vellman and Welsch (1981) suggested that $3p/n$ is the criterion.

The leverage of outliers can also be assessed:

* constructing and analyzing studentized residuals;
* constructing and analyzing the leverage of the high and low studentized residuals;
* using Cook's $D$ to help determine how problematic outliers are.
