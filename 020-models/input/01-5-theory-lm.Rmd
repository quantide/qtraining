---
title: "Some Theory on Linear Models"
---

```{r setup, include=FALSE, purl=FALSE}
require(knitr) 
opts_chunk$set(list(dev = 'png',fig.cap='',fig.show='hold',dpi=100,fig.width=7, fig.height=7,fig.pos='H!',fig.path="images/lm-"))
```

Next paragraphs contain some basic information to understand the theory of Linear Models, and their application in R.

## Smoothing
Regression is the study of the change of the distribution of one variable, $y$, according to the value of another variable, $x$.
Both continuous and discrete variables can be included in regression problems.

Formally, a relationship between $x$ and $y$ usually means that the expected value of $y$ is different for different values of $x$.
At this stage, changes in $\sigma^2$ (the variance) or other aspects of the distribution are not considered.

When $y$ is continuous, changes in $y$ are smooth (continuous and differentiable), and the model used is:
$$
E(y \vert x) = g(x)
$$
for some unknown smooth function $g(\cdot)$.

## Regression
A variety of functions can be used to estimate $g(\cdot)$, called **regression function**. These functions are called **scatterplot smoothers**.

For a pair $(x_i, y_i)$, $i = 1, \dots, n$ the model states that:
      $$
      y_i = g(x_i) + \varepsilon_i
      $$

Where:

* $n$ is the sample size, or the number of measured units used to estimate the model. 
* $\varepsilon_i \; (i = 1, \dots, n)$ are $n$ $\text{ i.i.d.}$ random values arising from a Normal distribution with 0 as expected value and $\sigma$ as standard deviation ($\varepsilon_i \sim N(0,\sigma^2)$)

## Linear model
In general, a linear model may be expressed in the form:  
      $$
      E(y_i)\equiv E(y_i\vert\underline{x}_i) = \beta_0 + \displaystyle\sum_{j=1}^p \beta_j x_{ij}
      $$
And the above formula can be rewritten as  
      $$
      \underline{\mu} = E(\underline{y}) = \underline{X} \underline{\beta}
      $$
where $\underline{X}$ is usually a $n \times (p+1)$ matrix which includes all ones in the first column (the intercept column) and the $p$ explanatory variables in the other columns; $\underline{\beta}$ is a vector of $\beta_0, \beta_1, \dots, \beta_p$.

## Deterministic and random components
The general form of the linear model is then:
      $$
      \underline{y} = \underline{X} \underline{\beta} + \underline{\varepsilon}
      $$
or    
      $$
      y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \dots + \beta_p \cdot x_{ip} + \varepsilon_i
      $$
or
      $$
      y_i \sim N(\mu_i, \sigma^2)
      $$  
with 

$$
\mu_i = \beta_0 + \sum_j \beta_j x_{ij} \text{ and } \varepsilon_i \sim N(0, \sigma^2) \text{ i.i.d.}.
$$

This model has then $p+2$ unknown parameters: $\beta_0, \beta_1, \dots, \beta_p, \sigma^2$, and:

$$
\underline{X}\underline{\beta} = \underline{\mu}
$$  

is the **systematic** part, or **deterministic** part, or **signal**; it represents the explanaible differences between populations.  
The value:

$$\underline{\varepsilon}=(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)^T$$

is the **random** or **noise** component. 

Finally, following relationship between _response_, _signal_ and _noise_ holds:

 $\phantom{is the random or noise comp}$ **response** $=$ **signal** $+$ **noise**.

**Summarizing**: models where

* signal is a linear function of parameters
* response is a linear function of signal and noise

are said **linear models**. 

## Maximum Likelihood estimate
The likelihood function is the joint probability density function of observed values:  
       $\phantom{aaaaaaaaaaaa}L(\beta_0, \dots, \beta_p, \sigma^2) = \displaystyle\prod_{i=1}^n p(y_i\vert\beta_0, \dots, \beta_p, \sigma^2)$  
            $\phantom{aaaaaaaaaaaaL(\beta_0, \dots, \beta_p, \sigma^2) }= \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2}\left(\frac{y_i-\mu_i}{\sigma}\right)^2}$  
            $\phantom{aaaaaaaaaaaaL(\beta_0, \dots, \beta_p, \sigma^2) }= \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}{\left[y_i-\left(\beta_0+\sum_j \beta_j x_{ij}\right)\right]}^2}$  
            $\phantom{aaaaaaaaaaaaL(\beta_0, \dots, \beta_p, \sigma^2) }= (2\pi\sigma^2)^{-\frac{n}{2}} e^{-\frac{1}{2\sigma^2} \sum_i\left[y_i-\left(\beta_0+\sum_j \beta_j x_{ij}\right)\right]^2}$

but evaluated with respect to unknown parameters.

**MLE** (Maximum Likelihood Estimate) is obtained by taking the log of the likelihood expression  

$\phantom{aaaaaaaaaaaa}\ell(\beta_0, \dots, \beta_p, \sigma^2) = \log L(\beta_0, \dots, \beta_p, \sigma^2) \\  
      \phantom{aaaaaaaaaaaa\ell(\beta_0, \dots, \beta_p, \sigma^2) } = c - \frac{n}{2} \log\sigma^2 -\frac{1}{2\sigma^2} \sum_i\left[y_i-(\beta_0+\sum_j \beta_j x_{ij})\right]^2,$  
      
(the so-called **log-likelihood**), differentiating with respect to each parameter in turn, setting the derivative equal to 0 and solving.  
This obtains a linear equations system:  
      $$
      \begin{cases} 
          \frac{1}{\hat{\sigma}^2} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right] = 0 \\
          \frac{1}{\hat{\sigma}^2} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right] x_{i1} = 0 \\
          \hspace{2cm} \vdots \\
          \frac{1}{\hat{\sigma}^2} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right] x_{ip} = 0 \\
          -\frac{n}{\hat{\sigma}^2} + \frac{1}{\hat{\sigma}^3} \sum_i\left[y_i-(\hat{\beta}_0+\sum_j \hat{\beta}_j x_{ij})\right]^2 = 0
        \end{cases}
      $$     

The first $p+1$ equations can be multiplied by $\hat\sigma^2$, yielding $p+1$ linear equations in the $p+1$ unknown $\hat\beta$'s.

Since they are linear, equations can be solved by linear algebra:
      $$
      \underline{\hat{\beta}} = (\underline{X}^T\underline{X})^{-1} \underline{X}^T \underline{y}
      $$
from which
      $$
      \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_p x_{ip}
      $$
where $\hat{y}_i$ are the **fitted values**.

Also, the values: 
$$
r_i = y_i - \hat{y}_i
$$
are said **residuals**, and are estimates of $\varepsilon_i$ (for this reason, sometime they are represented with $\hat\varepsilon_i$ symbol).

The MLE for $\sigma^2$, $\hat{\sigma}^2$, is found from the last equation, that can be rewritten as

$$
-\frac{n}{\hat{\sigma}} + \frac{1}{\hat{\sigma}^3}\sum_i{r_i^2} = 0
$$

from which  
$$
\hat{\sigma}^2_{ML} = \frac{1}{n} \sum_i r_i^2
$$  
Note that an unbiased, "non ML", and more often used  version of $\sigma^2$ estimate is: 
$$
\displaystyle{\hat{\sigma}^2 = \frac{1}{n-p-1} \sum_i r_i^2}.
$$

## Assumptions on linear model
As we have seen before, the main assumption on linear model is that the model itself is linear in parameters.

Other assumptions concerning the error terms $\underline{\varepsilon}$ and the "components" of linear models are then:  

* $E(\varepsilon_{i}) = 0 \,\,\,(i = 1, \dots, n)$
* $Var(\varepsilon_{i}) = \sigma^2  \,\,\,(i = 1, \dots, n )$ (homoscedasticity)
* $\underline{X}$ is not stochastic
* $\varepsilon_i \sim\; N(0,\,\sigma^2) \,\,\,(i = 1, \dots, n )$
* $Corr(\varepsilon_{i},\,\varepsilon_{i-j}) = 0 \,\,\,(i = 1, \dots, n;\, 0 < j < i)$ (incorrelation of residuals)

## Distribution of estimators
If linear model assumptions are met, $\underline{\hat{\beta}}$ are random variables with a known distribution (except for $\underline\beta$ and $\sigma$ parameter values).

Indeed, if the model is correctly specified, then 
$$
\underline{\hat\beta} \sim N\left(\underline\beta, \sigma^2{(\underline{X}^T\underline{X})}^{-1}\right).
$$

It is then possible:

1. To test hyphoteses of the form $H_0: \beta_j = \beta_j^*$.
2. To provide confidence intervals for the regression parameters $\beta_j$.

In particular, testing the hypothesis that $\beta_j = 0$ is of particular interest, as its rejection means that the $j$-th variable is significant, i.e. that it explains in some manner the behaviour of response variable, net of the other explanatory variables.

### Test on one parameter

To test if $\beta_j$ is equal to some value $\beta_j^*$, $j = 0, 1, \dots, p$, the Wald $t$-test statistic can be used:
      $$
      t = \frac{\hat{\beta}_j-\beta_j^*}{\hat{se}(\hat\beta_j)} 
      $$
where $\hat{se}(\hat\beta_j)$ denotes the estimated standard error of the estimator $\hat{\beta}_j$ and is the square root of the $(j+1)$-th element of the diagonal of the matrix $\hat\sigma^2{(\underline{X}^T\underline{X})}^{-1}$, with $\hat\sigma^2 = \frac{1}{n-p-1} \sum_i r_i^2$, the unbiased estimate of $\sigma^2$.

When $H_0$ is true, $t$-test statistics follows a *Student's t* distribution with $n - p - 1$ degrees of freedom, where $p + 1$ is the number of estimated regression parameters.

### Confidence interval on one parameter

Given the considerations about the distribution of previous $t$-test, the $(1 - \alpha)$ confidence interval for $\beta_j$ may then be calculated by replacing $\beta_j^*$ with $\beta_j$ in above Wald $t$-test formula, so obtaining the following:
  $$
  \left[\hat{\beta}_j - t_{n-p-1;\frac{\alpha}{2}} \cdot \hat{se}(\hat\beta_j);\, \hat{\beta}_j + t_{n-p-1;\frac{\alpha}{2}} \cdot \hat{se}(\hat\beta_j)\right]
  $$
where $t_{k;\gamma}$ is the $\gamma$-th right percentile of Students-t distribution with $k$ degrees of freedom.

## Comparing two nested models: $F$ test

An $F$ test can be used to compare two models and to choose one of them too: suppose we are interested to compare the "complete" model ($p +1$ parameters)
  $$
  y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_px_{ip}
  $$
with the "reduced" model ($p_0 +1$ parameters)
  $$
  y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_{p_0}x_{ip_0}\,,\quad p_0 < p
  $$
that is to verify the hypothesis $H_0: \beta_{p_0+1} = \beta_{p_0+2} = \ldots = \beta_{p} = 0$.  
The following $F$ test can be calculated:
  $$
  F = \frac{(SS_{R_0} - SS_{R_1})/(p-p_0)}{SS_{R_1}/(n-p-1)}
  $$
Where $SS_{R_0}$ and $SS_{R_1}$ are the residual Sum of Squares (see the next paragraph for definitions of Sum of Squares) for the reduced and the complete model, respectively.

If the null hypothesis holds, then $F \sim F_{p-p_0, n-p-1}$,

where $F_{a, b}$ is the Fisher-Snedecor distribution with $a$ (numerator) and $b$ (denominator) degrees of freedom.

If the previous $F$ test refuses $H_0$, the complete model is maintained; otherwise, if the $F$ test does not refuse $H_0$, the reduced model is used, since the last $p-p_0$ variables result not significant in order to explain the response variable.

Notice that using the formula above, the $F$ test simply establishes if the difference (in terms of residual variability $SS_R$) between nested models is significant or not.

## Determination coefficient $R^2$
The **Determination Coefficient** is defined as:
$$
R^2 = 1 - \frac{SS_{R}}{SS_{TOT}} = \frac{SS_E}{SS_{TOT}}\,,
$$

where $SS_R = \sum_i r_i^2$ is the residual sum of squares (residual deviance), $SS_E = \sum_i {(\hat{y}_i - \overline{y})^2}$ is the regression sum of squares (explained deviance) and $SS_{TOT} = SS_E + SS_R = \sum_i {(y_i - \overline{y})^2}$ is the total sum of squares (total deviance).

$R^2$ can take values in the $[0,1]$ interval and represents the percentage of response variability explained by explanatory variables. 

If $R^2=0$, then the explanatory (independent) variables don't add any information to explain the dependent variable; if $R^2=1$, then the explanatory (independent) variables explain all the variability of dependent variable.

If $p=1$, then $R^2 = \widehat{[Corr(\underline{x}, \underline{y})]}^2$

where $\widehat{Corr(\underline{x}, \underline{y})}$ is the estimate of Pearson linear correlation index using $\underline{x}$ and $\underline{y}$ data.

A so-called "adjusted" version of $R^2$ also exists. It is the $R^2$ corrected by the degrees of freedom of sums of squares:

$$
\overline{R}^2 = 1 - \frac{n-1}{n-p-1}\frac{SS_R}{SS_{TOT}}=1 - \frac{n-1}{n-p-1}\left(1-R\right)
$$

And then it can assume values in the $[- \frac{p}{n-p-1}, 1]$ interval. 

### Comparing two nested models: alternative formula

Returning back to the nested models hypothesis testing formula, one can see that $F$ may be rewritten as:
$$
  F = \frac{(R_1^2-R_0^2)/(p-p_0)}{(1-R_1^2)/(n-p-1)}
$$

Where $R_0^2$ and $R_1^2$ are the determination coefficients for the reduced and the complete model, respectively.

In this case, using the formula with the $R^2$ coefficients, $R_1^2-R_0^2$ is in the numerator, and this because when two models are nested, the "parent" model always has a greater $R^2$. Also, the $F$ test as seen with this formulation, establishes if the difference (in terms of explained variability, $R^2$) is significant or not.
<!--- 
% ### Inference using estimable functions (generalization of tests)
As previously seen, it may be shown that 
$$
Var(\underline{\hat\beta}) = \sigma^2 {\left( \underline{X}^T \underline{X} \right)}^{-1} 
$$ 
and then
$$ 
Var \left( \underline{K}^T \underline{\hat\beta} \right) = \sigma^2 \underline{K}^T {\left( \underline{X}^T \underline{X} \right)}^{-1} \underline{K}
$$
where $\underline{K}$ is a $((p+1) \times k)$ matrix, and $\underline{K}^T \underline{\beta}$ is a **estimable function**.

The statistic for $H_0 = \underline{K}^T \underline{\beta} = \underline{K}^T \underline{\beta_0}$ is
$$
\dfrac{\left( \underline{K}^T \underline{\hat\beta} - \underline{K}^T \underline{\beta_0} \right)^T \left[ \underline{K}^T \left( \underline{X}^T \underline{X} \right)^{-} \underline{K} \right] \left( \underline{K}^T \underline{\hat\beta} - \underline{K}^T \underline{\beta_0} \right)/\nu}{SSR/(n-\nu)}
$$
Where $\nu = \text{rank}(\underline{K})$.

The statistic, under $H_0$, is distributed as a $F_{\nu,n-\nu}$ 

As a particular case, the scalar form is the quare of Wald statistic shown above:
$$
\frac{(\hat\beta - \beta_0)^2}{Var(\hat\beta - \beta_0)}
$$
--->

## A particular case of $F$ test for nested models
When $p_0 = 0$, that is when the reduced model has only the intercept, we get the model goodness-of-fit $F$ test for the complete model.
It tests  
  
$H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0$,  
  
that is if $H_0$ is refused, at least one parameter included in the complete model is significant. Since in this case $R^2_0 = 0$, and $SS_{R_0} = \sum_i {(y_i - \overline{y})}^2 = SS_{TOT}$ we call $R^2_1 = R^2$ and $SS_{R_1} = SS_R$:
  $$ 
  F = \frac{(SS_{TOT} - SS_R)/p}{SS_R/(n-p-1)} =
      \frac{R^2/p}{(1-R^2)/(n-p-1)}\,,
  $$

$F \sim F_{p, n-p-1}$ under $H_0$.
This $F$ test is also a way to associate a p-value to the coefficient $R^2$

In **R** 
```
summary(lm(formula))
``` 
reports both the value of $F$, its p-value and the value of $R^2$.

## Prediction in linear models (LM)

Linear regression can be used to fit a predictive model to an observed data set of $\underline{y}$ and $\underline{X}$ values. After developing such a model, if an additional vector of independent variable values $\underline{x}$, say $\underline{x}_{n+1}$, is given without its accompanying value of $y_{n+1}$, the fitted model can be used to make a prediction of the value of $y_{n+1}$.

It can be shown that the $(1-\alpha)$ confidence interval for the regression for a given value of $\underline{x}_*$ is limited by the following lower and upper confidence limits (_LCL_ and _UCL_):
$$
LCL: g(\underline{x}_*;\hat{\underline{\beta}})-t_{n-p-1;\,\frac{\alpha}{2}}\cdot \hat{\sigma} \cdot \sqrt{\underline{x}_*^T(\underline{X}^T\underline{X})^{-1}\underline{x}_*}
$$   

$$
UCL: g(\underline{x}_*;\hat{\underline{\beta}})+t_{n-p-1;\,\frac{\alpha}{2}}\cdot \hat{\sigma} \cdot \sqrt{\underline{x}_*^T(\underline{X}^T\underline{X})^{-1}\underline{x}_*}
$$ 

where $g(\underline{x}_*;\hat{\underline{\beta}})$ is the regression function evaluated in $\underline{x}_*$, $\hat\sigma$ is the estimated standard deviation of residuals, $\hat{\underline{\beta}}$ is the vector of $p+1$ parameters estimated in model (intercept included), and $t_{n-p-1;\,\frac{\alpha}{2}}$ is the value of the $t_{n-p-1}$ distribution that has a probability of having values greater than it equal to $\frac{\alpha}{2}$.

The confidence interval may be thought as the interval containing the "true" regression line (the "true" mean, given $\underline{x}$) with a given confidence level.

Instead, the prediction interval will contain, with a given confidence level, the true (and "future") $y_{n+1}$ value, given $\underline{x}_{n+1}$ values.

The $(1-\alpha)$ prediction interval for a given value of $\underline{x}_{n+1} = \underline{x}_*$ is limited by the following lower and upper prediction limits (_LPL_ and _UPL_):
$$
LPL: g(\underline{x}_*;\hat{\underline{\beta}})-t_{n-p-1;\,\frac{\alpha}{2}}\cdot \hat{\sigma} \cdot \sqrt{\underline{x}_*^T(\underline{X}^T\underline{X})^{-1}\underline{x}_*+1}
$$ 

$$
UPL: g(\underline{x}_*;\hat{\underline{\beta}})+t_{n-p-1;\,\frac{\alpha}{2}}\cdot \hat{\sigma}  \cdot \sqrt{\underline{x}_*^T(\underline{X}^T\underline{X})^{-1}\underline{x}_*+1}
$$ 


## Suggestions for enhancing a Linear model (LM)

If a linear model does not satisfy the assumptions or its $R^2$ is still too low, we can try to enhance it by using several techniques:

* transform the response variable with a function $t(\cdot)$ and perform the regression with $t(\underline{y})$; possible transformations: log, square root, reciprocal, Box-Cox;
* add explanatory variables raised to some power ($x_{ij}^2, x_{ij}^3, \ldots$);
* transform explanatory variables;
* add other explanatory variables, if available;
* change model, performing a GLM, a LMM or a GLMM.

In order to have suggestions on possible transformations to be applied to the response variable and/or to the explanatory variables, plots of residuals on fitted values and of residuals on explanatory variables may be helpful.

## Box-Cox transformation

Box-Cox is a family of transformations of dependent variable that may help the statistician to enhance linear models (LM).  
If the residuals of model are non-normal and/or heteroscedastic, a power transformation of dependent variable sometime may allow one to obtain a correct linear model on transformed variable.  
The only requirement for the application of Box-Cox transform is that $y_i>0$. 

Let us start with something simple, like a simple linear regression, i.e. 
    $$
    y_i=\beta_0+\beta_1x_i+\varepsilon_i
    $$ 
The Box-Cox transformation uses following family of (power) transformations 
    $$
    y_i^{(\lambda)}=\begin{cases}
     & \dfrac{y_i^\lambda-1}{\lambda} \text{    } (\lambda\neq0)\\ 
     & \log(Y_i) \text{    }(\lambda=0)
    \end{cases}
    $$

The log-likelihood of this model (assuming that transformed observations are independent, with distribution $\varepsilon_i\sim\mathcal{N}(0,\sigma^2)$) depends also on the parameter $\lambda$:  

$\ell(\lambda)=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n\left[Y_i^{(\lambda)}-(\beta_0+\beta_1 X_i)\right]^2+$  
     $\phantom{\ell(\lambda)=}+(\lambda-1)\sum_{i=1}^n\log(Y_i)$

The $\lambda$ value that maximizes the above formula is the value that "best approximates" the distribution of regression residuals on transformed data to a Normal one.



## t-test and ANOVA are regressions!

### One sample $t$-test
One-sample Student's $t$-test is equivalent to the linear regression of the intercept-only model.  
The one-sample $t$-test p-value is equal to the intercept regression parameter ($\beta_0$) p-value, where the response variable minus $\beta^*_0$ is measured (see examples in above chapters).

### Two independent samples t-test
Student's $t$-test for independent samples is equivalent to the linear regression of the response variable on the grouping variable.  
The $t$-test for indipendent samples p-value is equal to the slope regression parameter ($\beta_1$) p-value.  
The linear regression model says data are normally distributed about the regression line with constant standard deviation. The grouping variable takes on only two values.  
Therefore, there are only two locations along the regression line where data are. "Homoscedastic normally distributed values about the regression line" is equivalent to "two normally distributed populations with equal variances".  
Thus, the hypothesis of equal means ($H_0: \mu_A - \mu_B = 0$) is equivalent to the hypothesis that the regression coefficient of $x$ is 0 ($H_0: \beta_1 = 0$). That is, the population means are equal if and only if the regression line is horizontal (see examples).

### ANOVA
More than two samples can be compared using ANOVA. In the ANOVA, the categorical variable is effect coded, which means that each category's mean is compared to the grand mean.  
In the regression, the categorical variable is dummy coded, which means that each category's intercept is compared to the reference group's intercept.  
So an ANOVA reports each mean and a p-value that says at least two are significantly different. A regression reports only one mean (as an intercept), and the differences between that one and all other means, but the p-values evaluate those specific comparisons.  
Thus, the hypothesis of equal means ($H_0: \mu_0 = \mu_1 = \dots = \mu_p$) of the ANOVA is equivalent to the hypothesis that the regression dummy coefficients $\beta_1, \beta_2, \dots, \beta_p$ are all equal to zero (see examples).
