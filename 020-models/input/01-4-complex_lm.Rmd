---
title: "Complex Linear Models Examples"
---

```{r setup, include=FALSE, purl=FALSE}
require(knitr) 
opts_chunk$set(list(dev = 'png',fig.cap='',fig.show='hold',dpi=100,fig.width=7, fig.height=7,fig.pos='H!',fig.path="images/lm-"))
```



```{r,message=FALSE}
require(ggplot2)
require(dplyr)
require(car)
require(MASS)
require(gridExtra)
require(GGally)
require(qdata)
```

## Example: Hardness and Density regression
```{r, echo=FALSE}
rm(list=ls())
```
### Data description

Janka Hardness is an importance rating of Australian hardwood timbers. The test itself
measures the force required to imbed a steel ball into a piece of wood and therefore 
provides a good indication to how the timber will withstand denting and wear.
Janka hardness is strongly related to the density of the timber and can usually be 
modelled using a polynomial relationship.  
The dataset consists of density and hardness measurements from 36 Australian Eucalypt hardwoods.  
Variables in dataframe are:

* Density: Density measurements
* Hardness: Janka hardness

### Data loading
```{r}
data(janka)
head(janka)
str(janka)
```

### Descriptives

Let us begin with a plot showing the relation between `Hardness` and `Density`
```{r,fig.cap="Scatterplot of Hardness Vs. Density with loess line"}
ggp <- ggplot(data = janka, mapping = aes(x = Density, y = Hardness)) +
  geom_point(colour="darkblue", size=2) + 
  geom_smooth(method = "loess", colour="green", se = FALSE, span=0.8)

print(ggp)
```

And now a similar graph but with lm regression line added:

```{r,fig.cap="Scatterplot of Hardness Vs. Density with loess and lm linear fit"}
ggp <- ggplot(data=janka, mapping = aes(x=Density, y=Hardness)) +
  geom_point(colour="darkblue", size=2) + 
  geom_smooth( method = "lm", colour="red", se=FALSE, size=1) +
  geom_smooth( method = "loess", colour="green", se=FALSE, size=1, span=0.8) 
print(ggp)
```
It seems that a simple linear model, or, at most, a slightly curved model, could be used to fit the data.

### Inference and models

Let we start with a simple linear model
```{r}
fm1 <- lm(Hardness ~ Density, data = janka)
summary(fm1)
```
Parameters are all highly significant.

Now a residual plot to check correctness of model:
```{r,fig.show='hold',fig.cap="Residual plot of straight linear model"}
op <- par(mfrow=c(2,2))
plot(fm1)
par(op)
```

One observation (32) seems to be an outlier. Residuals would also suggest to add the square of explanatory variable to the model.  
We will try to "increase" the model until a good result is obtained.

Zoom to first plot on residuals:
```{r,tidy=FALSE,fig.cap="Residuals Vs. fitted values"}
res_fitted_df <- data.frame(res=residuals(fm1), fit=fitted(fm1))

ggp <- ggplot(data= res_fitted_df, mapping = aes(x=fit, y=res)) +
  geom_point(color="darkblue", size=2) + 
  stat_smooth(method="loess", colour="green", se=FALSE, span = 0.8) +
  geom_hline(yintercept = 0, color="red", linetype=2) +
  ylab("Residuals") + xlab("Fitted Values") +
  ggtitle("Hardness ~ Density Residuals Plot")

print(ggp)
```

Since a curvature (and, perhaps, heteroscedasticity, but we will be analyze this later) appear in residuals, a quadratic term is added to model:
```{r}
fm2 <- update(fm1, . ~ . + I(Density^2))
```

Wald's $t$-test p-value's are:
```{r}
summary(fm2)
```
We can ask R to show only a part of the summary
```{r}
summary(fm2)$coef
```

And the ANOVA for nested models is:
```{r}
anova(fm2,fm1)
```
The contribution of quadratic term is significant.

First degree coefficient is not significant: the axis of parabola may be in 0.  
Let us try to add a cubic term:
```{r}
fm3 <- update(fm2, . ~ . + I(Density^3))
summary(fm3)$coef

anova(fm2,fm3)
```
Adding third degree term does not add significant contributions to model.

In this case intercept is not significant.  
However, because of marginality principle, when a p-_th_ degree term of an explanatory variable is significant, statisticians tend to keep all the lower-degree terms (intercept, second degree, third degree, ..., (p-1)-_th_ degree) of the same exploratory variable.  
The same for interactions: if we keep a three-way interaction, also all two-way interactions and main effects of variables included in three-way interaction should we keep in the model.

If, for example, a linear effect within a more complex model is not significant, this does not necessarily mean that the linear effect does not exist. Sometimes (or, perhaps better, frequently), the non-significancy of a term could be simply due to the design or to the chosen measurement unit.  
Consequently, based on marginality principle, it does not make sense to check significancy of main effects (or lower order effects) if interaction effects (or higher order effects) involving main effects are significant.

In this case, if we try to transform the independent variable centering it around its median with:
```{r}
janka <- janka %>% mutate(norm_density = Density - median(Density))
```

This is the plot of relation:
```{r,tidy=FALSE,fig.cap="Hardness Vs. normalized (shifted on median) Density plot"}
ggp <- ggplot(data = janka, mapping = aes(x = norm_density, y = Hardness)) +
  geom_point(colour="darkblue") + 
  geom_smooth(method = "loess", colour="green", se = FALSE, span=0.8) +
  geom_smooth(method = "lm", colour="red", se = FALSE) 

print(ggp)
```

The general "behavior" of relation is mainly the same, but the "normalized" model estimates are:
```{r}
fmd <- lm(Hardness~norm_density+I(norm_density^2), janka)
summary(fmd)$coef
```
Here, the second degree coefficient is identical to the previous one, 
whereas intercept and first degree coefficient are now significant.  
This modified result is obtained only shifting the explanatory variable.
Significativity of lower degree terms may then depend on measurement scale or design.

Finally, notice also that centering the explanatory variables is a tool to "reduce collinearity", and then to reduce correlation between coefficient estimates:
```{r}
summary(fm2,correlation=TRUE)$correlation
summary(fmd,correlation=TRUE)$correlation
```
Now, we make other checks with studentized residuals (see _Some Theory on Linear Models_ chapter):
```{r}
res_fitted_df <- data.frame(res=studres(fmd), fit=fitted(fmd))

ggp1 <- ggplot(data= res_fitted_df, mapping = aes(x=fit, y=res)) +
  geom_point(color="darkblue", size=2) + 
  stat_smooth(method="loess", colour="green", se=FALSE, span = 0.8) +
  geom_hline(yintercept = 0, color="red", linetype=4) +
  ylab("Residuals") + xlab("Fitted Values") +
  ggtitle("Residuals vs Fit Plot")

ggp2 <- ggplot(data = res_fitted_df, mapping = aes(sample = res)) + 
  stat_qq(color="darkblue", size=2) +
  geom_abline(mapping=aes(intercept=mean(res),slope=sd(res)), color="red", linetype=2) +
  xlab("Normal scores") + ylab("Sorted studentized residuals") +
  ggtitle("Residuals Normal Probability Plot")

grid.arrange(ggp1, ggp2, ncol=2)
```

Residual variability seems non constant (heteroscedasticity exists), and seems to increase as the response variable increases. We should search for a transformation of the response variable which produces models with "more normal" (and with reduced heteroscedasticity) residuals.

We will search for the transformation within the Box-Cox family (see _Some Theory on Linear Models_ chapter):
```{r,fig.cap="Box-Cox log-likelihood Vs. Lambda"}
bxcx <- boxcox(fmd,lambda = seq(-0.25, 1, len=20))
```
`boxcox()` returns a plot with the MLE of $\lambda$ and a confidence interval for it, identified by the intersections of horizontal dotted line with log-likelihood.
When 0 is included in the confidence interval, often $\lambda$=0 (the logarithm) is an advisable choice.

We then re-analyze the model using the log-transform of the dependent variable:
```{r}
lfmd <- lm(log(Hardness) ~ norm_density + I(norm_density^2), janka)
```
or, equivalently,
```{r}
lfmd <- update(fmd, log(.) ~ .) 
```

The coefficients significances:
```{r}
round(summary(lfmd)$coef, 4)
```
The coefficients are all significant.

And finally (studentized) residual check:
```{r,tidy=FALSE,fig.cap="Studentized residual plot"}
res_fitted_df <- data.frame(res=studres(lfmd), fit=fitted(lfmd))

ggp1 <- ggplot(data= res_fitted_df, mapping = aes(x=fit, y=res)) +
  geom_point(color="darkblue", size=2) + 
  stat_smooth(method="loess", colour="green", se=FALSE, span = 0.8) +
  geom_hline(yintercept = 0, color="red", linetype=4) +
  ylab("Residuals") + xlab("Fitted Values") +
  ggtitle("Residuals vs Fit Plot")

ggp2 <- ggplot(data = res_fitted_df, mapping = aes(sample = res)) + 
  stat_qq(color="darkblue", size=2) +
  geom_abline(mapping=aes(intercept=mean(res),slope=sd(res)), color="red", linetype=2) +
  xlab("Normal scores") + ylab("Sorted studentized residuals") +
  ggtitle("Residuals Normal Probability Plot")

grid.arrange(ggp1, ggp2, ncol=2)
```
Plots are improved and this last model seems the best one.  
Now, since the response is the logarithm of y, contributions of the explanatory variables are multiplicative with respect to y.
<!--
Now residual check in "classic" style:
-->
```{r,fig.show='hold', eval=FALSE, echo=FALSE, purl=FALSE, fig.cap="Studentized residuals Vs. Fit and Normal Probability Plot of studentized residuals"}
op <- par(mfrow = c(1,2))
plot(fitted(lfmd) , studres(lfmd) , main =  "Residuals vs Fit", pch = 16, col = "darkgray",
     xlab = "Fitted Values", ylab = "Studentized Residuals")
abline(h = 0, col = "red", lty = 4)
loe <- predict (loess(studres(lfmd)~fitted(lfmd)))
lines(fitted(lfmd), loe, col = "darkblue", lty = 3, lwd = 2)
qqnorm(studres(lfmd))
qqline(studres(lfmd))
par(op)
```

And standard residual check:
```{r,fig.show='hold',fig.cap="'Standard' residual plot on final model"}
op <- par(mfrow = c(2,2))
plot(lfmd)
par(op)
```

Now the plot of models predictions (in log-scale and in original scale) with 95% Prediction Intervals.  
```{r,fig.show='hold',tidy=FALSE,fig.cap="Final model predictions with 95% intervals in log-scale and original scale", warning=FALSE}
pred <- data.frame(predict(lfmd, interval = "prediction"))
plot_df <- data.frame(d = janka$norm_density, h = log(janka$Hardness), pred)

ggp1 <- ggplot(data = plot_df, mapping = aes(x=d, y=h)) +
  geom_point(col = "darkblue", size = 2) +
  geom_line(mapping=aes(x=d, y=fit), col = "mediumvioletred") +
  geom_line(mapping=aes(x=d, y=lwr), col = "mediumorchid1", linetype=4) +
  geom_line(mapping=aes(x=d, y=upr), col = "mediumorchid1", linetype=4) +
  xlab("Normalized Density") + ylab("log (Hardness)")

ggp2 <- ggplot(data = plot_df, mapping = aes(x=d, y=exp(h))) +
  geom_point(col = "darkblue", size = 2) +
  geom_line(mapping=aes(x=d, y=exp(fit)), col = "mediumvioletred") +
  geom_line(mapping=aes(x=d, y=exp(lwr)), col = "mediumorchid1", linetype=4) +
  geom_line(mapping=aes(x=d, y=exp(upr)), col = "mediumorchid1", linetype=4) +
  xlab("Normalized Density") + ylab("Hardness")

grid.arrange(ggp1, ggp2, ncol=2)  
```


## Example: Oxidant Vs. meteo vars regression
```{r, echo=FALSE}
rm(list=ls())
```

### Data description

Following data returns levels of an air pollutant, "Oxidant",
together with levels of four meteorological variables
recorded on 30 days during one summer.
Which, if any, of the four indipendent variables seem to be related to
levels of oxidant?

### Data loading
```{r}
data(oxidant)
str(oxidant)
head(oxidant)
```
### Descriptives
Let us start producing a matrix of scatterplots:
```{r,fig.cap="Matrix of scatterplots between metereological variables and also Oxidant"}
ggpairs(data = oxidant)
```
The above line of code draws a matrix of scatterplots between all possible pairs of variables.  
Some relations between `oxidant` and the independent variables appear.  
We will use the models to asses which independent variables actually influence the dependent variable.

### Inference and models
We begin estimating a linear model with all the independent variables, excluding `day`. 
```{r}
fm1 <- lm(oxidant ~ windspeed+temperature+humidity+insolation, data = oxidant)
summary(fm1)
```
The model seems to fit data well enough (R-square equal to 0.798). Now let us try to "expand" the initial model adding all two-way interactions.
```{r}
fm2 <- lm(oxidant ~ (windspeed+temperature+humidity+insolation)^2, data = oxidant)
summary(fm2) 
```
The Wald statistics of two-way interactions are all non significant. Now we will check if all interactions together are not significant, comparing the two above models:
```{r}
anova(fm2, fm1, test = "F")
```
There is a non-significant difference between the two models (adding all interaction effects seems to not produce better fit).
`fm1` seems the better model.
```{r}
summary(fm1)
```

Let us verify if the `humidity:insolation` interaction (the one with smaller p-value) added alone is always non-significant.
```{r}
fm3 <- update(fm1,.~.+humidity:insolation)
summary(fm3)
```
The `humidity:insolation` interaction is significant!
`fm3` could be the best model.

```{r}
anova(fm1,fm3)
```

Now the usual residuals checks:
```{r,fig.show='hold',fig.cap="Residual plot of first model rescribing Oxidant"}
op <- par(mfrow = c(2, 2))
plot(fm3)
par(op)
```

In case of models with many explanatory variables a useful tool to check residuals is to compare the residuals also with the explanatory variables.  
Now, plot all pairs with residuals
```{r,fig.cap="Matrix of scatterplot between all variables and also residuals"}
oxidant2 <- oxidant %>% bind_cols(data.frame(res=residuals(fm3)))
ggpairs(cbind(oxidant2))
```
Looking at the residuals versus the explanatory variables scatterplots (last row of matrix), it seems that `humidity^2` may be useful to explain `oxidant`:
```{r}
fm4 <- update(fm3,.~.+I(humidity^2))
summary(fm4)
```
Maybe, many other coefficients (humidity, insolation and the interaction between them) were significant
only because the square of humidity was not included in the model.  
Let us first try to drop the interaction:
```{r}
fm4 <- update(fm4,.~.-humidity:insolation)
summary(fm4)
```
Now the model is simpler, and `insolation` seems not significant.

Now let us try to remove non-significant effects.  
Always remove terms one-by-one :
```{r}
fm <- update(fm4, .~.-insolation)
summary(fm)
```
Now all coefficients (except for intercept) are significant.

We make further checks with overparametererization, adding an interaction:
```{r}
anova(fm, update(fm , .~.+windspeed:temperature), test = "F")
```
Adding the new term does not add any useful information to model.

`fm` seems the best model. Let us check the model with usual residuals plots:
```{r,fig.show='hold',fig.cap="Residual plot for second model"}
op <- par(mfrow = c(2, 2))
plot(fm)
par(op)
```
Residuals plots seem better than before.

And now plot all pairs to check if some other relation may emerge.
```{r,fig.show='hold',fig.cap="Matrix of scatterplot between variables and also residuals"}
oxidant3 <- oxidant %>% bind_cols(res=data.frame(residuals(fm)))
ggpairs(oxidant3)
```
The model seems good enough.

<!--- More analysis could be performed to deep relations.. --->

## Example: Calories Vs. categorical and continuous indep. vars regression
```{r,echo=FALSE}
options(contrasts = c("contr.treatment", "contr.poly"))
rm(list = ls())
```

### Data description

Data contains calories and sodium content for three types of hot dogs:
20 Beef, 17 Meat and 17 Poultry hot dogs.  
This data is a sample from much larger populations.
We want to find if `type` of hotdog and `sodium` content may influence the `calories` contents in hot dogs themselves.

### Data loading
```{r}
data(hotdogs)
str(hotdogs)
head(hotdogs)
```
### Descriptives
Now let us produce a boxplot for each type of hot dog:
```{r,fig.cap="Box and whisker plot of calories by type of hot dog"}
ggp <- ggplot(data = hotdogs, mapping = aes(x = type, y=calories, fill=type)) +
  geom_boxplot()
print(ggp)
```
Some difference appears between types of hot dogs. `Beef` and `meat` hot dogs seem similar.  
Following graphs represent the relation between `calories` and `sodium` without and with categorization per `type`. Splitting on categorization is then performed in several ways.
```{r,tidy=FALSE,fig.cap="Simple scatterplot of calories Vs. sodium"}
ggp <- ggplot(data=hotdogs, mapping = aes(x=sodium, y=calories)) +
  geom_point(size=2, color="darkblue")
print(ggp)
```
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium with types with different colors"}
ggp <- ggplot(data=hotdogs, mapping = aes(x=sodium, y=calories, color=type)) +
  geom_point(size=2)
print(ggp)
```
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium with types with different symbols"}
ggp <- ggplot(data=hotdogs, mapping = aes(x=sodium, y=calories, shape=type)) +
  geom_point(size=2)
print(ggp)

```
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium with types with different colors and symbols"}
ggp <- ggplot(data=hotdogs, mapping = aes(x=sodium, y=calories, shape= type, color=type)) +
  geom_point(size=2)
print(ggp)
```
An increasing relationship between `calories` and `sodium` seems to exist with differences with respect to `type`s.

Plot `calories` vs `sodium` by `type`:
```{r,tidy=FALSE,fig.cap="Scatterplot of calories Vs. sodium by type"}
ggp <- ggplot(data=hotdogs, mapping = aes(x=sodium,y=calories)) + 
  geom_point(color="darkblue", size=2)+ 
  geom_smooth(method = "lm", se=FALSE, colour="red") +
  geom_smooth(method = "loess", se=FALSE, colour="green") + 
  facet_wrap(facets = ~type)
print(ggp)
```

### Models and inference
Let us start with the estimation of full model, with main effects and interactions:
```{r}
fm <- lm(calories ~type*sodium, data = hotdogs) 
summary(fm)
summary.aov(fm) 
```
The model includes the intercept, a coefficient for the dummy variable for `meat` intercept, a coefficient for the dummy variable for `poultry` intercept, the
`sodium` variable coefficient, the interaction coefficients between `type` and `sodium` which tell us how much `sodium` slope coefficient varies when `type` is `meat` or `poultry` instead of `beef`.  
`sodium` and `type` coefficients are significant, the interaction between `sodium` slope and `type` is not.  
p-values of `aov` table suggest a three parallel regression lines model.

Notice that, since the design in unbalanced, we obtain different results switching the order of independent variables in model formulation.
```{r}
fm <- lm(calories ~ sodium * type, data = hotdogs) 
summary(fm)
summary.aov(fm) 
```
p-value for interaction remains always the same in this case, and is non significant. The p-values of main effects, however, are always less than $\alpha=$ 0.05.  
Now we will remove the interaction term:
```{r}
fm <- update(fm, .~.-sodium:type) 
summary.aov(fm)
summary(fm)
```
<!--- Mettere anche formula modello? # E(Yi) = mu + deltaMeat*x1 + deltaPpoultry*x2 --->
The difference in calories contents between `meat` and `beef` hot dogs is non significant.  
However, now we will try to change parameterization to make the model more readable:
```{r}
fm <- lm(calories ~ sodium + type - 1, data = hotdogs)
```
This parameterization for `fm` may generally be used to better understand the meaning of model and parameters values.  
In such as formulation, `summary.aov(fm)` usually does not make sense because it compares the model with 0 gran mean with the analyzed model.
```{r}
summary(fm)
```
The parameters of model specified in this manner are more easily interpretable: there are three parallel straight lines with slope equal to 0.2022, and intercepts equal to 75.7350, 74.0767, 25.9521 for `beef`, `meat`, and `poultry`, respectively.

Since `beef` and `meet` hot dogs seem similar, we try to compare their intercepts using Helmert-type contrasts:
```{r}
contrasts(hotdogs$type) <- matrix(c(-1,-1,1,-1,0,2),ncol=2,nrow=3,byrow=TRUE) 
fm3 <- lm(calories ~ sodium + type, data = hotdogs)
summary(fm3)
```
<!--- model.matrix(fm3) --->
`type1` coefficient represents the comparison between `beef` and `meet` intercepts.  
Since `type1` coefficient p-value is greater than $\alpha$=0.05, `beef` and `meet` hot-dogs can be considered equal.  
On the contrary, `type2` coefficient is the difference between `poultry` intercept and `meet` and `beef` pooled intercept effect. `type2` is quite significant, and then `poultry` hot dog type may be considered different from the other (pooled) types.

Let us try to recode factor type to obtain only two types of hot-dogs.
```{r}
hotdogs$type2 <- as.character(hotdogs$type)
hotdogs$type2[hotdogs$type2 == "beef"] <- "meat"
hotdogs$type2 <- factor(hotdogs$type2, levels = c("meat", "poultry"))
```

<!---
or:
```{r, purl=FALSE}
levels(hotdogs$type)
hotdogs$type3 = hotdogs$type
levels(hotdogs$type3)[1] = "meat"
levels(hotdogs$type3)
table(hotdogs$type)
table(hotdogs$type3)

levels(hotdogs$type)[1] = "meat"
```
--->

And now the new model is fitted to data:
```{r}
fm1 <- lm(calories ~ sodium + type2, data = hotdogs)
summary(fm1)
```

Next two lines of code compare residual standard deviation of model with three types of hot-dogs with residual standard deviation of model with two types of hot-dogs:
```{r}
summary(fm)$sigma
summary(fm1)$sigma
```
They are very similar (the residual standard deviation of "reduced" model is actually smaller), and this confirms that `meat` and `beef` hot-dogs are really similar, and may be considered equal in terms of relation between calories and sodium.

### Model diagnostics
The usual diagnostic plots for the model with three and two types of hot dogs, respectively, are reported below. 
```{r,fig.show='hold',fig.cap="Residual plot of model with distinct types for meat and beef"}
op <- par(mfrow = c(2, 2))
plot(fm)
par(op)
```

```{r,fig.show='hold',fig.cap="Residual plot of model with pooled types for meat and beef"}
op <- par(mfrow = c(2, 2))
plot(fm1)
par(op)
```

Both graphs report that the residuals are "very good".


## Example: Toy model with Istat data
```{r,echo=FALSE}
rm(list = ls())
```

### Data description

Data contain weight, height, gender and geographical area ("Nord", "Centro",
"Sud" and "Isole") from 1806 Italian people.  
The main goal of this analysis is on finding a simple model that fits the relation between height and weight of italian people.

### Data loading
```{r}
data(istat)
str(istat)
head(istat)
```

### Descriptives
Initially, we produce a plot that shows the relationship between `Weight` and `Height`:
```{r,fig.cap="Scatterplot of Weight Vs. Height"}
ggp <- ggplot(data=istat, mapping = aes(x=Height, y=Weight))+
  geom_point(color="darkblue")

print(ggp)
```
And then the above plot by gender with regression lines:
```{r,tidy=FALSE,fig.cap="Scatterplot of Weight Vs. Height by Gender"}
ggp <- ggplot(data=istat,mapping = aes(x=Height, y= Weight)) + 
  geom_point(colour="darkblue") +
  facet_wrap(facets = ~Gender) + 
  geom_smooth(method = "lm", colour= "red", se=FALSE)

print(ggp)
```
Now, a plot by area and gender with regression lines:
```{r,tidy=FALSE,fig.cap="Scatterplot of Weight Vs. Height by Area and Gender"}
ggp <- ggplot(data=istat, mapping = aes(x=Height, y= Weight)) + 
  geom_point(colour="darkblue") +
  facet_grid(facets = Gender~Area) + 
  geom_smooth(method = "lm", colour= "red", se=FALSE)

print(ggp)
```

### Inference and models

After having seen the above graphs, we will try a model with all two-way interactions.
```{r}
fm <- lm(Weight ~ (Height + Gender + Area)^2, data = istat)
summary.aov(fm)
```

Now we simplify the model by removing all non significant interactions and checking if their global contribution is significant:
```{r}
fm <- lm(Weight ~ Height + Gender + Area + Height:Gender, data = istat)
summary.aov(fm)

anova(fm,update(fm, .~. +Height:Area+Gender:Area))
```

The simplified model assesses that the slope between `Height` and `Weight` is modified by `Gender` only. Differences in intercepts appear, due to `Area` and `Gender`.

Now we plot the diagnostic graphs.
```{r,fig.show='hold',fig.cap="Residual plot of first model"}
op <- par(mfrow = c(2, 2))
plot(fm)
par(op)
```
The residuals are not symmetric.

We re-plot graph by gender with smoothing as reference for modelling.
```{r,fig.cap="Scatterplot of Weight Vs. Height by Gender with lowess and fitted line"}
ggp <- ggplot(data=istat,mapping = aes(x=Height, y= Weight)) + 
  geom_point(colour="darkblue") +
  facet_wrap(facets = ~Gender) + 
  geom_smooth(method = "lm", colour= "red", se=FALSE) +
  geom_smooth(method = "loess", colour= "green", se=FALSE, span=0.5)

print(ggp)
```

Now we try to transform data by using Box-Cox to correct skewness of residuals, and an apparent non-linearity in relation.  
This is the model:
```{r}
fm <- lm(Weight ~ Area + Height * Gender, data = istat)
```

And then the Box-Cox transformation analysis:
```{r,fig.cap="Box-Cox likelihood graph for Istat toy model"}
boxcox(fm, lambda = seq(-0.5, 0.5, 1/20))
grid()
```
A transformation with $\lambda=-\frac{1}{5}$ seems a good choice.  
This is the model on transformed scale:
```{r}
fm_l <- update(fm, Weight^(-0.2)~.)
summary.aov(fm_l)
```
Let us remove `Area`, that seems not significant, to complete the analysis.
```{r}
fm_l <- update(fm_l, .~.-Area)
```

(Note: Is this choice completely correct? Is Area surely non significant?)

### Model diagnostics
```{r,fig.show='hold',fig.cap="Residual plot for Istat toy model"}
op <- par(mfrow = c(2, 2))
plot(fm_l)
par(op)
```

Residuals are not really normal, but as a "toy" model, this result can be considered acceptable enough.  
Now we calculate the predicted values on transformed scale:
```{r}
newdata <- data.frame(Height = c(150:199, 150:199), Gender = factor(rep(c("Female", "Male"), each = 50)))
pred <- predict(fm_l, newdata = newdata, se = T, interval = "prediction")
```

And the predicted values with prediction interval on transformed scale:
```{r}
newdata$fit <- pred$fit[,1] # predicted values
newdata$lwr <- pred$fit[,2] # UPL
newdata$upr <- pred$fit[,3] # LPL
ylim <- range(istat$Weight^(-0.2))*c(0.95, 1.05) # compute y range
```

Now the predictions on transformed scale are plotted:
```{r,fig.show='hold',tidy=FALSE,fig.cap="Plot of model predictions by Gender with 95% prediction intervals on transformed scale", warning=FALSE}
ggp1 <- ggplot(data=istat %>% filter(Gender=="Male"),mapping=aes(x=Height, y=Weight^(-0.2))) +
  geom_point(color="darkblue") +
  geom_line(data=newdata %>% filter(Gender=="Male"),
    mapping=aes(x=Height, y=fit), color="mediumvioletred") +
  geom_line(data=newdata %>% filter(Gender=="Male"),
    mapping=aes(x=Height, y=upr), color="mediumorchid1", linetype=2) +
  geom_line(data=newdata %>% filter(Gender=="Male"),
    mapping=aes(x=Height, y=lwr), color="mediumorchid1", linetype=2) +
  ggtitle("Male") + ylim(ylim) 

ggp2 <- ggplot(data=istat %>% filter(Gender=="Female"),mapping=aes(x=Height, y=Weight^(-0.2))) +
  geom_point(color="darkblue") +
  geom_line(data=newdata %>% filter(Gender=="Female"),
    mapping=aes(x=Height, y=fit), color="mediumvioletred") +
  geom_line(data=newdata %>% filter(Gender=="Female"),
    mapping=aes(x=Height, y=upr), color="mediumorchid1", linetype=2) +
  geom_line(data=newdata %>% filter(Gender=="Female"),
    mapping=aes(x=Height, y=lwr), color="mediumorchid1", linetype=2) +
  ggtitle("Female") +ylim(ylim) 
  
grid.arrange(ggp1, ggp2, ncol=2)
```

Now we prepare some data to plot preditions on original scale
```{r}
ylim <- range(istat$Weight * c(0.95, 1.05))
```

And then the predictions are actually plotted:
```{r,fig.show='hold',tidy=FALSE,fig.cap="Plot of model predictions by Gender with 95% prediction intervals on original scale", warning=FALSE}
ggp1 <- ggplot(data=istat %>% filter(Gender=="Male"), mapping=aes(x=Height, y=Weight)) +
  geom_point(color="darkblue") +
  geom_line(data=newdata %>% filter(Gender=="Male"), 
    mapping=aes(x=Height, y=(fit^(-5))), color="mediumvioletred") +
  geom_line(data=newdata %>% filter(Gender=="Male"), 
    mapping=aes(x=Height, y=(upr^(-5))), color="mediumorchid1", linetype=2) +
  geom_line(data=newdata %>% filter(Gender=="Male"), 
    mapping=aes(x=Height, y=(lwr^(-5))), color="mediumorchid1", linetype=2) +
  ggtitle("Male") + ylim(ylim) 

ggp2 <- ggplot(data=istat %>% filter(Gender=="Female"), mapping=aes(x=Height, y=Weight)) +
  geom_point(color="darkblue") +
  geom_line(data=newdata %>% filter(Gender=="Female"), 
    mapping=aes(x=Height, y=(fit^(-5))), color="mediumvioletred") +
  geom_line(data=newdata %>% filter(Gender=="Female"), 
    mapping=aes(x=Height, y=(upr^(-5))), color="mediumorchid1", linetype=2) +
  geom_line(data=newdata %>% filter(Gender=="Female"), 
    mapping=aes(x=Height, y=(lwr^(-5))), color="mediumorchid1", linetype=2) +
  ggtitle("Female") + ylim(ylim) 
  
grid.arrange(ggp1, ggp2, ncol=2)
```

## Example: Iowa wheat
```{r,echo=FALSE}
rm(list = ls())
```

### Data description
The Iowa dataset is a toy example that summarizes the yield of wheat (bushels per acre)
for the state of Iowa between 1930-1962. In addition to yield, year, rainfall and temperature were recorded as the main predictors of yield:  
`Year`:  Year of harvest  
`Rain0`:	Pre-season rainfall  
`Temp1`:	Mean temperature for growing month 1  
`Rain1`:	Rainfall for growing month 1  
`Temp2`:	Mean temperature for growing month 2  
`Rain2`:	Rainfall for growing month 2  
`Temp3`:	Mean temperature for growing month 3  
`Rain3`:	Rainfall for growing month 3  
`Temp4`:	Mean temperature for harvest month  
`Yield`:	Yield in bushels per acre  

We want to find a first descriptive model that approximates the relation between Yield and the other variables.

### Data loading
```{r}
data(iowheat)
str(iowheat)
head(iowheat)
```

### Descriptives
We begin the analysis producing a matrix of scatterplots:
```{r,fig.cap="Matrix of scatterplot between dataframe variables"}
ggpairs(iowheat)
```
Some relations seem to appear. Now we will try to produce some models.

### Inference and models
Let us start with a second degree model:
```{r}
fmA <- lm(Yield ~ .^2, data = iowheat)
fmA
```
Unfortunately, there are more parameters than observations:
```{r}
dim(iowheat)
```
and then we cannot estimate all the parameters.

We then try with a second model that uses first degree effects only:
```{r}
fmA <- lm(Yield ~ ., data = iowheat)
summary(fmA)
```

Now let us try to use automatic model selection procedures.

In a first attempt a lower stepwise procedure is used. Such a procedure tries to build an "optimum" model starting from a (almost) complete model and then removing the non significant terms with a step-wise fashion, one by one.  
During the process, if some removed term become significant, it will be re-inserted in model until the `upper` element of the `scope` parameter (see also the `direction = "both"` parameter).

In this case, the model must always include `Year` as an explicative variable (see the `lower` element of `scope` parameter).
```{r}
lower_step_aic_fm <- stepAIC(fmA, scope = list(lower  =  ~Year, upper = ~ .), direction = "both")
summary(lower_step_aic_fm)
```
The final model has the terms: `Year`, `Rain0`, `Rain2`, and `Temp4`.

A second attempt with a upper stepwise procedure is then tried. Such a procedure tries to build an "optimum" model starting from the `lower_step_aic_fm` model and then adding significant terms, one by one, with a step-wise fashion.  
During the process, if some added term becomes non significant, it will be removed from model (see again the `direction = "both"` parameter)..  
In this case, the maximal model has all two-way interactions (see the `scope` parameter again):
```{r}
upper_step_aic_fm <- stepAIC(lower_step_aic_fm, scope = list(upper  =  ~.^2), direction = "both")
summary(upper_step_aic_fm)
```
The final model has the terms: `Year`, `Rain0`, `Rain2`, `Temp4`, `Rain2:Temp4`. It is slightly more complex than the previous one.

### Diagnostic residual analysis
The model arising from the upper stepwise process is then the final one, and then residual analysis follows:
```{r,fig.show='hold',fig.cap="Residual plot of model"}
op <- par(mfrow = c(2,2))
plot(upper_step_aic_fm)
par(op)
```