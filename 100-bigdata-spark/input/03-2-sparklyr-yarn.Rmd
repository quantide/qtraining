---
title: "Spark on Yarn"
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}
require(knitr)
opts_chunk$set(eval = FALSE, fig.width = 5.5)
```

# Hadoop cluster

## Yarn

Now is the time to use cluster Hadoop and Spark. 

As stated before, an Hadoop cluster is seen as a single unit. A good solution is to install R on a edge node. An edge node is the gateway between the cluster and the outside network. In this way, it can access all the nodes and it can be accessed by the client. In this case on the edge we have installed R and RStudio Server. 

In order to process data we need to open a browser, address it to the RStudio Server and login.

Now sparklyr needs to use the spark binary and configuration of the cluster. This task is achived setting the `SPARK_HOME` environment variable with the Spark path on the edge file system. Then the master option is `yarn-client` in this case. 

```{r}
require(sparklyr)

### Cluster Version
Sys.setenv(SPARK_HOME = "/usr/lib/spark/")

sc <- spark_connect(master = "yarn-client", version = "1.6.0")
connection_is_open(sc)

spark_disconnect_all()

```

As we did before we can read a file from local and push it in the Spark process, which is distributed across the cluster.

```{r}

csv_file <- "file:///data/2008.csv"

spark_table <- spark_read_csv(
  sc = sc,
  name = "year2008",
  path = csv_file
)

spark_table
```

Notice that `csv_file` is now a url with the protocol `<protocol>:///path/to/file.csv` which explicit the necessity of distinguish the local file (`file://`) and, for example, hdfs (`hdfs://`) for example.

## HDFS

Actually it is not that useful reading a file from local, because of using a sigle hard drive is a bottleneck to access big data. So the tipical use case is:

1. some one prepeared a big dataset on HDFS
2. we want to read that big file and load it in the distributed Spark ram memory.

This bash command allows to be sure that the csv file `sample_07` is present in that path of HDFS.

```{bash}
hdfs dfs -ls /user/hive/warehouse/sample_07/sample_07
```

Now changing the path of the file, we use the same code to read that file. 

```{r}
csv_file <- "hdfs:///user/hive/warehouse/sample_07/sample_07"

spark_table <- spark_read_csv(
  sc = sc,
  name = "year2008",
  path = csv_file, delimiter = "\t"
)

spark_table
```

## Hive

The Apache Hive â„¢ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. (source: https://hive.apache.org/)

Poorly speaking, this piece of sofware perform the following operations:

1. read csv from HDFS
2. assign them a schema: each column is registered with its own type (i.e. String or integer, etc..)
3. import those files from a graphical interface HUE
4. apply HiveSQL queries to those tables (http://localhost:8888) where `localhost` in the Domain Name of the HUE server. Now is local because we are using an SSH Tunnel to reach that machine.
5. R can access those table with the Hive JDBC Driver or through Spark. Here Sparklyr commands are explained.

Hive can be considered a database, so as we did for databases we need to declare a pointer to that table.

```{r}
src_tbls(sc)

mtcars_d <- tbl(sc, "mtcars_d")
mtcars_d
class(mtcars_d)
```

`mtcars_d` is a `tbl_spark` object but its data are still not loaded by spark, they will be loaded each time a dplyr query will be applied to data. In case we need to apply those operation many times, it is convenient to cache the table. This way Spark load data in the distributed RAM.


```{r}
class(mtcars_d)
# do the caching
tbl_cache(sc, "mtcars_d")
# delete the cache
tbl_uncache(sc, "mtcars_d")
```
