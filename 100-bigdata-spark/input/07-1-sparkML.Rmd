# Spark ML

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Introduction

`sparklyr` allows you to access the machine learning routines provided by the `spark.ml` package by providing three families of functions that you can use with Spark machine learning:

* Machine learning algorithms for analyzing data (`ml_*`)
* Feature transformers for manipulating individual features (`ft_*`)
* Functions for manipulating Spark DataFrames (`sdf_*`)

In this chapter you will learn how to use machine learning algorithm from the `ml_*` family to model your data and how to inspect your model fit, and use it to make predictions with new data.

## Algorithms

Here is an overview of the main functions to access Spark's machine learning library:

* K-Means Clustering: `ml_kmeans`	
*	Linear Regression: `ml_linear_regression`
* Logistic Regression: `ml_logistic_regression`
* Survival Regression: `ml_survival_regression`
*	Generalized Linear Regression: `ml_generalized_linear_regression`
*	Decision Trees: `ml_decision_tree`
* Random Forests: `ml_random_forest`
* Gradient-Boosted Trees: `ml_gradient_boosted_trees`
*	Principal Components Analysis: `ml_pca`
*	Naive-Bayes: `ml_naive_bayes`
* Multilayer Perceptron: `ml_multilayer_perceptron`
*	Latent Dirichlet Allocation: `ml_lda`

The `ml_*` functions take the arguments `response` and `features`. But features can also be a formula with main effects (note that it currently does not accept interaction terms). Later on an example will be showed. 

Moreover there is a set of `ml_*` functions for interacting with Spark ML model fits. Here is a brief list with a short description:

* binary models: `ml_binary_classification_eval` calculates the area under the curve for a binary classification model; `ml_classification_eval` calculates performance metrics (i.e. f1, precision, recall, weightedPrecision, weightedRecall, and accuracy) for binary and multiclass classification model.
* decision trees: `ml_tree_feature_importance` calculates variable importance for decision trees (i.e. decision trees, random forests, gradient boosted trees)


## Examples

In this section, we see a list of examples based on the usual `flights` dataset. Before we start anything, we need to load the needed libraries: 

```{r packages, results="hide"}
library(tidyverse)
library(sparklyr)
```


and initialize Spark:

```{r, eval = FALSE}
sc <- spark_connect("yarn", version = "2.0.0")
spark_disconnect(sc)
sc <- spark_connect("local", version = "1.6.2")
csv_file <- '~/data/*.csv'
ontime_tbl <- spark_read_csv(sc = sc, 
                             'ontime_tbl' , 
                             path = csv_file,
                             header = TRUE, delimiter = ',')

```


### Linear Regression in R

```{r}
model <- lm(Petal.Length ~ Petal.Width, data = iris)

iris %>%
	select(Petal.Width, Petal.Length) %>%
	ggplot(aes(Petal.Length, Petal.Width)) +
	geom_point(data = iris, aes(Petal.Width, Petal.Length), size = 2, alpha = 0.5) +
	geom_abline(aes(slope = coef(model)[["Petal.Width"]],
									intercept = coef(model)[["(Intercept)"]],
									color = "red"))
```

### Linear Regression in Spark

```{r}
model <- iris_tbl %>%
	select(Petal_Width, Petal_Length) %>%
	ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

iris_tbl %>%
	select(Petal_Width, Petal_Length) %>%
	collect %>%
	ggplot(aes(Petal_Length, Petal_Width)) +
	geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +
	geom_abline(aes(slope = coef(model)[["Petal_Width"]],
									intercept = coef(model)[["(Intercept)"]],
									color = "red"))
```

### Logistic Regression in R

```{r}
# Prepare beaver dataset
beaver <- beaver2
beaver$activ <- factor(beaver$activ, labels = c("Non-Active", "Active"))

# Fit model
model <- glm(activ ~ temp, data = beaver, family = binomial(link = "logit"))
print(model)

# Plot prediction curve
newdata <- data.frame(
	temp = seq(min(beaver$temp), max(beaver$temp), length.out = 128)
)

df <- data.frame(
	x = newdata$temp,
	y = predict(model, newdata = newdata, type = "response") + 1
)

ggplot(beaver, aes(x = temp, y = activ)) +
	geom_point() +
	geom_line(data = df, aes(x, y), col = "red") +
	labs(
		x = "Body Temperature (ÂºC)",
		y = "Activity",
		title = "Beaver Activity vs. Body Temperature",
		subtitle = "From R's built-in 'beaver2' dataset"
	)

```

### Logistic Regression in Spark

```{r}
beaver_tbl <- copy_to(sc, beaver, "beaver", overwrite = TRUE)

model <- beaver_tbl %>%
	mutate(response = as.numeric(activ == "Active")) %>%
	ml_logistic_regression(response = "response", features = "temp")

print(model)
```


### Random Forests with Spark

```{r}
mForest <- iris_tbl %>%
	ml_random_forest(
		response = "Species",
		features = c("Petal_Length", "Petal_Width"),
		max.bins = 32L,
		max.depth = 5L,
		num.trees = 20L
	)
mPredict <- predict(mForest, iris_tbl)
head(mPredict)
```


### Decision Tree with Spark

```{r}
mDecisionTree <- iris_tbl %>%
	ml_decision_tree(
		response = "Species",
		features = c("Petal_Length", "Petal_Width"),
		max.bins = 32L,
		max.depth = 5L
	)
mPredict <- predict(mDecisionTree, iris_tbl)
head(mPredict)
```


```{r}
spark_disconnect(sc)
```
