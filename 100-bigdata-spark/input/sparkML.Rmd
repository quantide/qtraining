# RStudio ml examples

## Initialization

```{r setup, results="hide"}
library(sparklyr)
library(dplyr)
library(ggplot2)


sc <- spark_connect("yarn", version = "2.0.0")
spark_disconnect(sc)
sc <- spark_connect("local", version = "2.0.0")
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
```

## KMeans in R

```{r}
cl <- iris %>%
	select(Petal.Width, Petal.Length) %>%
	kmeans(centers = 3)

centers <- as.data.frame(cl$centers)

iris %>%
	select(Petal.Width, Petal.Length) %>%
	ggplot(aes(Petal.Length, Petal.Width)) +
	geom_point(data = centers, aes(Petal.Width, Petal.Length), size = 60, alpha = 0.1) +
	geom_point(data = iris, aes(Petal.Width, Petal.Length), size = 2, alpha = 0.5)
```

## KMeans in Spark

```{r}
model <- iris_tbl %>%
	select(Petal_Width, Petal_Length) %>%
	ml_kmeans(centers = 3)

iris_tbl %>%
	select(Petal_Width, Petal_Length) %>%
	collect %>%
	ggplot(aes(Petal_Length, Petal_Width)) +
	geom_point(data = model$centers, aes(Petal_Width, Petal_Length), size = 60, alpha = 0.1) +
	geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5)

```

## Linear Regression in R

```{r}
model <- lm(Petal.Length ~ Petal.Width, data = iris)

iris %>%
	select(Petal.Width, Petal.Length) %>%
	ggplot(aes(Petal.Length, Petal.Width)) +
	geom_point(data = iris, aes(Petal.Width, Petal.Length), size = 2, alpha = 0.5) +
	geom_abline(aes(slope = coef(model)[["Petal.Width"]],
									intercept = coef(model)[["(Intercept)"]],
									color = "red"))
```

## Linear Regression in Spark

```{r}
model <- iris_tbl %>%
	select(Petal_Width, Petal_Length) %>%
	ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

iris_tbl %>%
	select(Petal_Width, Petal_Length) %>%
	collect %>%
	ggplot(aes(Petal_Length, Petal_Width)) +
	geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +
	geom_abline(aes(slope = coef(model)[["Petal_Width"]],
									intercept = coef(model)[["(Intercept)"]],
									color = "red"))
```

## Logistic Regression in R

```{r}
# Prepare beaver dataset
beaver <- beaver2
beaver$activ <- factor(beaver$activ, labels = c("Non-Active", "Active"))

# Fit model
model <- glm(activ ~ temp, data = beaver, family = binomial(link = "logit"))
print(model)

# Plot prediction curve
newdata <- data.frame(
	temp = seq(min(beaver$temp), max(beaver$temp), length.out = 128)
)

df <- data.frame(
	x = newdata$temp,
	y = predict(model, newdata = newdata, type = "response") + 1
)

ggplot(beaver, aes(x = temp, y = activ)) +
	geom_point() +
	geom_line(data = df, aes(x, y), col = "red") +
	labs(
		x = "Body Temperature (ºC)",
		y = "Activity",
		title = "Beaver Activity vs. Body Temperature",
		subtitle = "From R's built-in 'beaver2' dataset"
	)

```

## Logistic Regression in Spark

```{r}
beaver_tbl <- copy_to(sc, beaver, "beaver", overwrite = TRUE)

model <- beaver_tbl %>%
	mutate(response = as.numeric(activ == "Active")) %>%
	ml_logistic_regression(response = "response", features = "temp")

print(model)
```

## Survival Regression in R

```{r}
library(survival)
data(ovarian, package = "survival")

fit <- survreg(
	Surv(futime, fustat) ~ ecog.ps + rx,
	data = ovarian,
	dist = "weibull"
)

coefficients(fit)
```

## Survival Regression in Spark

```{r}
ovarian_tbl <- copy_to(sc, ovarian, overwrite = TRUE)
fit <- ovarian_tbl %>%
	ml_survival_regression(
		response = "futime",
		censor = "fustat",
		features = c("ecog_ps", "rx")
	)

coefficients(fit)
```

## Partitioning in R 

```{r}
set.seed(1099)
ratio <- 0.75
trainingSize <- floor(ratio * nrow(iris))
indices <- sample(seq_len(nrow(iris)), size = trainingSize)

training <- iris[ indices, ]
test     <- iris[-indices, ]

fit <- lm(Petal.Length ~ Petal.Width, data = iris)
predict(fit, newdata = test)
```

## Partitioning in Spark

```{r}
partitions <- tbl(sc, "iris") %>%
	sdf_partition(training = 0.75, test = 0.25, seed = 1099)

fit <- partitions$training %>%
	ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

predict(fit, partitions$test)
```


## Principal Components Analysis in R

```{r}
model <- iris %>%
	select(-Species) %>%
	prcomp()
print(model)

# calculate explained variance
model$sdev^2 / sum(model$sdev^2)
```

## Principal Components Analysis in Spark

```{r}
model <- tbl(sc, "iris") %>%
	select(-Species) %>%
	ml_pca()
print(model)
```

## Random Forests with R

```{r}
rForest <- randomForest::randomForest(
	Species ~ Petal.Length + Petal.Width,
	ntree = 20L,
	nodesize = 20L,
	data = iris
)
rPredict <- predict(rForest, iris)
head(rPredict)
```

## Random Forests with Spark

```{r}
mForest <- iris_tbl %>%
	ml_random_forest(
		response = "Species",
		features = c("Petal_Length", "Petal_Width"),
		max.bins = 32L,
		max.depth = 5L,
		num.trees = 20L
	)
mPredict <- predict(mForest, iris_tbl)
head(mPredict)
```

## Comparing Random Forest Classification

Using the model to predict the same data it was trained on is
certainly not best practice, but it at least showcases that
the results produced are concordant between R and Spark.

```{r}
df <- as.data.frame(table(x = rPredict, y = mPredict), stringsAsFactors = FALSE)

ggplot(df) +
	geom_raster(aes(x, y, fill = Freq)) +
	geom_text(aes(x, y, label = Freq), col = "#222222", size = 6, nudge_x = 0.005, nudge_y = -0.005) +
	geom_text(aes(x, y, label = Freq), col = "white", size = 6) +
	labs(
		x = "R-predicted Species",
		y = "Spark-predicted Species",
		title = "Random Forest Classification — Comparing R and Spark")
```

## Neural Networks with R

```{r}
library(neuralnet)

XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)

xor.data

net.xor <- neuralnet( XOR~Var1+Var2, xor.data, hidden = 2, rep = 5)
plot(net.xor, rep="best")
```

## Decision Tree with Spark

```{r}
mDecisionTree <- iris_tbl %>%
	ml_decision_tree(
		response = "Species",
		features = c("Petal_Length", "Petal_Width"),
		max.bins = 32L,
		max.depth = 5L
	)
mPredict <- predict(mDecisionTree, iris_tbl)
head(mPredict)
```

## Naive-Bayes with Spark

```{r}
mNaiveBayes <- iris_tbl %>%
	ml_naive_bayes(
		response = "Species",
		features = c("Petal_Length", "Petal_Width")
	)
mPredict <- predict(mNaiveBayes, iris_tbl)
head(mPredict)
```

## Cleanup

```{r}
spark_disconnect(sc)
```
# other
## tempo

Posix

## env

```{r env}
Sys.getenv("SPARK_HOME")
Sys.getenv("HADOOP_CONF_DIR")
Sys.getenv("JAVA_HOME")
```
