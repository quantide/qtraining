---
title: "Spark and dplyr"
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}
require(knitr)
opts_chunk$set(eval = FALSE, fig.width = 5.5)
```

## dplyr 5 verbs

Let us consider the `spark_table` as built in the previous sections. We now can use sparklyr and dplyr functions to deal with this dataset. 

```{r}
require(dplyr)

colnames(spark_table)

## or

tbl_vars(spark_table)

 ## we are interested only in some columns, for example
spark_table_detail <-
    spark_table %>%
    select(Year, Month, DayofMonth, DepTime, ArrTime, DepDelay, ArrDelay, TailNum, FlightNum )
spark_table_detail

spark_table_detail %>%
    filter( ! TailNum == "" ) %>%
    arrange( FlightNum, TailNum  )

spark_table_detail %>%
    group_by( Year, FlightNum ) %>%
    summarize( avg = mean( ArrDelay ) ) %>%
    arrange( desc(avg) )

```

## Cast

As you can see variable like `ArrDelay` are read as *strings*. In the previous case the cast (change of type of the variable) was automatic. However we want to cast them into integers previously, for this we pass through dplyr:

```{R}
spark_table_detail %>%
    group_by( Year, FlightNum ) %>%
    mutate( ArrDelay = as.integer( ArrDelay ) ) %>%   # cast 
    summarize( avg = mean( ArrDelay ) ) %>%
    arrange( desc(avg) )

```

So, in this way, we can cast more than one var at a time

```{R}
spark_table_1 <-
    spark_table_detail %>%
    mutate(
        DepDelay = as.integer(DepDelay) ,
        ArrDelay = as.integer(ArrDelay) ) %>% # cast
    select(                             # only a column sorting
        ArrDelay,
        DepDelay,
        everything() )
spark_table_1
```
