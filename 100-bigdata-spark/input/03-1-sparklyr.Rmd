---
title: "Spark with sparklyr"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# The dataset
	
## Introduction

The dataset we will use in this part of the course is again the dataset regarding on-time performance of domestic flights operated by large air carriers. You find data and a short explanation of tables on this page.

	http://stat-computing.org/dataexpo/2009/the-data.html
	

## First data manipulation

Data is in bzipped csv format. We can download them from the terminal following these instructions:

```{bash}
cd ~
mkdir data			# create data dir
cd data	  			# enter this dir
wget http://stat-computing.org/dataexpo/2009/2006.csv.bz2
wget http://stat-computing.org/dataexpo/2009/2007.csv.bz2
wget http://stat-computing.org/dataexpo/2009/2008.csv.bz2
bunzip *.csv.bz2
ls
```

We thus have 3 csv files in this folder. We can print out the first 5 line of one of this file with the `head` command

```{bash}
head -n 5 2008.csv
```

The first line of each file contains the column names, which are equals in the three files. Now we want concatenate these files in a bigger csv. We will follow this steps:

1. Create a new file `ontime.csv` with the column names
2. Add the content of each file csv starting from the second line (without header)


```{bash}
	
head -n 1 2008.csv  > ontime.csv   # step 1.
tail -n +2 2008.csv >> ontime.csv  # step 2. 
tail -n +2 2007.csv >> ontime.csv  # step 2. 
tail -n +2 2006.csv >> ontime.csv  # step 2. 

```

## Faliure of R with read.table()

Now the file `~/data/ontime.csv` contains about 2 Gb of data. If you are using a machine with 2 Gb or less of RAM, the reading of this file will fail.

```{R}
ontime <- read.table(file = "~/data/ontime.csv", header = T, sep = ",", quote = "\"")
```

If we want to succeeded in reading a csv file on a 2 Gb machine we need to take a sample of these files.

```{bash}
head -n 30000 2008.csv > mini.csv
```

Now from RStudio-server

```{R}
csv_file <- "~/data/mini.csv"
R_df <- read.table(file = csv_file, header = T, sep = ",", quote = "\"")
R_df
```

# Sparklyr

## Install Spark

First of all, you need to have `sparklyr` available to R. So install it and load it in this way:

```{R sparklyr installation}
install.packages("devtools")
devtools::install_github("rstudio/sparklyr")
```

```{R require-sparklyr}
require(sparklyr)
```


If you want to run Spark locally or you cannot connect to a cluster, you should install a version directly from R. This will install the requested Spark version in the `~/.cache/` folder.

```{R}
spark_install(version = "2.0.0")
```

## Initialize spark environment

Now that spark is correctly install we need to initialize a spark context

```{R}
sc <- spark_connect( master = "local", version = "2.0.0" )
```

where:

1. "local" means that spark will work on our local machine, this is in contrast with the concept of cluster.
2. `version` refers to the version of Spark installed with `sparklyr`, we chose the version previously installed.

<!-- new -->

After the initializiation we can work with the Spark connector. For now we only ask for the Spark version.

```{r}
spark_version(sc)
```

When we will be done with Spark we should close the connection.

```{r}
spark_disconnect(sc)
```

# Data import

## Csv reading

The following code is to read a csv file. We are reading it from our local hard-drive but also distributed filesystem protocols like `hdfs://` and `s3n://` are allowed.

```{r}

csv_file <- "/data/2008.csv"

spark_table <- spark_read_csv(
  sc = sc,
  name = "year2008",
  path = csv_file
)

spark_table
```

## Sparklyr compared to R 	

Where does Spark table differ to dplyr `tbl_df` or `data.frame`? Spark, like a database do, move the data out of R, so the management of memory is left to the Spark process. The object spark_table in R is only a handle to an object of Spark.

```{r}
R_df <- read.table(file = csv_file, header = T, sep = ",", quote = "\"")
R_df <- tbl_df( R_df )
R_df

format( object.size(R_df), units = "auto" )
format( object.size(spark_table), units = "auto" )

```

<!-- todo: There are differences also in getting the structure of the date: -->

```{r}
require(dplyr)

# Examine structure of tibble
str(R_df)

# Examine structure of data
glimpse(spark_table)

```

## Copy R data.frame to Spark

If uploading a data.frame on Spark is needed the `dplyr::copy_to` function do this job. In this course this function can be used to upload a small dataframe to perform tests on Spark.  

```{r}
class(mtcars)

mtcars_spark <- copy_to(sc, df = mtcars, name = "mtcars")

class(mtcars_spark)
```

As result, a table named "mtcars" has appered in the Spark tab, which content can be recalled with `src_tbls` function:

```{r}
src_tbls(sc)
```

## Copy Spark data.frame to R 

This operation should be thought with attention. Importing in R a big data.frame can be a long process that could also exceed the local RAM: this will end up with a crash of the R process.

This is usually be done after the dataset is reduced to a smaller one in order to fit the local RAM: this can be obtained by filtering, aggregating, sampling, etc...

In this case a smaller random sample is taken from the original dataset:

```{r}
spark_sample <- sample_n(spark_table, size = 500)
class(spark_sample)

R_sample <- collect(spark_sample)
class(R_sample)
```

As you can see the spark object is a `tbl_spark`, while the collected one is an R `data.frame`. This is because the first is in the Spark process and the second is in the R one. 
This is also proved by the different dimension of those objects:

```{r}
format( object.size(spark_sample), units = "auto" )
format( object.size(R_sample), units = "auto" )
```


# dplyr

## 5 verbs

We now can use sparklyr and dplyr functions to deal with this dataset. 

```{r}
require(dplyr)

colnames(spark_table)

## or

tbl_vars(spark_table)

 ## we are interested only in some columns, for example
spark_table_detail <-
    spark_table %>%
    select(Year, Month, DayofMonth, DepTime, ArrTime, DepDelay, ArrDelay, TailNum, FlightNum )
spark_table_detail

spark_table_detail %>%
    filter( ! TailNum == "" ) %>%
    arrange( FlightNum, TailNum  )

spark_table_detail %>%
    group_by( Year, FlightNum ) %>%
    summarize( avg = mean( ArrDelay ) ) %>%
    arrange( desc(avg) )

```

## Cast

As you can see variable like `ArrDelay` are read as *strings*. In the previous case the cast (change of type of the variable) was automatic. However we want to cast them into integers previously, for this we pass through dplyr:

```{R}
spark_table_detail %>%
    group_by( Year, FlightNum ) %>%
    mutate( ArrDelay = as.integer( ArrDelay ) ) %>%   # cast 
    summarize( avg = mean( ArrDelay ) ) %>%
    arrange( desc(avg) )

```

So, in this way, we can cast more than one var at a time

```{R}
spark_table_1 <-
    spark_table_detail %>%
    mutate(
        DepDelay = as.integer(DepDelay) ,
        ArrDelay = as.integer(ArrDelay) ) %>% # cast
    select(                             # only a column sorting
        ArrDelay,
        DepDelay,
        everything() )
spark_table_1
```

## Plot with ggplot2

```{R}
 ## Collect some data
delay <-
    spark_table %>% 
    group_by(TailNum) %>%
    summarise(count = n(), dist = mean(Distance), delay = mean(ArrDelay)) %>%
    filter(count > 20, dist < 2000, !is.na(delay)) %>%
    collect()

 ## Plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) 
```

# SQL

```{R}
library(DBI)

new_spark_table_detail <- dbGetQuery(sc, "SELECT Year, Month, DayofMonth, DepTime, ArrTime, DepDelay, ArrDelay, TailNum, FlightNum from year2008 LIMIT 5")
new_spark_table_detail
sql_render(spark_table_detail)

class(spark_table_detail)
class(new_spark_table_detail)

```

# Lazy Vs Eager

Dplyr is a tool that provide lazy and eager object and function.

- Lazy: nothing happend. Instructions of the computation are stored in the output object and the computation will take place when it results will be required.
- Eager: when the computation is immediately run, taking care of run lazy object first.

```{r}
get_info <- function(tbl) {
	print(sql_render(tbl))
	# attr(spark_table$ops$src$con$backend, "conn_id")
	# attr(tmp$ops$x$src$con$backend, "conn_id")
}
```


```{r}

get_info(spark_table)

spark_table_detail_2 <-
    spark_table %>%
    # select(Year, Month, DayofMonth, DepTime, ArrTime, DepDelay, ArrDelay, TailNum, FlightNum )
    select(Year, DepTime, ArrTime, DepDelay, ArrDelay, TailNum, FlightNum, Distance )
get_info(spark_table_detail_2)

step1 <-
	spark_table_detail_2 %>%
	filter( ! TailNum == "" ) %>%
	arrange( FlightNum, TailNum  )

get_info(step1)

step2 <- 
	step1 %>%
	group_by(TailNum) %>%
	summarise(count = n(), dist = mean(Distance), delay = mean(ArrDelay)) %>%
	filter(count > 20, dist < 2000, !is.na(delay)) 

get_info(step2)
```

Till here nothing has happend, but different objects brings all the instructions. All dplyr verbs used till here are lazy operators.

Now in order to apply the lazy code, an eager operator is used: `collect`. Consistently with what explained before, it apply the lazy instructions and, finally, collect the data into an R data.frame.

```{r}
collect(step2)
```

This takes some time to run.

Now we add a further step starting from `step1` partial result. Then we collect the `step2` and `step3` dataset. 

```{r}
step3 <- 
	step1 %>%
	group_by(TailNum, FlightNum, Year) %>%
	summarise(count = n(), dist = mean(Distance), delay = mean(ArrDelay)) %>%
	filter(count > 10, dist < 8000, !is.na(delay))

collect(step2)
collect(step3)
```

Given the dplyr lazy behaviour it is easy to understand that `step1` is performed twice. This could take a lot of precious time. How to fix this?

Unfortunately `step1` is too big to fit in R memory, thus we cannot collect it. It is necessary break the lazyness of dplyr and preserve the partial result in the Spark process. For that we will use the eager operator `compute`, which apply the lazy instructions and create a Spark data.frame (`tbl_spark` object) with the output:

```{r}
computed_step1 <- compute(step1, "step1_sdf")

get_info(step1)
get_info(computed_step1)

```

As you can see the new object `computed_step1` has no lazy operations and uses "step1_sdf" as Spark data.frame. Now redefining `step2` and `step3`, two new optimized ojbect have been obtained.

```{r}
optimized_step2 <- 
	computed_step1 %>%
	group_by(TailNum) %>%
	summarise(count = n(), dist = mean(Distance), delay = mean(ArrDelay)) %>%
	filter(count > 20, dist < 2000, !is.na(delay)) 

get_info(optimized_step2)

optimized_step3 <- 
	computed_step1 %>%
	group_by(TailNum, FlightNum, Year) %>%
	summarise(count = n(), dist = mean(Distance), delay = mean(ArrDelay)) %>%
	filter(count > 10, dist < 8000, !is.na(delay))

get_info(optimized_step3)

collect(step2)
collect(step3)
```

# Benchmark

```{r}
spark_partial <- 	spark_table_detail %>%
		group_by( Year, FlightNum ) %>%
		summarize( avg = mean( ArrDelay ) ) %>%
		arrange( desc(avg) ) %>%
		compute(name = "spark_partial")

tmp <- collect(spark_partial)
tmp
library(microbenchmark)
microbenchmark({
	tmp_1 <- 
		spark_table_detail %>%
		group_by( Year, FlightNum ) %>%
		summarize( avg = mean( ArrDelay ) ) %>%
		arrange( desc(avg) ) %>%
		compute()
},{
	tmp_2 <- 
		spark_table_detail %>%
		group_by( Year, FlightNum ) %>%
		summarize( avg = mean( ArrDelay ) ) %>%
		arrange( desc(avg) ) %>%
		collect()
},{
	tmp_3 <- collect(spark_partial)
},{
	tmp_4 <- 
		R_df %>%
		group_by( Year, FlightNum ) %>%
		summarize( avg = mean( ArrDelay ) ) %>%
		arrange( desc(avg) ) %>%
		collect()
}, times = 10)

 ## todo
tmp_1
tmp_2
identical(tmp_2, tmp_3)

tmp_3 %>% ungroup() %>% summarise_all(funs(n(), mean, sd))
tmp_4 %>% ungroup() %>% summarise_all(funs(n(), mean, sd))
class(tmp_3)

```

# Spark_write_csv

As usual we have some methods to export manipulated data. For convenience we export only the first 10 rows of the spark table. So first of all we set this mini dataset called `mini_spark_table` and we write it onto disk.

Spark is made to deal with big data, thus is convenient (and default behavior of Spark) to split up this datasets in more files. So the destination of the writing operation will be a directory where put those files.

```{r}
mini_spark_table <- 
  head(spark_table, n=10)

out_csv_dir <- "/home/cloudera/data/mini_spark_table"

if (dir.exists(out_csv_dir))
  unlink(out_csv_dir, recursive = TRUE)

spark_write_csv(x = mini_spark_table, path = out_csv_dir)

if (dir.exists(out_csv_dir))
  unlink(out_csv_dir, recursive = TRUE)

spark_write_csv(x = spark_table, path = out_csv_dir )
```

Now we reload this family of csv, in a unique data set. Spark do the job for us simply taking as input the directory path in place of a single filename.

```{r}
spark_read_tbl <- spark_read_csv(sc = sc, 'read', path = out_csv_dir, header = T )

class(spark_read_tbl)
```

```{r}
src_tbls(sc) 

dim( spark_table )
dim( collect(spark_table) )
```




