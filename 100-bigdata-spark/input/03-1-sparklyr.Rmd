---
title: "Spark with sparklyr"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# Sparklyr

## Install Spark

First of all, you need to have `sparklyr` available to R. So install it and load it in this way:

```{R sparklyr installation}
install.packages("devtools")
devtools::install_github("rstudio/sparklyr")
```

```{R require-sparklyr}
require(sparklyr)
```


If you want to run Spark locally or you cannot connect to a cluster, you should install a version directly from R. This will install the requested Spark version in the `~/.cache/` folder.

```{R}
spark_install(version = "2.0.0")
```

## Initialize spark environment

Now that spark is correctly install we need to initialize a spark context

```{R}
sc <- spark_connect( master = "local", version = "2.0.0" )
```

where:

1. "local" means that spark will work on our local machine, this is in contrast with the concept of cluster.
2. `version` refers to the version of Spark installed with `sparklyr`, we chose the version previously installed.

<!-- new -->

After the initializiation we can work with the Spark connector. For now we only ask for the Spark version.

```{r}
spark_version(sc)
```

When we will be done with Spark we should close the connection.

```{r}
spark_disconnect(sc)
```






