
```{r, options, echo=FALSE, results='hide', message=FALSE, warning=FALSE, purl=FALSE}
require(knitr)
#opts_knit$set(root.dir = "../../../../data/manual")
```


# Statistical Models with R


## Regression Analysis

Regression is the study of the change of the distribution of one variable, $y$, according to the value of another variable, $x$. Both continuous and discrete variables can be included in regression problems.

Formally, a relationship between $x$ and $y$ usually means that the expected value of $y$ is different for different values of $x$.

Regression model can be expressed in matrix form as:

$\vec{\mu} = E(\vec{y}) = \matrix{X} \vec{\beta} + \vec{\varepsilon}$

where:

 - $\matrix{X}$ is a $n \text{ x } (p+1)$ matrix, with all ones in the first columns, 
 - $\vec{\beta}$ is the vector of $\beta_0, \beta_1, \dots, \beta_p$,
 - $\vec{\varepsilon}$ is a **random** or **noise** component.



Models where response variable is a linear function of parameters are linear models.

Parameters values are usually estimated via maximum likelihood.

The main assumption of linear model is that the model is linear in the parameters. Other assumptions concern the error terms $\vec{\varepsilon}$ are:

 - $E(\varepsilon_{i}) = 0$ \hspace{1.7cm} ($i = 1, \dots, n$)
 - $\varepsilon_i \sim\; N(0,\,\sigma^2_\varepsilon)$ \hspace{1cm} ($i = 1, \dots, n$)
 - $Corr(\varepsilon_{i},\,\varepsilon_{i-j}) = 0$ \hspace{0.4cm} ($i = 1, \dots, n; 0 < j < i$)


The `lm` and `aov` functions are used in R to fit respectively linear regression and analysis of variance models.

The common interface to fit a statistical model in R is made of a call to the corresponding function with arguments `formula` and `data`.

Model formula general expression is $y \sim x_1 \pm x_2 \pm, \dots, \pm , x_k$.

Extensions to the above formula can be summarized as:

 \begin{center}
    %\begin{footnotesize}
      \begin{tabular}{ll}
        \hline
        Expression & Description \\
        \hline
        \texttt{y }$\sim$\texttt{ x}                         & Simple regression \\
        \texttt{y }$\sim$\texttt{ 1 + x}                     & Explicit intercept \\
        \texttt{y }$\sim$\texttt{ -1 + x}                    & Through the origin \\
        \texttt{y }$\sim$\texttt{ x + x\^{ }2}               & Quadratic regression \\
        \texttt{y }$\sim$\texttt{ x1 + x2 + x3}              & Multiple regression \\
        \texttt{y }$\sim$\texttt{ G + x1 + x2}               & Parallel regressions \\
        \texttt{y }$\sim$\texttt{ G / (x1 + x2)}             & Separate regressions \\
        \texttt{sqrt(y) }$\sim$\texttt{ x + x\^{ }2}         & Transformed \\
        \texttt{y }$\sim$\texttt{ G}                         & Single classification \\
        \texttt{y }$\sim$\texttt{ A + B}                     & Randomized block \\
        \texttt{y }$\sim$\texttt{ B + N * P}                 & Factorial in blocks \\
        \texttt{y }$\sim$\texttt{ x + B + N * P}             & with covariate \\
        \texttt{y }$\sim$\texttt{ . -x1}                     & All variables except \texttt{x1} \\
        \texttt{y }$\sim$\texttt{ . + A:B}                   & Add interaction (\texttt{update}) \\
        \texttt{Nitrogen }$\sim$\texttt{ Times*(River/Site)} & More complex design \\ 
        \hline
      \end{tabular}   
    %\end{footnotesize}
  \end{center}

After fitting any statistical model, common R functions for inference are:

  \begin{center}
    \begin{tabular}{ll}
      \hline
      Expression & Description \\
      \hline
      \texttt{coef(obj)} & regression coefficients \\
      \texttt{resid(obj)} & residuals \\
      \texttt{fitted(obj)} & fitted values \\
      \texttt{summary(obj)} & analysis summary \\
      \texttt{predict(obj, newdata = ndat)} & predict for newdata \\
      \texttt{deviance(obj)} & residual sum of squares\\
      \hline
    \end{tabular} 
  \end{center}



### Drug Dosage and Reaction Time

In an experiment to investigate the effect of a depressant drug, the reaction times of ten males rats to a certain stimulus were measured after a specified dose of the drug had been administer to each rat. The results were as follows:

```{r, drug}
load("../data/drug.Rda")
str(drug)
```

Basic graphical data exploration may be achieved with: 
```{r, drugplot, fig = TRUE}
plot(time ~ dose, data = drug,
     xlab = "Dose (mg)",
     ylab =  "Reaction Time (secs)")
```

A simple model for these data might be a straight line:
$E(y_i|x_i) = \beta_0 + \beta_1 x_i \text{ for } i = 1, \dots, n$


The equivalent R command is given by:

```{r, lm}
fm = lm(time ~ dose, data = drug)
```

The resulting regression line can be easy superposed to the data scatterplot by:

```{r, reg, fig = T}
plot(time ~ dose, data = drug,
     xlab = "Dose (mg)",
     ylab =  "Reaction Time (secs)")

abline(reg = fm)
```

Regression results can be investigated by generic function `summary()`.

```{r, summary.lm}
summary(fm)
```

Small p-values for t-test on coefficients lead to consider both coefficients as significant. Adjust R-squared equal to 0.78 shows an acceptable portion of total variation explained by the regression model.

Small p-value for F-statistic confirm fitted model significance when compared with null model: $E(y_i|x_i) = \beta_0$.

Same result could be obtained by: 

```{r, anova}
anova(fm, test = F)
```

Prediction for response variable at specific values of the explanatories variable can be gained by:

```{r, predict}
newdata = data.frame(dose = c(0.2, 0.4, 0.6 ))
predict (fm , newdata = newdata)
```

Note how the `newdata` argument within `predict()` requires an object of class `data.frame` containing the same variables as in the explanatory part of the model.

Predict can be used for defining prediction interval for the fitted model. Prediction interval for any new observation possibly within the regression support is given by:

```{r, prediction}
predict (fm , newdata = newdata, interval = "prediction")
```

Prediction interval can be plotted by:

```{r, predictionplot, fig = T}
newdata = data.frame(dose = seq(0, 0.9 , len = 100))
newdata = cbind(newdata, predict(fm, newdata = newdata,
  interval = "prediction"))
ylim = c(min(newdata$lwr), max(newdata$upr)) * c(0.99, 1.01)

plot(time ~ dose, data = drug, type = "n", ylim = ylim,
  xlab = "Dose (mg)", ylab = "Reaction Time (secs)")

with(newdata, {
  polygon(x = c(dose, rev(dose)), y = c(lwr , rev(upr)), col = "gray")
  lines(dose, fit, col = "red")
})

with(drug, points(dose, time, pch = 16, col = "darkblue"))
```

Finally, the initial assumptions concerning the structure of the error term: 

 - $E(\varepsilon_{i}) = 0$ \hspace{1.7cm} ($i = 1, \dots, n$)
 - $\varepsilon_i \sim\; N(0,\,\sigma^2_\varepsilon)$ \hspace{1cm} ($i = 1, \dots, n$)
 - $Corr(\varepsilon_{i},\,\varepsilon_{i-j}) = 0$ \hspace{0.4cm} ($i = 1, \dots, n; 0 < j < i$)

can be verified by the visual inspection of the suite of residuals plots offered by default by R:

```{r, lmplot, fig = T}
par(mfrow = c(2,2))
plot(fm)
```

The "Residuals vs Fitted" plot does not show, in this example, any particular pattern. The presence of patterns may suggest that the model is inadequate.
The "Normal Q-Q" plot shows points close to the straight line. If the normal assumption of residuals is not satisfied, points are far from the straight line. The "Normal Q-Q plot" is less reliable on the distribution tails, i.e. points ought be very far from the straight line to suggest that residuals follow a non-normal distribution.
The "Scale location" is similar to the residuals versus fitted values plot, but it uses the square root of the standardized residuals in order to diminish skewness. Like the first plot, there should be no discernable pattern to the plot.
The "Residuals vs Leverage" shows leverage points. Leverage points are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation. Leverage points fall out the dotted lines.


### Car Seat

Three quality inspectors are studying the reproducibility of a measurement method aiming to test resistance of a specific material used to cover car seats.
As a result, an experiments involving 75 samples of material from the same batch is set up. Three operators: Kevin, Michelle and Rob are assigned to test 25 samples each.

Comparison of operators average measurements and within operators variations are the key points of the analysis.

```{r, carseat}
load("../data/carseat.Rda")
str(carseat)
```

Boxplot of _Strength by Operator_ along with within operators averages and between averages connecting line can be display by:
```{r, carseatplot, fig = TRUE}
boxplot(Strength~Operator, data = carseat, xlab = "Operator",
  ylab = "Resistance", col = "lightgray")
averages =  tapply(carseat$Strength, carseat$Operator, mean)
points(averages, col = "red", pch = 22, lwd = 2, type = "b")
```

At first glance, difference between operators is hard to detect as within variation seems to be quite large. An outstanding value is shown at Michelle.

A simple one way analysis of variance model is computed with: 

```{r, aov} 
fm = aov(Strength~Operator, data = carseat)
```

Results are shown with the usual `summary()` method.

```{r, summary.aov} 
summary(fm)
```

Operators appears to be borderline significant as its F test p-value is 0.0258. That is, not a real difference exists between operators measurements methods.

The overall influence of Michelle outstanding single result can be tested by repeating the analysis after the single observation is excluded from the data.

```{r, aov2} 
out = max(carseat$Strength[carseat$Operator == "Michelle"])
fm1 = aov(Strength~Operator, data = carseat[-which(carseat$Strength == out),])
summary(fm1)
```

Operators significance is cleary increased as its p-value decreased to 0.0226. Nevertheless, practical results remain unchanged.

In mathematical notation the model can be written as:
$E(y_i|x_i) = \beta_0+\beta_j$ with $j = 1,2,3$

Single coefficients can be visualized by: 

```{r, carseat summary.lm}
summary.lm(fm)
```

Note that the intercept assumes the average strength value for Kevin while the other coefficients represent the difference in _Strength_ between the corresponding operators and Kevin.

Finally, normality of residuals can be checked by:
```{r, residualscarseat, fig = T}
par(mfrow = c(1,2))
plot(fm, which = 1:2)
```

Despite a small departure from normality on the rigth side of the residuals distribution, model assumptions seem to confirm.



### Boiling Time

In an attempt to resolve a domestic dispute about which of two pans was the quicker pan for cooking, the following data were obtained:

```{r, boiling}
load("boiling.Rda")
str(boiling)
```

Various measured volumes (pints) of cold water were put into each pan and heated using the same setting of the cooker. The response variable was the time in minutes until the water boiled.

From a first visual investigation, it's easy to understand that differences in boling time means difference either in intercept or slope between the two lines:

```{r, boilingxyplot, fig=T}
library(lattice)
graph = xyplot(time ~ volume, data = boiling, groups = pan,
  type = "b", auto.key = list(space = "right", lines = F, points = T))
print(graph)
```

A simple linear model may provide the answer to the above questions:

```{r, boling lm}
fm1 = lm(time ~ volume + pan + volume:pan, data = boiling)
summary(fm1)$coeff
```

Given  $\alpha=0.05$ as significance level, only some coefficients result as significant. Note that `PanB` express the difference between the intercept, aliased with _PanA_, and _PanB_ while `volume:panB` represent the difference in slope between _PanA_ (volume) and _PanB_. 

_PanA_ has a shorter boiling time than _PanB_ but both the difference between the intercepts and the difference between slopes are not statistically significant.

A different parameterisation of the model, possibly more intuitive but less attinent with the original problem could be given by:  

```{r, boiling lm2}
fm11 = lm(time ~ pan + volume:pan - 1, data = boiling)
summary(fm11)$coeff
```

or alternatively by:
```{r, boiling lm3}
fm12 = lm(time ~ pan/volume - 1, data = boiling)
summary(fm12)$coeff
```


Finally, the same results could be obtained by using a generalized linear model with Gaussian family and its natural identity link function: 

```{r, boiling lm4}
fm2 = glm(time ~ pan * volume, data = boiling,
  family = gaussian(link = "identity"))
summary(fm12)$coeff
```

However, this model could be unsatisfactory in two respects:

 - boiling time cannot be negative,
 - the variance of boiling time might be expected to increase with its expectation.

The natural candidate for this problem is the gamma distibution as it respect the increasing relationship between mean and variance.

As a result, a generalized linear model with family gamma and link identity can be used in place of a standard liear model.

```{r, boiling glm} 
fm3 = glm(time ~ pan*volume, data = boiling,
  family = Gamma(link = "identity"))
summary(fm3)$coeff 
```

Models comparison can be given by the AIC statistics.
```{r, boiling aic} 
fm2$aic
fm3$aic
```

Gamma model appears to be slightly better.


## Summary
> In this chapter, we introduced statistical modelling with R. Through some examples on real data, we saw the `lm()` function to build a linear regression model, the `glm()` function to build a generalised linear model and `anova()` and `aov()` functions to build an ANOVA model. The `summary()` function is useful to investigate regression and ANOVA results. We used the `plot()` function both to draw the regression line in the scatter plot and to perform a residual analysis. P value were used to choose between two nested models, while AIC statistics were introduced to compare two not-nested models.


