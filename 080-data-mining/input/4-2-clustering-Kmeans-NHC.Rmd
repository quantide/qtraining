---
title: "Non-Hierarchical Clustering (K-Means) (NHC)"
---

```{r options, include=FALSE, purl=FALSE}
source("options.R")
```

```{r first, include=FALSE, purl=TRUE, message=FALSE}
# This code chunk contains R code already described in the previous chapters
# that is required by following examples

## Datasets from packages
require(qdata)
require(dplyr)
data(utilities)
## Other datasets used
# iris

##################################################
## packages needed: ggplot2, GGally, clusterSim ##
##################################################
```


# Introduction

## Non-Hierarchical (Partitioning) Methods

Given a preassigned number $K$ of groups, Non-Hierarchical Clustering (NHC)
methods seek to partition the data into $K$ clusters so that the items within
each cluster are similar to each other, whereas items from different
clusters are quite dissimilar.

A possible approach to do this would be to enumerate all the possible groupings of the
items in $K$ groups and then choose as the best solution the grouping that
optimizes some predefined criterion. Unfortunately, such an
approach would become rapidly infeasible, especially for large datasets,
requiring incredible amounts of computer time and storage. As a result, all
available clustering techniques are iterative and work on only a very
limited amount of enumeration.

Among the many non-hierarchical clustering algorithms developed so far, the
most popular is the K-means one. In its basic implementation, the
K-means algorithm starts either by assigning items to one of $K$ predetermined
clusters and then computing the $K$ cluster centroids, or by pre-specifying
the $K$ cluster centroids. The pre-specified centroids may be randomly selected
items or may be obtained by cutting a dendrogram at an appropriate height.
Then, through an iterative procedure, the algorithm seeks to minimize the
within-group sum of squares (WGSS) over all variables by reassigning items to
clusters. The procedure stops when no further reassignment reduces the value
of WGSS.

The solution will typically not be unique; the algorithm will only find a
local minimum of WGSS. Therefore, it is recommended that the algorithm be
run using different initial random assignments of the items to $K$ clusters
(or by randomly selecting K initial centroids) in order to find the global
minimum of WGSS and, hence, the best clustering solution based upon $K$
clusters.

As an example, we consider the `uscrime` dataset (see the section *Introduction and datasets used* for further information):

```{r message=FALSE}
require(ggplot2)
require(GGally)
```

```{r 02e-loadandsummaryuc}
set.seed(10)
utilities_tmp <- utilities[, 1:8]
ggpairs(utilities_tmp)
```

If we first calculate the variances of the crime rates for the different
types of crimes we find the following:

```{r 02e-sapplyuc}
sapply(utilities_tmp, var)
```

The variances are very different, and using K-means on the raw data would not
be sensible; we must standardize the data in some way, and here we
choose to standardize each variable by its range. After such standardization,
the variances become

```{r 02e-applyandsweepuc}
rge <- sapply(utilities_tmp, function(x) diff(range(x)))
utilities_s <- sweep(x = utilities_tmp, MARGIN = 2, STATS = rge, FUN = "/")
sapply(utilities_s, var)
```

We can now proceed with clustering the data. First we plot the WGSS for one-
to six-group solutions to see if we can get any indication of the number of
groups. The plot is obtained as follows:

```{r 02e-scree1uc}
k_max <- 8
wss <- rep(0, k_max)
for (k in 1:k_max) {
	wss[k] <- kmeans(utilities_s, centers = k)$tot.withinss
}
ggp <- ggplot(data = data.frame(x=1:k_max, y=wss), mapping = aes(x=x,y=y)) +
  geom_point() +
  geom_line() +
  xlab("Number of groups") + 
  ylab("Within groups sum of squares")
print(ggp)

```

As the number of groups increases, the sum of squares will necessarily
decrease, but an obvious "elbow" in the plot may be indicative of the most
useful solution for the investigator to look at in detail. In our case there
is only one possible "elbow" in the plot, at four groups, and we now
look at this solution. The group means for this solutions is computed by

```{r 02e-plotsuc1}
km_4 <- kmeans(utilities_s, centers = 4)
km_4$centers * rge

utilities_pca <- princomp(utilities_tmp, cor = TRUE)
summary(utilities_pca, loadings=TRUE)
plot(utilities_pca, type = "l")
abline(h = 1, lty = 2)
```

If we apply the "proportion of explained variance" criterion, the number of components to retain should be at least 4.  
Anyway, for this example, we will retain the first two components only.

```{r 02e-plotsuc1a}
biplot(utilities_pca)
ggcorr(cbind(utilities_s, utilities_pca$scores), label = TRUE, cex = 2.5)
```

The first principal component seems to oppose technical measures (`fuel`, `load`, `cost`) with financial ones (`coverage` and `return`), while the second principal component seems to oppose sales (`sales`) and costs (`fuel`) .  
Now we can try using the Calinski-Harabasz index to help us in finding the optimal number of clusters to "combine" with PCA analysis results:
```{r 02d-chfuncut,warning=FALSE, message=FALSE}
require(clusterSim)
minC <- 2
maxC <- 10
res <- numeric(maxC - minC)
for (nc in minC:maxC) {
	res[nc - minC + 1] <- index.G1(utilities_s, kmeans(utilities_s,centers = nc)$cluster)
}
ggp <- ggplot(data=data.frame(x=2:(length(res)+1), y= res), mapping = aes(x=x,y=y)) + 
  geom_point() + 
  geom_line() +
  xlab("Number of clusters") +
  ylab("Calinski-Harabasz pseudo F-statistic")
print(ggp)
```

This index gives an almost clear indication on the number of clusters to choose, i.e., 4.

Since many variables are available, we will try to plot the grouing information on the plot of scores of 
a simple 2-dimensions PCA analysis

```{r 02e-plotsuc2, fig.width=plot_with_legend_fig_width_short}
Cluster <- as.character(km_4$cluster)
utilities_scores <- cbind(data.frame(utilities_pca$scores[, c("Comp.1", "Comp.2")]), company=utilities$comp_short, Cluster=Cluster)
ggp <- ggplot(data= utilities_scores, mapping = aes(x = Comp.1, y=Comp.2, label=company, colour=Cluster)) +
  geom_point() +
  xlab("1st PCA dimension") +
  ylab("2nd PCA dimension") +
  geom_text(hjust=0.5, vjust=-0.5, size=3)
print(ggp)
```

This graph show how the 4 groups are related with the first 2 components of PCA. It shows that the group 1 and 3 are clearly separated.  
The other two groups are somehow confused; maybe, they could be better separated by using a third PC.

Without using 3D graphs, the next graph can help to find some differences between the remaining two groups:
```{r 02e-plotsuc3}
pairs(utilities_tmp, col = Cluster, pch = Cluster, cex = .75)
```

<!---

```{r 02e-plotsuc4, fig.width=plot_with_legend_fig_width_short, purl=FALSE}
# Cluster <- as.character(km_4$cluster)
# ggp <- ggplot(data=data.frame(x = uscrime_scores[, 1], y = uscrime_scores[, 2], Cluster=Cluster, state=uscrime$state), mapping = aes(x=x,y=y, colour=Cluster))+
#   geom_point() +
#   xlab("1st PCA dimension") +
#   ylab("2nd PCA dimension") +
#   geom_text(mapping = aes(label=state), hjust = 0.5, vjust = -0.5, size = 3)
# print(ggp)
```

```{r 02e-plotsuc5, purl=FALSE}
# pairs(utilities_tmp, col = Cluster, pch = Cluster, cex = .75)
```

The previous plots together with the next ones suggest that the two groups
solution seems to be more interpretable. Anyway, we can also cross ths information
with the geographic areas:

```{r 02e-levelplotsuc, purl=FALSE}
# uscrime$reg <- as.factor(uscrime$reg)
# levels(uscrime$reg) <- c("Northeast", "Midwest", "South", "West")
# 
# plot(table(uscrime$reg, km_2$cluster), main = "Clusters vs. US States region",
# 			xlab = "Region of US", ylab = "2 groups solution")
# 
# plot(table(uscrime$reg, km_4$cluster), main = "Clusters vs. US States region",
# 			xlab = "Region of US", ylab = "4 groups solution")
```
--->

The next grph will help to "identify" groups as for hierarchical clustering:

```{r 02d-membsplotut3, fig.width=7}
utilities$member <-  Cluster
util.summ <- summarise(group_by(utilities, member),
                       coverage = mean(coverage),
                       return   = mean(return),
                       cost     = mean(cost),
                       load     = mean(load),
                       peak     = mean(peak),
                       sales    = mean(sales),
                       nuclear  = mean(nuclear),
                       fuel     = mean(fuel))
palette(rainbow(8))
to.draw <- apply(util.summ[, -1], 2, function(x) x/max(x))
stars(to.draw, draw.segments = TRUE, scale = FALSE, key.loc = c(4.6,2.0), nrow=3, ncol=2,
      labels = c("CLUSTER 1", "CLUSTER 2","CLUSTER 3", "CLUSTER 4"),
      main = "Utilities data (cluster profiling)", cex = .75, ylim=c(0,8),
      flip.labels = TRUE)
palette("default")
```

Thus, it seems that the cluster are also associated with geographic areas of US states.
<!---
# Exercises with iris o utilities 
--->
