---
title: "Hierarchical (Agglomerative) Clustering (HC)"
---

```{r options, include=FALSE, purl=FALSE}
source("options.R")
```

```{r first, include=FALSE, purl=TRUE, message=FALSE}
# This code chunk contains R code already described in the previous chapters
# that is required by following examples

## Datasets from packages
require(qdata)
data(utilities)

## Other datasets used
# iris

################################################
## packages needed: GGally, dplyr, clusterSim ##
################################################
```


# Introduction

The starting point of every Hierarchical Clustering (HC) method is the calculation of the
dissimilarity (i.e., distance) of one item relative to another item. Which
definition of distance (Euclidean, Manhattan, Canberra, etc.) is used is
often application-dependent rather than a matter of subjective choice. Some
distance measures are appropriate only for certain types of data, while some
other have been introduced for clustering variables rather than observations.

A general tool in `R` for calculating dissimilarities is the `dist()` function

`?dist`

The `dist()` function produces and object of type `dist` that need to be
supplied to any function for HC. Another option for getting the
dissimilarities among a set of items is through the `daisy()` function in the
cluster package. This function includes a metric (Gower) that allows to
calculate the distance using a mixture of quantitative and qualitative
variables.

After the dissimilarity matrix has been obtained, one needs to specify how
to calculate the distance between groups of observations during the algorithm
iterations. There are many choice here as well. Among these, we find
the so-called linkage methods:

- **single linkage** ==> the distance between two groups is defined as the
   			   smallest value of the item distances;
- **complete linkage** ==> the distance between two groups is defined as the
						 largest value of the item distances;
- **average linkage** ==> this is a compromise between the previous two 
						approaches, obtained by averaging the corresponding
						distances.

Another popular approach for calculating the distances among groups is the
**Ward method**, which joins groups that obtain the minimum increase of a given
measure of heterogeneity.  
Ward's minimum variance criterion minimizes the total within-cluster variance.  
To implement this method, at each step find the pair of clusters that leads
to minimum increase in total within-cluster variance after merging. This
increase is a weighted squared distance between cluster centers.  
At the initial step, all clusters are singletons (clusters containing a single
point). To apply a recursive algorithm under this objective function, the initial
distance between individual objects must be (proportional to) squared Euclidean
distance.  
Ward's minimum variance method can be defined and implemented recursively by a
Lanceâ€“Williams algorithm, where, for disjoint clusters $C_i$, $C_j$, and $C_k$
with sizes $n_i$, $n_j$, and $n_k$ respectively, the distance from the "joined"
cluster $C_i \cup C_j, C_k$ and a third cluster $C_k$ is calculated as:

$d(C_i \cup C_j, C_k) = 
 \frac{n_i+n_k}{n_i+n_j+n_k}\;d(C_i,C_k) +
 \frac{n_j+n_k}{n_i+n_j+n_k}\;d(C_j,C_k) -
 \frac{n_k}{n_i+n_j+n_k}\;d(C_i,C_j)$. 
 
All the above approaches produce potentially different clusters, and which one
should be used in practice depends essentially on a subjective choice.  

As an illustration, consider the dataset `utilities` (see the section 
*Introduction and datasets used* for further information), which is about 22
US utility companies regarding the following variables:

- coverage   ==>   fixed-charge coverage ratio (income/debt)
- return     ==>   rate of return on capital
- cost       ==>   cost per kW capacity in place
- load       ==>   annual load factor
- peak       ==>   peak kWh demand growth from 1974 to 1975
- sales      ==>   sales (kWh use per year)
- nuclear    ==>   percent nuclear
- fuel       ==>   total fuel costs (cents per kWh)
- company    ==>   company full name
- comp_short ==>   company short name

```{r message=FALSE}
require(GGally)
```

```{r 02d-loadandsummaryut}
str(utilities)
ggpairs(utilities[, -c(9, 10)])
```

There are many functions to perform cluster analysis in `R` (for a complete list, see the CRAN
task view at the URL http://cran.r-project.org/web/views/Cluster.html). We
describe here the basic functions available in the `R` base version, in
particular the function hclust(). To avoid any scale effect, we first
standardize the variables:

```{r 02d-clusttryut}
utilities.std <- scale(utilities[, -c(9, 10)])
d <- dist(utilities.std)
util.SL <- hclust(d, method = "single")
util.CL <- hclust(d, method = "complete")
util.AL <- hclust(d, method = "average")
util.Ward <- hclust(d, method = "ward.D2")
```
As seen, the `hclust()` function requires a dissimilarity matrix as the input.
A standard graphical approach to represent the solution of an agglomerative
HC algorithm is through the dendrogram, which represents the sequence of
successive fusions of the different groups and the distances at which they
are merged. A dendrogram in `R` is available as a plot method for an object of
type "hclust":

```{r 02d-clusttryplotut}
op <- par(mfrow = c(2, 2))
plot(util.SL, labels = utilities[, 10], cex = .7,
		main = "Utilities data (single linkage)", xlab = "Utilities")
plot(util.CL, labels = utilities[, 10], cex = .7,
		main = "Utilities data (complete linkage)", xlab = "Utilities")
plot(util.AL, labels = utilities[, 10], cex = .7,
		main = "Utilities data (average linkage)", xlab = "Utilities")
plot(util.Ward, labels = utilities[, 10], cex = .7,
		main = "Utilities data (Ward)", xlab = "Utilities")
par(op)
```
The single linkage method has a tendency to create large groups by
successively adding items to already created groups (so-called "chaining").
The other three methods return similar results with more structured clusters.

Once a number of clusters has been chosen (more on this later), the function
`cutree()` can be used to obtain the cluster membership for each item:

```{r 02d-cuttreeut}
util.CL.m <- cutree(util.CL, k = 3)
util.CL.m
```

The cluster membership is useful to perform the profiling of the clusters,
that is to provide a more detailed description of the main features of the
identified clusters:

```{r message=FALSE}
require(dplyr)
```

```{r 02d-profilesut}
utilities <- cbind(utilities, member = util.CL.m)
table(utilities$member)
by(data = utilities[, -(9:11)], INDICES = utilities$member, FUN = summary)
util.summ <- group_by(utilities, member) %>%
  summarise(coverage = mean(coverage),
            return	 = mean(return),
            cost     = mean(cost),
            load     = mean(load),
            peak     = mean(peak),
            sales    = mean(sales),
            nuclear  = mean(nuclear),
            fuel     = mean(fuel))
palette(rainbow(8))
to.draw <- apply(util.summ[, -1], 2, function(x) x/max(x))
stars(to.draw, draw.segments = TRUE, scale = FALSE, key.loc = c(4.6, 2.3),
		labels = c("CLUSTER 1", "CLUSTER 2", "CLUSTER 3"),
		main = "Utilities data (cluster profiling)", cex = .75,
		flip.labels = TRUE)
palette("default")
```

The `cutree()` function can be used to generate more than one vector of cluster
membership at once:

```{r 02d-cuttree2ut}
util.CL.g234 <- cutree(util.CL, k = 2:4)
table(clus2 = util.CL.g234[, "2"], clus4 = util.CL.g234[, "4"])
```

A further graphical approach to get the cluster memberships is through the
function `rect.hclust()` which adds to the dendrogram rectangles showing the
identified clusters:

```{r 02d-rectclusut}
plot(util.CL)
rect.hclust(util.CL, k = 5)
```

It is possible to use the `identify()` function to directly select on the
dendrogram the number of clusters to use. You can use a code like this:

```{r 02d-rectclusut-identify, eval=FALSE}
plot(util.CL)
r <- identify(util.CL)
```

Finally, the `clusterSim` package provides many functions to compare the
goodness of different clustering solutions. Among these functions we find:

- index.DB()  ==> Calculates Davies-Bouldin's index
- index.G1()  ==> Calculates Calinski-Harabasz pseudo F-statistic
- index.G2()  ==> Calculates G2 internal cluster quality index
- index.G3()  ==> Calculates G3 internal cluster quality index
- index.Gap() ==> Calculates Tibshirani, Walther and Hastie gap index
- index.H()   ==> Calculates Hartigan index
- index.KL()  ==> Calculates Krzanowski-Lai index
- index.S()   ==> Calculates Rousseeuw's Silhouette internal cluster quality index

For more details see the help pages of these functions. As an example, we consider here the Calinski-Harabasz pseudo F-statistic:

```{r message=FALSE}
require(clusterSim)
require(ggplot2)
```

```{r 02d-chfuncut,warning=FALSE}
minC <- 2
maxC <- 10
res <- numeric(maxC - minC)
for (nc in minC:maxC) {
	res[nc - minC + 1] <- index.G1(utilities.std, cutree(util.Ward, k = nc))
}
ggp <- ggplot(data=data.frame(x=2:(length(res)+1), y= res), mapping = aes(x=x,y=y)) + 
  geom_point() + 
  geom_line() +
  xlab("Number of clusters") +
  ylab("Calinski-Harabasz pseudo F-statistic")
print(ggp)
```

According to the Calinski-Harabasz pseudo F-statistic, we conclude that the 4
clusters solution seems a good solution.

After a good solution has been identified, we can save the corresponding
cluster membership ans use it for graphical purposes:

```{r 02d-membsplotut2, warning=FALSE}
Cluster <- as.character(cutree(util.CL, k = (minC:maxC)[which.max(res)]))
ggscatmat(data = cbind(utilities[, -(9:11)], Cluster=Cluster), color = "Cluster")
```

```{r 02d-membsplotut3, fig.width=7}
utilities$member <-  Cluster
table(utilities$member)
by(utilities[, -(9:11)], utilities$member, summary)
util.summ <- summarise(group_by(utilities, member),
                       coverage = mean(coverage),
                       return   = mean(return),
                       cost     = mean(cost),
                       load     = mean(load),
                       peak     = mean(peak),
                       sales    = mean(sales),
                       nuclear  = mean(nuclear),
                       fuel     = mean(fuel))
palette(rainbow(8))
to.draw <- apply(util.summ[, -1], 2, function(x) x/max(x))
stars(to.draw, draw.segments = TRUE, scale = FALSE, key.loc = c(4.6,2.0), nrow=3, ncol=2,
      labels = c("CLUSTER 1", "CLUSTER 2","CLUSTER 3", "CLUSTER 4"),
      main = "Utilities data (cluster profiling)", cex = .75, ylim=c(0,8),
      flip.labels = TRUE)
palette("default")
```

<!---
Exercises with  uscrime or iris
--->

