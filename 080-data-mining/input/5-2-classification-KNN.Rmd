---
title: "K-Nearest Neighbors (KNN)"
---

```{r options, include=FALSE, purl=FALSE}
source("options.R")
```

```{r first, include=FALSE, purl=TRUE, message=FALSE}
# This code chunk contains R code already described in the previous chapters
# that is required by following examples

## Datasets from packages
data(Smarket, package = "ISLR")

## Other datasets used
# none

##################################
## packages needed: caret, ISLR ##
##################################
```


# Introduction

Given a positive integer $K$ and a test observation $x$, the K-Nearest Neighbors (KNN) classifier first
identifies the $K$ points in the training data that are closest to $x$.
It then estimates the probability for every class $j$ of
the response variable as the fraction of points in $K$ whose response values
equal $j$. Then, it classifies the test observation $x$ to the class with the
largest probability. Even if KNN is a very simple approach, it can often
produce surprisingly good results. The key point in applying KNN is the
choice of $K$, the number of closest observations to include in the neighbor of
a given point. When $K$ is small, the method produces very flexible decision
boundaries. As $K$ grows, the method becomes less flexible and produces
decision boundaries that are close to linear. $K$ is usually chosen using an
approach called "cross-validation".

One advantage of KNN is that it is a completely non-parametric approach,
because no assumptions are made about the shape of the decision boundaries.
Therefore, KNN can produce better results than LDA and logistic regression
especially when the decision boundaries are highly non-linear. On the other
hand, KNN is less informative than LDA and logistic regression, because it
doesn't provide an intuition about which predictors are important for
discriminating the response classes, since it doesn't produce a table of
coefficients.


## Example: Stock Market Data

We now apply KNN to `Smarket` (see the section *Introduction and datasets used* for further information), the same stock market data we already presented for LDA.
The package `caret` contains the function `knn3Train()` which directly returns the predictions for the response variable. This function requires the predictor values for both the training and test data, the class labels for the training observations, and a value for $K$. We still use the data from 2005 as the test set and all the remaining observations for training:

```{r message=FALSE}
require(caret)
```

```{r 02i-loadandknnst}
train <- (Smarket$Year < 2005)
train_X <- Smarket[train, c("Lag1", "Lag2")]
test_X <- Smarket[!train, c("Lag1", "Lag2")]
train_Y <- Smarket[train, "Direction"]
test_Y <- Smarket[!train, "Direction"]
set.seed(123)   # the random seed is needed because R will break ties at random
knn_pred_1 <- knn3Train(train_X, test_X, train_Y, k = 1, use.all = FALSE)
confusionMatrix(data = knn_pred_1, reference = test_Y, positive = "Up")
```

Using `k = 1` only 50% of the observations in the test set are correctly
predicted. We now repeat the analysis using `k = 3`:

```{r 02i-knnk3st}
knn_pred_3 <- knn3Train(train_X, test_X, train_Y, k = 3, use.all = FALSE)
confusionMatrix(data = knn_pred_3, reference = test_Y, positive = "Up")
```

With `k = 3` we get slightly better results (53.6% correctly classified). As an
attempt to improve the results, we look for the optimal value of $K$ by
evaluating the test error rate over different values of $K$ and choosing the
one that minimizes it:

```{r 02i-knnkmultist}
kmax <- 100
test_error <- numeric(kmax)
for (k in 1:kmax) {
  knn_pred <- knn3Train(train_X, test_X, train_Y, k = k, prob = FALSE,
						  use.all = FALSE)
	cm <- confusionMatrix(data = knn_pred, reference = test_Y, positive = "Up")
	test_error[k] <- 1 - cm$overall[1]
}
k_min <- which.min(test_error)
knn_pred_min <- knn3Train(train_X, test_X, train_Y, k = k_min, prob = FALSE,
						 use.all = FALSE)
cm <- confusionMatrix(data = knn_pred_min, reference = test_Y, positive = "Up")
ggp <- ggplot(data.frame(test_error), aes(x = 1:kmax, y = test_error)) +
	geom_line() +
  geom_point() +
  xlab("K (#neighbors)") + ylab("Test error") +
	ggtitle(paste0("KNN best K value = ", k_min,
				   " (min error = ",
				   format((1 - cm$overall[1])*100, digits = 4), "%)"))
print(ggp)
```

`k = 72` seems to provide the smallest test error rate (around 46%). To
conclude, it seems that KNN does not provide a better result than LDA and
QDA.

The `caret` package provides many very general functions, such as `train()`, for
fitting a wide range of predictive models using cross-validation or other
resampling methods, but these functions are considerably more complicated to
be used.


<!---
# Exercises with iris or utilities or uscrimes 
--->